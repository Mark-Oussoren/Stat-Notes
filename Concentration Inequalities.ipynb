{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39be92f",
   "metadata": {},
   "source": [
    "# Concentration Inequalities\n",
    "\n",
    "This notebook is dedicated to working through problems on concentration inequalities which more or less revolve around sums of independent random variables. \n",
    "\n",
    "I believe the layout of this notebook will be as follows: 1) I will first provide the inequalities, definitions, theorems and main results in Markdown - just the basics and sentences explaining them in english coupled with worked out solutions to the book problems of \"Concentration Inequalities\" by Boucheron et al.\n",
    "\n",
    "# Sums of Indendent Random Variables & the Martingale Method\n",
    "\n",
    "## Review of Independence\n",
    "\n",
    "Here I would like to touch on measure-theoretic definitions motivating what independent random variables actually imply. As with Kolmogorov, I first discuss the concept of independent sets, $\\sigma$-algebra, and finally random variables.\n",
    "\n",
    "**Definition**: Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space with $\\{\\mathcal{F}_i\\}_{i\\in\\mathcal{I}}\\;$  denoting sub $\\sigma$-algebras of $\\mathcal{F}$. We say that the $\\sigma$-algebras $\\mathcal{F}_i$, $i\\in\\mathcal{I}$ are mutually $\\mathbb{P}$-independent if for every finite subset $\\{i_1,\\dots,i_n\\}$ of $\\underline{distinct}$ elements of $\\mathcal{I}$ and every choice of sets $A_{i_k}\\in\\mathcal{F}_{i_k}$ for $k\\in[n]$, we have\n",
    "\n",
    "$$\\mathbb{P}\\left(\\bigcap\\limits_{k=1}^n A_{i_k}\\right)=\\prod\\limits_{k=1}^n \\mathbb{P}\\left(A_{i_k}\\right).$$\n",
    "\n",
    "In particular, if $\\left\\{A_i: i\\in\\mathcal{I}\\right\\}$ is a family of sets in $\\mathcal{F}$, then we say $A_i$, $i\\in\\mathcal{I}$ are $\\mathbb{P}$-independent if the associated $\\sigma$-algebras: $\\mathcal{F}_i=\\left\\{\\emptyset, A_i,A_i^c,\\Omega\\right\\}$, $i\\in\\mathcal{I}$, are.\n",
    "\n",
    "Now as random variables are measurable functions on $(\\Omega,\\mathcal{F})$ taking values in a measure space $(E_i,\\mathcal{B}_i)$, we can express the definition of independent random variables by proving independence of their pull-back $\\sigma$-algebras.\n",
    "\n",
    "**Definition**: We say that the random variables $X_i$, $i\\in\\mathcal{I}$ are mutually $\\mathbb{P}$-independent if the $\\sigma$-algebras \n",
    "\n",
    "$$\\sigma(X_i)=\\left\\{X_i^{-1}(B_i):B_i\\in\\mathcal{B}_i\\right\\},\\;\\;\\;i\\in\\mathcal{I}$$\n",
    "\n",
    "are $\\mathbb{P}$-independent. There are many equivalent formulations that will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccb639",
   "metadata": {},
   "source": [
    "# Chapter 2 - Basic Inequalities Problems\n",
    "\n",
    "**2.1** Let $\\mathbb{M}[Z]$ be a median of the square-integrable random variable $Z$. Show that \n",
    "\n",
    "$$|\\mathbb{M}[Z] - \\mathbb{E}[Z]|\\leq \\sqrt{\\mathbb{V}[Z]}.$$\n",
    "\n",
    "- asdf\n",
    "\n",
    "**2.2** Let $X$ be a random variable with median $\\mathbb{M}[X]$ such that there exists positive constants $a$ and $b$, so that for all $t>0$,\n",
    "\n",
    "$$\\mathbb{P}\\left\\{ |X - \\mathbb{M}[X]| > t \\right\\}\\leq ae^{-t^2/b}.$$\n",
    "\n",
    "Show that $|\\mathbb{M}[X] - \\mathbb{E}[X]|\\leq \\min\\left(\\sqrt{ab}, a\\sqrt{b\\pi}/2\\right)$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**2.3** Prove the following one-sided improvement of Chebyshev's inequality: for any real-valued random variable $Y$ and $t>0$,\n",
    "\n",
    "$$\\mathbb{P}\\left\\{ Y - \\mathbb{E}[Y] \\geq t \\right\\} \\leq \\frac{\\mathbb{V}(Y)}{\\mathbb{V}(Y) + t^2}.$$\n",
    "\n",
    "- asdf\n",
    "\n",
    "**2.4** Show that if $Y$ is a nonnegative random variable, then for any $a\\in(0,1)$,\n",
    "\n",
    "$$\\mathbb{P}\\left\\{ Y \\geq a\\mathbb{E}[Y] \\right\\} \\geq (1-a)^2\\frac{(\\mathbb{E}[Y])^2}{\\mathbb{E}[Y^2]}.$$\n",
    "\n",
    "- asdf\n",
    "\n",
    "**2.5** Show that moment bounds for tail probabilities are always better than Cram\\'er-Chernoff bounds. More preciselly, let $Y$ be a nonnegative random variable and let $t>0$. The best moment bound for the tail probability $\\mathbb{P}\\{Y\\geq t\\}$ is $\\min_q \\mathbb{E}[Y^q]t^{-q}$ where the minimum is taken over all positive integers. The best Cram\\'er-Chernoff bound is $\\inf_{\\lambda>0} \\mathbb{E}[e^{\\lambda (Y - t)}]$. Prove that \n",
    "\n",
    "$$\\min\\limits_q \\mathbb{E}[Y^q]t^{-q} \\leq \\inf\\limits_{\\lambda > 0} \\mathbb{E}[e^{\\lambda (Y - t)}].$$\n",
    "\n",
    "- asdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30baeb15",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- \"Concentration Inequalities\" by Boucheron, Lugosi, Massar\n",
    "\n",
    "- https://math.mit.edu/~dws/175/prob01.pdf\n",
    "- https://terrytao.wordpress.com/2015/10/12/275a-notes-2-product-measures-and-independence/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
