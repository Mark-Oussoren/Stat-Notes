{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1013ae",
   "metadata": {},
   "source": [
    "Notes on basics are to be included here - probably just source MIT or some other system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac53795",
   "metadata": {},
   "outputs": [],
   "source": [
    "Notes on numerical linear algebra are to be included here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe017e",
   "metadata": {},
   "source": [
    "# Exercises from \"Introduction to Matrix Analysis\" by Richard Bellman\n",
    "## 2 - Vectors and Matrices\n",
    "### 3. Vector Addition\n",
    "1. Show that we have commutativity, $x+y=y+x$, and associativity, $x+(y+z)=(x+y)+z$.\n",
    "\n",
    "- I assume our matrices are either integer, rational, real, or complex. In any of these cases, it trivially follows that since $x+y=y+x$ for $x,y\\in\\mathcal{F}$ that the same holds if we perform the addition elementwise as done in vector addition. On a similar note, we know these fields of numbers are associative, so associativity is also an inherent condition. \n",
    "\n",
    "3. Define subtraction of two vectors directly, and in terms of addition; that is, $x-y$ is a vector $z$ such that $y+z=x$.\n",
    "\n",
    "- Let $z=x+(-y)$, so $y+(x+(-y))=x+(y+(-y))=x$ by associativity.\n",
    "\n",
    "### 4. Scalar Multiplication\n",
    "1. Show that $(c_1+c_2)(x+y)=c_1x+c_1y+c_2x+c_2y$.\n",
    "\n",
    "- $(c_1+c_2)(x+y)=(c_1+c_2)x+(c_1+c_2)y=c_1x+c_2x+c_1y+c_2y$.\n",
    "\n",
    "2. Define the null vector, written 0, to be the vector all of whose components are zero. Show that it is uniquely determined as the vector 0 for which $x+0=x$ for all $x$.\n",
    "\n",
    "- Again, this question stems from our understanding of the field of numbers we are working with. Clearly $x+0=x$ for $x\\in\\mathbb{R}$ and this concept extends for each entry as we denote the zero vector by $n$ zeros in a column vector.\n",
    "\n",
    "### 5. The Inner Product of Two Vectors\n",
    "1. If $x$ is real, show that $(x,x)>0$ unless $x$ is $0$.\n",
    "\n",
    "- If $x\\neq 0$, $(x,x)=\\sum\\limits_{i=1}^n x_i^2>0$ for some component $x_i\\neq 0$ in $x$.\n",
    "\n",
    "2. Show that $(ux+vy,ux+vy)=u^2(x,x)+2uv(x,y)+v^2(y,y)$ is a non-negative definite quadratic form in the scalar variables $u$ and $v$ if $x$ and $y$ are real and hence that \n",
    "$$(x,y)^2\\leq (x,x)(y,y)$$\n",
    "for any two real vectors $x$ and $y$ (Cauchy's inequality).\n",
    "\n",
    "- $(ux+vy,ux+vy)=\\sum\\limits_{i=1}^n (ux_i+vy_i)^2$. This quantity is clearly non-negative provided the field of numbers is real or rational. To show Cauchy's inequality, we know $(ux+vy)(ux+vy)=u^2(x,x)+2uv(x,y)+v^2(y,y)$ by expanding out the inner product. If we consider $u=-\\sqrt{(y,y)}$ and $v=\\sqrt{(x,x)}$, then the above becomes\n",
    "\n",
    "$$\\left(-\\sqrt{y,y}\\right)^2(x,x)-2\\sqrt{(y,y)(x,x)}(x,y)+\\left(\\sqrt{x,x}\\right)^2(y,y)\\geq 0\\iff 2(x,x)(y,y)\\geq 2\\sqrt{(y,y)(x,x)}(x,y)$$\n",
    "\n",
    "$$\\iff (x,x)^2(y,y)^2\\geq (x,x)(y,y)(x,y)^2\\iff (x,x)(y,y)\\geq (x,y)^2.$$\n",
    "\n",
    "3. What is the geometric interpretation of this result?\n",
    "\n",
    "- The projection of a vector onto another does not have longer length than the vector itself.\n",
    "\n",
    "4. Using this result, show that for real vectors $x$ and $y$ that\n",
    "\n",
    "$$(x+y,x+y)^{1/2}\\leq (x,x)^{1/2}(y,y)^{1/2}.$$\n",
    "\n",
    "- Using Cauchy's, $(x+y,x+y)=(x,x)+2(x,y)+(y,y)\\leq(x,x)+2\\sqrt{(x,x)(y,y)}+(y,y)=\\left((x,x)^{\\frac{1}{2}}+(y,y)^{\\frac{1}{2}}\\right)^2$. From here, we simply take the square root of both sides and call it a day. \n",
    "\n",
    "5. Why is this inequality called the \"triangle inequality\"?\n",
    "\n",
    "- Because in $\\mathbb{R}^2$, we have that a triangle's smallest two sides summed is greater than or equal to the hypotenuse - hence the name stemming from this geometric shape. \n",
    "\n",
    "### 6. Orthogonality\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 7. Matrices\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 8. Matrix Multiplication - Vector by Matrix\n",
    "1. Show that $(A+B)(x+y)=Ax+Ay+Bx+By$.\n",
    "\n",
    "- $(A+B)(x+y)=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N (a_{ij}+b_{ij})(x_j+y_j)=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N a_{ij}x_j+a_{ij}y_j+b{ij}x_j+b_{ij}y_j=Ax+Ay+Bx+By$.\n",
    "\n",
    "### 9. Matrix Multiplication - Matrix by Matrix.\n",
    "2. If \n",
    "\n",
    "$$T(\\theta)=\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{bmatrix}$$\n",
    "\n",
    "show that \n",
    "\n",
    "$$T(\\theta_1)T(\\theta_2)=T(\\theta_2)T(\\theta_1)=T(\\theta_1+\\theta_2).$$\n",
    "\n",
    "- Using the identity $\\cos\\theta_1+\\theta_2=\\cos\\theta_1\\cos\\theta_2-\\sin\\theta_1\\sin\\theta_2$ and $\\sin\\theta_1+\\theta_2=\\cos\\theta_1\\sin\\theta_2+\\cos\\theta_2\\sin\\theta_1$, we can clearly expand out the linear operators to arrive at the equalities.\n",
    "\n",
    "3. Let $A$ be a matrix with the property that $a_{ij}=0$ if $j\\neq i$, a diagonal matrix, and let $B$ be a matrix of similar type. Show that $AB$ is again a diagonal matrix, and that $AB=BA$.\n",
    "\n",
    "- Let $C=AB$. $c_{ij}=\\sum\\limits_{k=1}^n a_{ik}b_{kj}=a_{ik}b_{kj}=a_{ii}b_{ii}\\iff i=j$ otherwise the sum is zero. Thus the matrix is diagonal. Similarly, letting $D=BA$ we obtain $d_{ij}=\\sum\\limits_{k=1}^n b_{ij}a_{jk}=b_{ik}a_{kj}=b_{ii}a_{ii}\\iff i=j$ otherwise the sum is zero. As The field is commutative, interchanging these elements is allowed and so $C=D$.\n",
    "\n",
    "4. Let $A$ be a matrix with the property that $a_{ij}=0$, $j>i$, a triangular or semi-diagonal matrix, and let $B$ be a matrix of similar type. Show that $AB$ is again a triangular matrix, but that $AB\\neq BA$ in general.\n",
    "\n",
    "- Let $C=AB$ so $c_{ij}=\\sum\\limits_{i=1}^n a_{ik}b_{kj}=0$ if $k>i$ or $j>k$ which is equivalent to saying $j>i$ which implies the matrix is triangular. If $A=\\begin{bmatrix}a&0\\\\b&c\\end{bmatrix}$ and $B=\\begin{bmatrix}x&0\\\\y&z\\end{bmatrix}$, $AB\\neq BA$ if we ensure $bx+cy\\neq ay+bz$.\n",
    "\n",
    "6. Use the relation $|AB|=|A||B|$ to show that \n",
    "\n",
    "$$(a_1^2+a_2^2)(b_1^2+b_2^2)=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2$$\n",
    "\n",
    "- If we let $A=\\begin{bmatrix}a_1&a_2\\\\-a_2&a_1\\end{bmatrix}$ and $B=\\begin{bmatrix}b_1&b_2\\\\-b_2&b_1\\end{bmatrix}$, then\n",
    "\n",
    "$$|AB|=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2=|A||B|=(a_1^2+a_2^2)(b_1+b_2^2).$$\n",
    "\n",
    "10. Consider the linear fractional transformation \n",
    "\n",
    "$$w=\\frac{a_1z+b_1}{c_1z+d_1}=T_1(z)$$\n",
    "\n",
    "If $T_2(z)$ is a similar transformation, with coefficients $a_1,b_1,c_1,d_1$ replaced by $a_2,b_2,c_2,d_2$, show that $T_1(T_2(z))$ and $T_2(T_1(z))$ are again transformations of the same type.\n",
    "\n",
    "- I will just show the first case as the latter follows with the exact same argument in mind:\n",
    "$$T_1(T_2(z))=T_1\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)=\\frac{a\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)+b_1}{c_1\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)+d_1}=\\frac{(a_1a_2+b_1c_2)z+(a_1b_2+b_1d_2)}{(a_2c_1+c_2d_1)z+(b_2c_1+d_1d_2)}.$$\n",
    "\n",
    "11. If the expression $a_1d_1-b_1c_1\\neq 0$, show that $T_1^{-1}(z)$ is a transformation of the same type. If $a_1d_1-b_1c_1=0$, what type of a transformation is $T_1(z)$?\n",
    "\n",
    "- Not solved\n",
    "\n",
    "12. Consider a correspondence between $T_1(z)$ and the matrix \n",
    "\n",
    "$$A_1=\\begin{bmatrix}a_1&b_1\\\\c_1&d_1\\end{bmatrix}$$\n",
    "\n",
    "written $A_1\\sim T_1(z)$. Show that if $A_1\\sim T_1(z)$ and $A_2\\sim T_2(z)$, then $A_1A_2\\sim T_1(T_2(z))$. What then is the condition that $T_1(T_2(z))=T_2(T_1(z))$ for all $z$?\n",
    "\n",
    "- If we let $A_1=\\begin{bmatrix}a_1&b_1\\\\c_1&d_1\\end{bmatrix}$ and $A_2=\\begin{bmatrix}a_2&b_2\\\\c_2&d_2\\end{bmatrix}$, then $A_1A_2=\\begin{bmatrix} a_1a_2+b_1c_2&a_1b_2+b_1d_2\\\\a_2c_1+c_2d_1&b_2c_1+d_1d_2\\end{bmatrix}$. From problem 10, we showed that\n",
    "\n",
    "$$T_1(T_2(z))=\\frac{(a_1a_2+b_1c_2)z+(a_1b_2+b_1d_2)}{(a_2c_1+c_2d_1)z+(b_2c_1+d_1d_2)},$$\n",
    "\n",
    "and expressed in similar correspondence, this is equivalent to $A_1A_2$. Thus our condition is that $A_1A_2=A_2A_1$.\n",
    "\n",
    "14. Show that \n",
    "\n",
    "$$\\begin{bmatrix} -1&-1\\\\1&0\\end{bmatrix}^3=I$$\n",
    "\n",
    "- The matrix represents a $120^\\circ$ rotation. Thus if we apply the transformation three times, we end up in the same spot.\n",
    "\n",
    "### 10. Noncommutativity\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 11. Associativity\n",
    "3. Show that $A^m$ and $B^n$ commute if $A$ and $B$ commute.\n",
    "\n",
    "- If $AB=BA$, then $A^mB^n=A\\cdots A\\times B\\cdots B=A\\cdots B\\times A\\cdots B$ and iteratively, we continue this procedure to prove commutativity of the matrices raised to some power.\n",
    "\n",
    "4. Write $$X^n=\\begin{bmatrix}x_1(n)&x_2(n)\\\\x_3(n)&x_4(n)\\end{bmatrix}\\;\\;\\;\\;\\text{where}\\;\\;\\;\\; X=\\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix}$$ is a given matrix. Using the relation $X^{n+1}=XX^n$, derive recurrence relations for the $x_i(n)$ and thus derive the analytic form of the $x_i(n)$.\n",
    "\n",
    "- $X^{n+1}=XX^n=\\begin{bmatrix}x_1(n)&x_2(n)\\\\x_3(n)&x_4(n)\\end{bmatrix}\\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix}=\\begin{bmatrix}x_1x_1(n)+x_3x_2(n)&x_2x_1(n)+x_4x_2(n)\\\\x_1x_3(n)+x_3x_4(n)&x_2x_3(n)+x_4x_4(n)\\end{bmatrix}$ which form our respective recurrence relations for the system. As for the analytic form, I believe we can leave the expression in matrix form (i.e. $X^n$), but for explicit solutions, we know $x_i(n+1)=x_i(0)^n+\\text{cross-terms}$ but not sure how useful that is.\n",
    "\n",
    "6. Use these relations to find explicit representations for the elements of $$X^n$$ where \n",
    "\n",
    "$$X=\\begin{bmatrix}\\lambda&1\\\\0&\\lambda\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\; X=\\begin{bmatrix}\\lambda&1&0\\\\0&\\lambda & 1\\\\0&0&\\lambda\\end{bmatrix}$$\n",
    "\n",
    "- For the 2x2 matrix, we have that \n",
    "\n",
    "$$\\begin{bmatrix}\\lambda&1\\\\0&\\lambda\\end{bmatrix}^3=\\begin{bmatrix}\\lambda^3&3\\lambda^2\\\\0&\\lambda^3\\end{bmatrix}$$ \n",
    "\n",
    "and proceeding by induction or inspection without using the earlier recurrence relations, we see that \n",
    "\n",
    "$$X^n=\\begin{bmatrix}\\lambda^n&n\\lambda^{n-1}\\\\0&\\lambda^n\\end{bmatrix}.$$\n",
    "\n",
    "As for the 3x3 matrix, we can similary proceed by first observing the first few powers and continuing with induction - that is\n",
    "\n",
    "$$X^n=\\begin{bmatrix}\\lambda^n&n\\lambda^{n-1}&\\frac{n(n-1)}{2}\\lambda^{n-2}\\\\0&\\lambda^n&n\\lambda^{n-1}\\\\0&0&\\lambda^n\\end{bmatrix}$$\n",
    "\n",
    "7. Find all 2x2 solutions of $X^2=X.$\n",
    "\n",
    "- This can be solved by algebra\n",
    "\n",
    "$$\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}^2=\\begin{bmatrix}a^2+bc&ab+bd\\\\ac+cd&d^2+bc\\end{bmatrix}$$\n",
    "\n",
    "Thus the first set of solutions is simply given by the class of diagonal matrices where $b=c=0.$ The second class can be found where $a=1-d$. In this solution space, we have that $b$ and $c$ are free variables in $\\mathbb{R}$ and thus for some fixed $bc\\in\\mathbb{R}$, we can find the diagonal entries as follows: $$d^2+bc=d\\iff d^2-d+bc=0\\iff d=\\frac{1\\pm\\sqrt{1-4bc}}{2}$$ and thus $$a=\\frac{1\\mp\\sqrt{1-4bc}}{2}.$$\n",
    "\n",
    "8. Find all 2x2 solutions of $X^n=X$.\n",
    "\n",
    "- This solution is a bit more difficult to derive - I believe we can use the binomial theorem to expand this out however we can not end up with a nice description of the class of solutions as we were able to derive above. \n",
    "\n",
    "### 12. Invariant Vectors\n",
    "Skipped - no exercises.\n",
    "\n",
    "### 13. Quadratic Forms as Inner Products\n",
    "\n",
    "1. Does $(x,Ax)=(x,Bx)$ for all $x$ imply $A=B$?\n",
    "\n",
    "- If for all $x$, we have \n",
    "\n",
    "$$(x,Ax)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j=\\sum\\limits_{i,j=1}^N b_{ij}x_ix_j=(x,Bx),$$ then the intuitive justifiication is that if under every vector in the space, we have the two matrices/transformations map to the same point, then it is necessarily the case that the two transformations are equivalent. I'm not sure how to carry out this same justification rigorously to be honest.\n",
    "\n",
    "2. Under what conditions does $(x,Ax)=0$ for all $x$?\n",
    "\n",
    "- This is only the case for the zero matrix - where every entry is defined by zero. This can be taken for granted if we are working with the real or complex field of numbers.\n",
    "\n",
    "### 14. The Transpose Matrix\n",
    "Skipped - no exercises.\n",
    "\n",
    "### 15. Symmetric Matrices\n",
    "\n",
    "1. Show that $(A^T)^T=A$.\n",
    "\n",
    "- As $A^T=\\sum\\limits_{i,j=1}^N a_{ij}$, then clearly $(A^T)^T=\\sum\\limits_{i,j=1}^N a_{ij}=A$.\n",
    "\n",
    "2. Show that $(A+B)^T=A^T+B^T$, $(AB^T)=B^TA^T$, $(A_1A_2\\cdots A_n)^T=A_n^T\\cdots A_2^TA_1^T$.\n",
    "\n",
    "- $(A+B)_{ij}=a_{ij}+b_{ij}\\Rightarrow (A+B)^T_{ij}=a_{ji}+b_{ji}$, so $(A+B)^T=A^T+B^T$. For the second equality to show, we have that $$(B^TA^T)_{ij}=\\sum\\limits_{k=1}^N (B^T)_{ik}(A^T)_{kj}=\\sum\\limits_{k=1}^N B_{kj}A_{jk}=(AB)_{ji}=\\left((AB)^T\\right)_{ij}.$$ For the third, let $B=(A_1\\cdots A_{n-1})$, so then from the second equality, $(BA_n)^T=A_n^TB^T$ and we can iteratively repeat this procedure up till $B=A_1$ to arrive at the conclusion. As for the fourth equality, we trivially have after lettting $A_1=\\cdots A_n=A$, that $(A^n)^T=(A^T)^n$.\n",
    "\n",
    "3. Show that $AB$ is not necessarily symmetric if $A$ and $B$ are. \n",
    "\n",
    "- Let $A=\\begin{bmatrix}-1&0\\\\0&0\\end{bmatrix}$ and $B=\\begin{bmatrix}1&-1\\\\-1&1\\end{bmatrix}$, so $AB=\\begin{bmatrix}-1&1\\\\0&0\\end{bmatrix}$ which is no longer symmetric.\n",
    "\n",
    "4. Show that $A^TBA$ is symmetric if $B$ is symmetric.\n",
    "\n",
    "- Let $Y=A^TBA$, then $$Y_{ij}=\\sum\\limits_{k=1}^N (A^T)_{ik}(BA)_{kj}=\\sum\\limits_{k=1}^N A_{ji}\\left(\\sum\\limits_{l=1}^N B_{kl}A_{lj}\\right)$$ while $$(Y^T)_{ji}=A^TB^TA_{ij}=\\sum\\limits_{k=1}^N (A^T)_{ik}(B^TA)_{kj}=\\sum\\limits_{k=1}^N (A^T)_{ik}\\left(\\sum\\limits_{l=1}^N (B^T)_{kl}A_{lj}\\right)$$ where we see the two are equivalent as $(B^T)_{ij}=B_{ij}$. The implication of this result is great in statistics where we often are dealing with dispersion or covariance matrices (which are symmetric generally) and have scalers or noise inputted in. \n",
    "\n",
    "5. Show that $(Ax,By)=(x,A^TBy)$\n",
    "\n",
    "- We can just apply the definition to get the result alongside the fact that we can freely interchange the finite sums - $$(x,A^TBy)=\\sum\\limits_{i=1}^N x_i\\left(\\sum\\limits_{j=1}^N (A^TB)_{ij}y_j\\right)=\\sum\\limits_{i=1}^N x_i\\left(\\sum\\limits_{j=1}^N\\left(\\sum\\limits_{k=1}^N A_{ki}B_{kj}\\right)y_j\\right)$$\n",
    "\n",
    "$$=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N \\sum\\limits_{k=1}^N (A_{ki}x_i)(B_{kj}y_j)=\\sum\\limits_{k=1}^N \\left(\\sum\\limits_{i=1}^N A_{ki}x_i\\right)\\left(B_{kj}y_j\\right)=\\sum\\limits_{k=1}^N (Ax)_k(By)_k=(Ax,By)$$\n",
    "\n",
    "6. Show that $\\det A=\\det A^T$.\n",
    "\n",
    "- I will first provide an intuitve justification here and dive more into the details after covering \"Linear Algebra\" by Hoffman and Kunze. The intuitive justification is through the geometric definition of the determinant - the volume of a parallelpiped formed by the column vectors. From this definition, you can intuitively see that if the matrix is square, then these will preserve the same mass of the parallelpiped. At a slightly more computational level, you can see this as cofactor expansion to compute the determinant does not matter if I traverse through the rows or columns - of course if you know the proof of this theorem, then you can use it. I'll cover proof of the latter and a broader generalization of the definition taught in first course linear algebra above.\n",
    "\n",
    "7. Show that when we write $Q(x)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j$, there is no loss of generality in assuming that $a_{ij}=a_{ji}$.\n",
    "\n",
    "- This question is masked a bit on the outside but still very interesting. It is really asking for the reader to show that for any real quadratic form, the matrix $A$ is symmetric. To realize this, it boils down to an age old linear algebra trick which I will derive here. For any $N\\times N$ matrix $A$, we can express $A$ as the sum of it's symmetric and non-symmetric parts - $$A=\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T).$$ The first weighted matrix is symmetric trivially - $$\\left(A+A^T\\right)^T=A^T+A=A+A^T$$ while the second matrix is not - $$\\left(A-A^T\\right)^T=A^T-A=-(A-A^T).$$ Now returning to the problem at hand, we have that $$Q(x)=(x,Ax)=\\left(x,\\left(\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T)\\right)x\\right)=\\left(x,\\frac{1}{2}(A+A^T)x\\right)+\\left(x,\\frac{1}{2}(A-A^T)x\\right)$$\n",
    "\n",
    "$$=\\left(x,\\frac{1}{2}(A+A^T)x\\right)$$ since $$Q^*(x)=\\left(x,\\frac{1}{2}(A-A^T)x\\right)=\\left(x,-\\frac{1}{2}(A^T-A)^T x\\right)=-\\left(x,\\frac{1}{2}(A-A^T)\\right)=-Q^*(x)\\iff Q^*(x)=0.$$\n",
    "\n",
    "which finally proves the statement in full breadth.\n",
    "\n",
    "### 16. Hermitian Matrices\n",
    "\n",
    "1. A real Hermitian matrix is symmetric. \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557a45d",
   "metadata": {},
   "source": [
    "# Some Competition Linear Algebra Problems & Answers \n",
    "\n",
    "Here are an assortment of easy competition problems - my solutions are not complete, but sufficient if we are talking about statistics for the sake of my sanity with latex.\n",
    "\n",
    "1. Putnam 2008 Problem A2: Alan and Barbara play a game in which they take turns filling entries of an initially empty $2008\\times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy?\n",
    "\n",
    "- There is a very neat solution/optimal strategy for Barbara to win every time. If Alan fills in a real number $x$ on the $a_{ij}$ entry, Barbara can fill in the same real number x in the $a_{ij+1}$ or $a_{ij-1}$ entry (sticking to either adjacent rows or columns for the entire game). Thus in the end, there will always be at least one pairing of identical columns. Barbara could also have done this pairing strategy horizontally which also ensures the determinant of the resultant matrix is $0$.\n",
    "\n",
    "2. IMC 2005 Day 1 Problem 1: Let $A$ be an $n\\times n$ matrix such that $A_{ij}=i+j$. Find the rank of $A$.\n",
    "\n",
    "- For $n=1$, we have that the rank is 1. For $n\\geq 2$, we can do the following: take the first row and subtract it from each of the subsequent rows. Then the second through $n$th row are all increasing multiples of each other. Hence we have just two rows that make up the rank for any matrix larger than $1\\times 1$.\n",
    "\n",
    "3. Putnam 2011 Problem A4: For which positive integers n is there an n\\times n matrix with integer entries such that every dot product of a row with itself is even, while every dot product of two different rows is odd?\n",
    "\n",
    "- WLOG, we can just inspect matrices with entries in $\\mathbb{Z}/2\\mathbb{Z}$ as we are only concerned with whether or not the dot product is even/odd. For odd matrices, it is easy to see that if we consider a matrix with 0s along the diagonal and 1s everywhere else, then what we have is that each row as an even n-1 number of 1s whereas when we take the dot product with another row, we will always have 2 cases of $0*1$ and the rest are $1*1 – n-2$ is odd, so this works. This generalizes for all odd $n$, so now we turn attention to even $n$ where this simple argument doesn’t extend nicely. Instead, we can use a bit of linear algebra. If we have row vectors $V_1,\\dots,V_n$, our problem is characterized by trying to show that $\\forall i,\\;j\\in\\{1,\\dots,n\\}$: $i\\neq j,\\;V_i\\cdot V_j=1$ and $V_i\\cdot V_i=0$. As the dot product of a vector with itself is even, we must have that the matrix is not invertible. But, there’s a contradiction – left up to the reader 🙂\n",
    "\n",
    "## Other Comp Problems \n",
    "- https://artofproblemsolving.com/community - this is a goat resource to improve, look at contest, high school olympiad and college math forums.\n",
    "- https://href.li/?http://www.math.utoronto.ca/barbeau/putnamla.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38408ed9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projections-onto-subspaces/MIT18_06SCF11_Ses2.2sum.pdf\n",
    "- Introduction to Matrix Analysis by Richard Bellman\n",
    "- Linear Statistical Inference and Its Applications by C.R Rao\n",
    "- Linear Algebra by Hoffman and Kunze\n",
    "- Mathematics for Machine Learning by Deisenroth et al."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
