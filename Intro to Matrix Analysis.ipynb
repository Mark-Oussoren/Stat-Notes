{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a39d7d",
   "metadata": {},
   "source": [
    "# Exposition\n",
    "\n",
    "The goal of this set of notes is to understand linear algebra as a whole better. The utility and applications of the subject is lucid, but not many really understand what the heck is going on. My goal is to do exactly that - find out what's going on with enough depth to traverse through topics in ML and high dimensional statistics with ease. To do so, I will be solving all the linear algebra questions from Bellman's \"Introduction to Matrix Analysis\", revisiting material from \"Linear Algebra\" by Hoffman & Kunze, as well as going through Rao's chapter on vector spaces specific to statistics. As always, the layout is notes, followed by exercises, followed by sources if you hate my writing. \n",
    "\n",
    "The entries of a vector belong to a **field**. A field  $\\mathcal{F}$ is a set that satisfies the following axioms. For $x,y,z\\in\\mathcal{F}$,\n",
    "\n",
    "a) $x+y=y+x$ and $xy=yx$ (commutative under addition and multiplication)\n",
    "\n",
    "b) $(x+y)+z=x+(y+z)$ and $(xy)z=x(yz)$ (associative under addition and multiplication)\n",
    "\n",
    "c) For all $a\\in\\mathcal{F}\\setminus \\{0\\}$, there are elements $b$ and $c\\neq 0$ such that $a+b=0$ and $ac=1$ (existence of additive and multiplicative inverse)\n",
    "\n",
    "Usually we take fields for granted, but they are extremely important when manipulating systems. Common fields used include the reals $\\mathbb{R}$, the complex numbers $\\mathbb{C}$, and the rationals $\\mathbb{Q}$. Nonexamples include the integers (no multiplicative inverse of course, the binary set $\\{0,1\\}$, the set of prime numbers (again existence of inverses is violated). \n",
    "\n",
    "For a field $\\mathcal{F}$, we can now define a $n$-dimensional row vector as a collection of $n$ elements taken from $\\mathcal{F}$. For instance, in space we may consider a space shuttle's location as represented conveniently by a vector - the $(x,y,z)$ coordinates taken from the rationals. Just like our field, vectors while they can make sense without imposing any rules or limitations, are much more useful to us and developed in theory under the following axioms. These following axioms define what is called a **linear vector space**. A linear vector space $\\mathcal{X}$ on some field $\\mathcal{F}$ satisfies the following for elements $x,y\\in\\mathcal{X}$ and $a,b\\in\\mathcal{F}$:\n",
    "\n",
    "a) $x+y=y+x$ (commutative under addition)\n",
    "\n",
    "b) $(x+y)+z=x+(y+z)$ and $a(bx)=b(ax)$ (associative under addition and scalar multiplication)\n",
    "\n",
    "c) $(a+b)(x+y)=ax+ay+bx+by$ (distributive law for vectors and scalar multiplication)\n",
    "\n",
    "d) There is a unique $z\\in\\mathcal{F}$ such that $x+z=y$.\n",
    "\n",
    "Examples of linear vector spaces include the $k$-tuples of real numbers $(x_1,\\dots,x_k)$ included in so many applications, the set of polynomials of degree $n$ - $a_1x^n+a_2x^{n-1}+\\cdots +a_nx+a_{n+1}$ which pops up in regression and density estimation, along with the trivial vector space - $\\{0\\}$ which shows up nowhere outside of a math textbook (this is a called a trivial/improper vector space).\n",
    "\n",
    "A **linear subspace** of a vector space $\\mathcal{X}$ is a subset of vectors $\\mathcal{A}$ closed under vector addition and scalar multiplication. By closed, I mean that the sum of vectors is still in the subspace and the scalar multiples of all vectors are still vectors in the subspace. **TODO** - introduce concept of bases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe017e",
   "metadata": {},
   "source": [
    "# Exercises from \"Introduction to Matrix Analysis\" by Richard Bellman\n",
    "\n",
    "In Bellman's words: \"Together with working out a number of the exercises, [Chapters 1-5 and Chapters 10-11] should cover a semester course at an undergraduate senior, or first-year graduate level.\" An impressive feature of the author (Bellman), is that he wrote it (and 100+ papers) after having a brain tumor removed that left him severely disabled.\n",
    "\n",
    "## 2 - Vectors and Matrices\n",
    "\n",
    "### 3. Vector Addition\n",
    "\n",
    "**1.** Show that we have commutativity, $x+y=y+x$, and associativity, $x+(y+z)=(x+y)+z$.\n",
    "\n",
    "- I assume our matrices are either integer, rational, real, or complex. In any of these cases, it trivially follows that since $x+y=y+x$ for $x,y\\in\\mathcal{F}$ that the same holds if we perform the addition elementwise as done in vector addition. On a similar note, we know these fields of numbers are associative, so associativity is also an inherent condition. \n",
    "\n",
    "**3.** Define subtraction of two vectors directly, and in terms of addition; that is, $x-y$ is a vector $z$ such that $y+z=x$.\n",
    "\n",
    "- Let $z=x+(-y)$, so $y+(x+(-y))=x+(y+(-y))=x$ by associativity.\n",
    "\n",
    "### 4. Scalar Multiplication\n",
    "\n",
    "**1.** Show that $(c_1+c_2)(x+y)=c_1x+c_1y+c_2x+c_2y$.\n",
    "\n",
    "- $(c_1+c_2)(x+y)=(c_1+c_2)x+(c_1+c_2)y=c_1x+c_2x+c_1y+c_2y$.\n",
    "\n",
    "**2.** Define the null vector, written 0, to be the vector all of whose components are zero. Show that it is uniquely determined as the vector 0 for which $x+0=x$ for all $x$.\n",
    "\n",
    "- Again, this question stems from our understanding of the field of numbers we are working with. Clearly $x+0=x$ for $x\\in\\mathbb{R}$ and this concept extends for each entry as we denote the zero vector by $n$ zeros in a column vector.\n",
    "\n",
    "### 5. The Inner Product of Two Vectors\n",
    "\n",
    "**1.** If $x$ is real, show that $(x,x)>0$ unless $x$ is $0$.\n",
    "\n",
    "- If $x\\neq 0$, $(x,x)=\\sum\\limits_{i=1}^n x_i^2>0$ for some component $x_i\\neq 0$ in $x$.\n",
    "\n",
    "**2.** Show that $(ux+vy,ux+vy)=u^2(x,x)+2uv(x,y)+v^2(y,y)$ is a non-negative definite quadratic form in the scalar variables $u$ and $v$ if $x$ and $y$ are real and hence that \n",
    "$$(x,y)^2\\leq (x,x)(y,y)$$\n",
    "for any two real vectors $x$ and $y$ (Cauchy's inequality).\n",
    "\n",
    "- $(ux+vy,ux+vy)=\\sum\\limits_{i=1}^n (ux_i+vy_i)^2$. This quantity is clearly non-negative provided the field of numbers is real or rational. To show Cauchy's inequality, we know $(ux+vy)(ux+vy)=u^2(x,x)+2uv(x,y)+v^2(y,y)$ by expanding out the inner product. If we consider $u=-\\sqrt{(y,y)}$ and $v=\\sqrt{(x,x)}$, then the above becomes\n",
    "\n",
    "$$\\left(-\\sqrt{y,y}\\right)^2(x,x)-2\\sqrt{(y,y)(x,x)}(x,y)+\\left(\\sqrt{x,x}\\right)^2(y,y)\\geq 0\\iff 2(x,x)(y,y)\\geq 2\\sqrt{(y,y)(x,x)}(x,y)$$\n",
    "\n",
    "$$\\iff (x,x)^2(y,y)^2\\geq (x,x)(y,y)(x,y)^2\\iff (x,x)(y,y)\\geq (x,y)^2.$$\n",
    "\n",
    "**3.** What is the geometric interpretation of this result?\n",
    "\n",
    "- The projection of a vector onto another does not have longer length than the vector itself.\n",
    "\n",
    "**4.** Using this result, show that for real vectors $x$ and $y$ that\n",
    "\n",
    "$$(x+y,x+y)^{1/2}\\leq (x,x)^{1/2}(y,y)^{1/2}.$$\n",
    "\n",
    "- Using Cauchy's, $(x+y,x+y)=(x,x)+2(x,y)+(y,y)\\leq(x,x)+2\\sqrt{(x,x)(y,y)}+(y,y)=\\left((x,x)^{\\frac{1}{2}}+(y,y)^{\\frac{1}{2}}\\right)^2$. From here, we simply take the square root of both sides and call it a day. \n",
    "\n",
    "**5.** Why is this inequality called the \"triangle inequality\"?\n",
    "\n",
    "- Because in $\\mathbb{R}^2$, we have that a triangle's smallest two sides summed is greater than or equal to the hypotenuse - hence the name stemming from this geometric shape. \n",
    "\n",
    "### 6. Orthogonality\n",
    "\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 7. Matrices\n",
    "\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 8. Matrix Multiplication - Vector by Matrix\n",
    "\n",
    "**1.** Show that $(A+B)(x+y)=Ax+Ay+Bx+By$.\n",
    "\n",
    "- $(A+B)(x+y)=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N (a_{ij}+b_{ij})(x_j+y_j)=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N a_{ij}x_j+a_{ij}y_j+b{ij}x_j+b_{ij}y_j=Ax+Ay+Bx+By$.\n",
    "\n",
    "### 9. Matrix Multiplication - Matrix by Matrix.\n",
    "\n",
    "**2.** If \n",
    "\n",
    "$$T(\\theta)=\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{bmatrix}$$\n",
    "\n",
    "show that \n",
    "\n",
    "$$T(\\theta_1)T(\\theta_2)=T(\\theta_2)T(\\theta_1)=T(\\theta_1+\\theta_2).$$\n",
    "\n",
    "- Using the identity $\\cos\\theta_1+\\theta_2=\\cos\\theta_1\\cos\\theta_2-\\sin\\theta_1\\sin\\theta_2$ and $\\sin\\theta_1+\\theta_2=\\cos\\theta_1\\sin\\theta_2+\\cos\\theta_2\\sin\\theta_1$, we can clearly expand out the linear operators to arrive at the equalities.\n",
    "\n",
    "**3.** Let $A$ be a matrix with the property that $a_{ij}=0$ if $j\\neq i$, a diagonal matrix, and let $B$ be a matrix of similar type. Show that $AB$ is again a diagonal matrix, and that $AB=BA$.\n",
    "\n",
    "- Let $C=AB$. $c_{ij}=\\sum\\limits_{k=1}^n a_{ik}b_{kj}=a_{ik}b_{kj}=a_{ii}b_{ii}\\iff i=j$ otherwise the sum is zero. Thus the matrix is diagonal. Similarly, letting $D=BA$ we obtain $d_{ij}=\\sum\\limits_{k=1}^n b_{ij}a_{jk}=b_{ik}a_{kj}=b_{ii}a_{ii}\\iff i=j$ otherwise the sum is zero. As The field is commutative, interchanging these elements is allowed and so $C=D$.\n",
    "\n",
    "**4.** Let $A$ be a matrix with the property that $a_{ij}=0$, $j>i$, a triangular or semi-diagonal matrix, and let $B$ be a matrix of similar type. Show that $AB$ is again a triangular matrix, but that $AB\\neq BA$ in general.\n",
    "\n",
    "- Let $C=AB$ so $c_{ij}=\\sum\\limits_{i=1}^n a_{ik}b_{kj}=0$ if $k>i$ or $j>k$ which is equivalent to saying $j>i$ which implies the matrix is triangular. If $A=\\begin{bmatrix}a&0\\\\b&c\\end{bmatrix}$ and $B=\\begin{bmatrix}x&0\\\\y&z\\end{bmatrix}$, $AB\\neq BA$ if we ensure $bx+cy\\neq ay+bz$.\n",
    "\n",
    "**6.** Use the relation $|AB|=|A||B|$ to show that \n",
    "\n",
    "$$(a_1^2+a_2^2)(b_1^2+b_2^2)=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2$$\n",
    "\n",
    "- If we let $A=\\begin{bmatrix}a_1&a_2\\\\-a_2&a_1\\end{bmatrix}$ and $B=\\begin{bmatrix}b_1&b_2\\\\-b_2&b_1\\end{bmatrix}$, then\n",
    "\n",
    "$$|AB|=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2=|A||B|=(a_1^2+a_2^2)(b_1+b_2^2).$$\n",
    "\n",
    "**10.** Consider the linear fractional transformation \n",
    "\n",
    "$$w=\\frac{a_1z+b_1}{c_1z+d_1}=T_1(z)$$\n",
    "\n",
    "If $T_2(z)$ is a similar transformation, with coefficients $a_1,b_1,c_1,d_1$ replaced by $a_2,b_2,c_2,d_2$, show that $T_1(T_2(z))$ and $T_2(T_1(z))$ are again transformations of the same type.\n",
    "\n",
    "- I will just show the first case as the latter follows with the exact same argument in mind:\n",
    "\n",
    "$$T_1(T_2(z))=T_1\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)=\\frac{a\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)+b_1}{c_1\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)+d_1}=\\frac{(a_1a_2+b_1c_2)z+(a_1b_2+b_1d_2)}{(a_2c_1+c_2d_1)z+(b_2c_1+d_1d_2)}.$$\n",
    "\n",
    "**11.** If the expression $a_1d_1-b_1c_1\\neq 0$, show that $T_1^{-1}(z)$ is a transformation of the same type. If $a_1d_1-b_1c_1=0$, what type of a transformation is $T_1(z)$?\n",
    "\n",
    "- Not solved\n",
    "\n",
    "**12.** Consider a correspondence between $T_1(z)$ and the matrix \n",
    "\n",
    "$$A_1=\\begin{bmatrix}a_1&b_1\\\\c_1&d_1\\end{bmatrix}$$\n",
    "\n",
    "written $A_1\\sim T_1(z)$. Show that if $A_1\\sim T_1(z)$ and $A_2\\sim T_2(z)$, then $A_1A_2\\sim T_1(T_2(z))$. What then is the condition that $T_1(T_2(z))=T_2(T_1(z))$ for all $z$?\n",
    "\n",
    "- If we let $A_1=\\begin{bmatrix}a_1&b_1\\\\c_1&d_1\\end{bmatrix}$ and $A_2=\\begin{bmatrix}a_2&b_2\\\\c_2&d_2\\end{bmatrix}$, then $A_1A_2=\\begin{bmatrix} a_1a_2+b_1c_2&a_1b_2+b_1d_2\\\\a_2c_1+c_2d_1&b_2c_1+d_1d_2\\end{bmatrix}$. From problem 10, we showed that\n",
    "\n",
    "$$T_1(T_2(z))=\\frac{(a_1a_2+b_1c_2)z+(a_1b_2+b_1d_2)}{(a_2c_1+c_2d_1)z+(b_2c_1+d_1d_2)},$$\n",
    "\n",
    "and expressed in similar correspondence, this is equivalent to $A_1A_2$. Thus our condition is that $A_1A_2=A_2A_1$.\n",
    "\n",
    "**14.** Show that \n",
    "\n",
    "$$\\begin{bmatrix} -1&-1\\\\1&0\\end{bmatrix}^3=I$$\n",
    "\n",
    "- The matrix represents a $120^\\circ$ rotation. Thus if we apply the transformation three times, we end up in the same spot.\n",
    "\n",
    "### 10. Noncommutativity\n",
    "\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 11. Associativity\n",
    "\n",
    "**3.** Show that $A^m$ and $B^n$ commute if $A$ and $B$ commute.\n",
    "\n",
    "- If $AB=BA$, then $A^mB^n=A\\cdots A\\times B\\cdots B=A\\cdots B\\times A\\cdots B$ and iteratively, we continue this procedure to prove commutativity of the matrices raised to some power.\n",
    "\n",
    "**4.** Write \n",
    "\n",
    "$$X^n=\\begin{bmatrix}x_1(n)&x_2(n)\\\\x_3(n)&x_4(n)\\end{bmatrix}\\;\\;\\;\\;\\text{where}\\;\\;\\;\\; X=\\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix}$$\n",
    "\n",
    "is a given matrix. Using the relation $X^{n+1}=XX^n$, derive recurrence relations for the $x_i(n)$ and thus derive the analytic form of the $x_i(n)$.\n",
    "\n",
    "- $X^{n+1}=XX^n=\\begin{bmatrix}x_1(n)&x_2(n)\\\\x_3(n)&x_4(n)\\end{bmatrix}\\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix}=\\begin{bmatrix}x_1x_1(n)+x_3x_2(n)&x_2x_1(n)+x_4x_2(n)\\\\x_1x_3(n)+x_3x_4(n)&x_2x_3(n)+x_4x_4(n)\\end{bmatrix}$ which form our respective recurrence relations for the system. As for the analytic form, I believe we can leave the expression in matrix form (i.e. $X^n$), but for explicit solutions, we know $x_i(n+1)=x_i(0)^n+\\text{cross-terms}$ but not sure how useful that is.\n",
    "\n",
    "**6.** Use these relations to find explicit representations for the elements of $$X^n$$ where \n",
    "\n",
    "$$X=\\begin{bmatrix}\\lambda&1\\\\0&\\lambda\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\; X=\\begin{bmatrix}\\lambda&1&0\\\\0&\\lambda & 1\\\\0&0&\\lambda\\end{bmatrix}$$\n",
    "\n",
    "- For the 2x2 matrix, we have that \n",
    "\n",
    "$$\\begin{bmatrix}\\lambda&1\\\\0&\\lambda\\end{bmatrix}^3=\\begin{bmatrix}\\lambda^3&3\\lambda^2\\\\0&\\lambda^3\\end{bmatrix}$$ \n",
    "\n",
    "and proceeding by induction or inspection without using the earlier recurrence relations, we see that \n",
    "\n",
    "$$X^n=\\begin{bmatrix}\\lambda^n&n\\lambda^{n-1}\\\\0&\\lambda^n\\end{bmatrix}.$$\n",
    "\n",
    "As for the 3x3 matrix, we can similary proceed by first observing the first few powers and continuing with induction - that is\n",
    "\n",
    "$$X^n=\\begin{bmatrix}\\lambda^n&n\\lambda^{n-1}&\\frac{n(n-1)}{2}\\lambda^{n-2}\\\\0&\\lambda^n&n\\lambda^{n-1}\\\\0&0&\\lambda^n\\end{bmatrix}$$\n",
    "\n",
    "**7.** Find all 2x2 solutions of $X^2=X.$\n",
    "\n",
    "- This can be solved by algebra\n",
    "\n",
    "$$\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}^2=\\begin{bmatrix}a^2+bc&ab+bd\\\\ac+cd&d^2+bc\\end{bmatrix}$$\n",
    "\n",
    "Thus the first set of solutions is simply given by the class of diagonal matrices where $b=c=0.$ The second class can be found where $a=1-d$. In this solution space, we have that $b$ and $c$ are free variables in $\\mathbb{R}$ and thus for some fixed $bc\\in\\mathbb{R}$, we can find the diagonal entries as follows: \n",
    "\n",
    "$$d^2+bc=d\\iff d^2-d+bc=0\\iff d=\\frac{1\\pm\\sqrt{1-4bc}}{2}$$\n",
    "\n",
    "and thus \n",
    "\n",
    "$$a=\\frac{1\\mp\\sqrt{1-4bc}}{2}.$$\n",
    "\n",
    "**8.** Find all 2x2 solutions of $X^n=X$.\n",
    "\n",
    "- This solution is a bit more difficult to derive - I believe we can use the binomial theorem to expand this out however we can not end up with a nice description of the class of solutions as we were able to derive above. \n",
    "\n",
    "### 12. Invariant Vectors\n",
    "\n",
    "Skipped - no exercises\n",
    "\n",
    "### 13. Quadratic Forms as Inner Products\n",
    "\n",
    "**1.** Does $(x,Ax)=(x,Bx)$ for all $x$ imply $A=B$?\n",
    "\n",
    "- If for all $x$, we have \n",
    "\n",
    "$$(x,Ax)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j=\\sum\\limits_{i,j=1}^N b_{ij}x_ix_j=(x,Bx),$$ then the intuitive justifiication is that if under every vector in the space, we have the two matrices/transformations map to the same point, then it is necessarily the case that the two transformations are equivalent. I'm not sure how to carry out this same justification rigorously to be honest.\n",
    "\n",
    "**2.** Under what conditions does $(x,Ax)=0$ for all $x$?\n",
    "\n",
    "- This is only the case for the zero matrix - where every entry is defined by zero. This can be taken for granted if we are working with the real or complex field of numbers.\n",
    "\n",
    "### 14. The Transpose Matrix\n",
    "\n",
    "Skipped - no exercises.\n",
    "\n",
    "### 15. Symmetric Matrices\n",
    "\n",
    "**1.** Show that $(A^T)^T=A$.\n",
    "\n",
    "- As $A^T=\\sum\\limits_{i,j=1}^N a_{ij}$, then clearly $(A^T)^T=\\sum\\limits_{i,j=1}^N a_{ij}=A$.\n",
    "\n",
    "**2.** Show that $(A+B)^T=A^T+B^T$, $(AB^T)=B^TA^T$, $(A_1A_2\\cdots A_n)^T=A_n^T\\cdots A_2^TA_1^T$.\n",
    "\n",
    "- $(A+B)_{ij}=a_{ij}+b_{ij}\\Rightarrow (A+B)^T_{ij}=a_{ji}+b_{ji}$, so $(A+B)^T=A^T+B^T$. For the second equality to show, we have that \n",
    "\n",
    "$$(B^TA^T)_{ij}=\\sum\\limits_{k=1}^N (B^T)_{ik}(A^T)_{kj}=\\sum\\limits_{k=1}^N B_{kj}A_{jk}=(AB)_{ji}=\\left((AB)^T\\right)_{ij}.$$\n",
    "\n",
    "For the third, let $B=(A_1\\cdots A_{n-1})$, so then from the second equality, $(BA_n)^T=A_n^TB^T$ and we can iteratively repeat this procedure up till $B=A_1$ to arrive at the conclusion. As for the fourth equality, we trivially have after lettting $A_1=\\cdots A_n=A$, that $(A^n)^T=(A^T)^n$.\n",
    "\n",
    "**3.** Show that $AB$ is not necessarily symmetric if $A$ and $B$ are. \n",
    "\n",
    "- Let $A=\\begin{bmatrix}-1&0\\\\0&0\\end{bmatrix}$ and $B=\\begin{bmatrix}1&-1\\\\-1&1\\end{bmatrix}$, so $AB=\\begin{bmatrix}-1&1\\\\0&0\\end{bmatrix}$ which is no longer symmetric.\n",
    "\n",
    "**4.** Show that $A^TBA$ is symmetric if $B$ is symmetric.\n",
    "\n",
    "- Let $Y=A^TBA$, then \n",
    "\n",
    "$$Y_{ij}=\\sum\\limits_{k=1}^N (A^T)_{ik}(BA)_{kj}=\\sum\\limits_{k=1}^N A_{ji}\\left(\\sum\\limits_{l=1}^N B_{kl}A_{lj}\\right)$$\n",
    "\n",
    "while \n",
    "\n",
    "$$(Y^T)_{ji}=A^TB^TA_{ij}=\\sum\\limits_{k=1}^N (A^T)_{ik}(B^TA)_{kj}=\\sum\\limits_{k=1}^N (A^T)_{ik}\\left(\\sum\\limits_{l=1}^N (B^T)_{kl}A_{lj}\\right)$$\n",
    "\n",
    "where we see the two are equivalent as $(B^T)_{ij}=B_{ij}$. The implication of this result is great in statistics where we often are dealing with dispersion or covariance matrices (which are symmetric generally) and have scalers or noise inputted in. \n",
    "\n",
    "**5.** Show that $(Ax,By)=(x,A^TBy)$\n",
    "\n",
    "- We can just apply the definition to get the result alongside the fact that we can freely interchange the finite sums - \n",
    "\n",
    "$$(x,A^TBy)=\\sum\\limits_{i=1}^N x_i\\left(\\sum\\limits_{j=1}^N (A^TB)_{ij}y_j\\right)=\\sum\\limits_{i=1}^N x_i\\left(\\sum\\limits_{j=1}^N\\left(\\sum\\limits_{k=1}^N A_{ki}B_{kj}\\right)y_j\\right)$$\n",
    "\n",
    "$$=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N \\sum\\limits_{k=1}^N (A_{ki}x_i)(B_{kj}y_j)=\\sum\\limits_{k=1}^N \\left(\\sum\\limits_{i=1}^N A_{ki}x_i\\right)\\left(\\sum\\limits_{j=1}^N B_{kj}y_j\\right)=\\sum\\limits_{k=1}^N (Ax)_k(By)_k=(Ax,By)$$\n",
    "\n",
    "**6.** Show that $\\det A=\\det A^T$.\n",
    "\n",
    "- I will first provide an intuitve justification here and dive more into the details after covering \"Linear Algebra\" by Hoffman and Kunze. The intuitive justification is through the geometric definition of the determinant - the volume of a parallelpiped formed by the column vectors. From this definition, you can intuitively see that if the matrix is square, then these will preserve the same mass of the parallelpiped. At a slightly more computational level, you can see this as cofactor expansion to compute the determinant does not matter if I traverse through the rows or columns - of course if you know the proof of this theorem, then you can use it. I'll cover proof of the latter and a broader generalization of the definition taught in first course linear algebra above.\n",
    "\n",
    "**7.** Show that when we write $Q(x)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j$, there is no loss of generality in assuming that $a_{ij}=a_{ji}$.\n",
    "\n",
    "- This question is masked a bit on the outside but still very interesting. It is really asking for the reader to show that for any real quadratic form, the matrix $A$ is symmetric. To realize this, it boils down to an age old linear algebra trick which I will derive here. For any $N\\times N$ matrix $A$, we can express $A$ as the sum of it's symmetric and non-symmetric parts - \n",
    "\n",
    "$$A=\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T).$$\n",
    "\n",
    "The first weighted matrix is symmetric trivially - \n",
    "\n",
    "$$\\left(A+A^T\\right)^T=A^T+A=A+A^T$$\n",
    "\n",
    "while the second matrix is not - \n",
    "\n",
    "$$\\left(A-A^T\\right)^T=A^T-A=-(A-A^T).$$ Now returning to the problem at hand, we have that $$Q(x)=(x,Ax)=\\left(x,\\left(\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T)\\right)x\\right)=\\left(x,\\frac{1}{2}(A+A^T)x\\right)+\\left(x,\\frac{1}{2}(A-A^T)x\\right)$$\n",
    "\n",
    "$$=\\left(x,\\frac{1}{2}(A+A^T)x\\right)$$ since $$Q^*(x)=\\left(x,\\frac{1}{2}(A-A^T)x\\right)=\\left(x,-\\frac{1}{2}(A^T-A)^T x\\right)=-\\left(x,\\frac{1}{2}(A-A^T)\\right)=-Q^*(x)\\iff Q^*(x)=0.$$\n",
    "\n",
    "which finally proves the statement in full breadth.\n",
    "\n",
    "### 16. Hermitian Matrices\n",
    "\n",
    "Bellman uses the notation that $\\overline{A^T}=A^*$.\n",
    "\n",
    "**1.** A real Hermitian matrix is symmetric. \n",
    "\n",
    "- This is trivially true as $\\bar{A^T}=A^T$ if $A$ is real, so $A=\\overline{A^T}=A^T$ which implies that $A$ is symmetric.\n",
    "\n",
    "**2.** Show $(A^*)^*=A$ and $(AB)^*=B^*A^*$.\n",
    "\n",
    "- Just following definition and using associativity of transpose under conjugation yields the answers.\n",
    "\n",
    "**3.** If $A+iB$ is Hermitian, $A, B$ real, then $A'=A,B'=-B$.\n",
    "\n",
    "- If $A+iB$ is Hermitian, then $A+iB=\\overline{A^T+iB^T}=A^T-iB^T\\Rightarrow A=A^T$ and $B=-B^T$.\n",
    "\n",
    "### 17. Invariance of Distance - Orthogonal Matrices\n",
    "\n",
    "Bellman defines a real matrix $T$ with the property that $T^TT=I$ as being orthogonal - this gives us a sense of what linear transformations leave the magnitude ($(x,x)=\\lVert x\\rVert_2^2 $) unchanged - i.e. they are exclusively rotating the matrix around the $N$-dimensional sphere of length $(x,x)$.\n",
    "\n",
    "**1.** Show that $T^T$ is orthogonal whenever $T$ is. \n",
    "\n",
    "- If $T$ is orthogonal, then \n",
    "\n",
    "$$0=T-T=T-TI=T-T(T^TT)=(I-TT^T)T\\iff I-TT^T=0$$\n",
    "\n",
    "since $T^TT=I$ by assumption so $T$ must have rank $N$ to begin with in order to claim the product equals the identity which has rank $N$. From here, we have that \n",
    "\n",
    "$$I-TT^T=0\\iff TT^T=I\\iff T^T \\text{ is orthogonal.}$$\n",
    "\n",
    "**2.** Show that every $2\\times 2$ orthogonal matrix with determinant $+1$ can be written in the form \n",
    "\n",
    "$$T(\\theta)=\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{bmatrix}$$\n",
    "\n",
    "What is the geometric significance of this result?\n",
    "\n",
    "- $T(\\theta)\\begin{bmatrix}x\\\\y\\end{bmatrix}=\\begin{bmatrix}x\\cos\\theta - y\\sin\\theta\\\\x\\sin\\theta+y\\cos\\theta\\end{bmatrix}$ from which we can see the geomtric interpretation - this matrix is a transformation that simply rotates a vector by $\\theta^\\circ$! As for proving the statement, I will try to stick with simple algebra instead of relying on linear algebra experience outside of the bounds of this book. Let us denote our orthogonal matrix $A$ as \n",
    "\n",
    "$$A=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}$$\n",
    "\n",
    "where by definition we have that \n",
    "\n",
    "$$A^TA=\\begin{bmatrix}a^2+c^2&ab+cd\\\\ab+cd&b^2+d^2\\end{bmatrix}=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}.$$\n",
    "\n",
    "Additionally, we have by assumption that $\\det A=ad-bc=1$. If we now make the substitution $a=\\cos\\theta, b=-\\sin\\theta,c=\\sin\\theta$ and $d=\\cos\\theta$, then we satisfy all four equations in our system.\n",
    "\n",
    "**3.** Show that the columns of $T$ are orthogonal vectors.\n",
    "\n",
    "- This problem is solved if we show using the matrix given in the previous problem - $a^2+b^2=\\cos^2\\theta+\\sin^2\\theta=1$ and $c^2+d^2=\\cos^2\\theta+(-\\sin\\theta)^2=1$. Note that if instead, we have that $ad-bc=-1$, then we simply make $c=-\\sin\\theta$ and $b=\\sin\\theta$. The determinant is always $\\pm 1$ as I will show in a later exercise.\n",
    "\n",
    "**4.** Show that the product of two orthogonal matrices is again an orthogonal matrix.\n",
    "\n",
    "- Trivial: $(AB)^TAB=B^TA^TAB=B^TB=I$ for orthogonal $A$ and $B$.\n",
    "\n",
    "**5.** Show that the determinant of an orthogonal matrix is $\\pm 1$. \n",
    "\n",
    "- From exercise 2, we have that for an arbitrary orthogonal matrix $A$, that \n",
    "\n",
    "$$A^TA=I\\iff \\det A^TA=\\det I=1\\iff \\left(\\det A\\right)^2=1\\iff \\det A=\\pm 1.$$\n",
    "\n",
    "**6.** Let $T_N$ be an orthogonal matrix of dimension $N$, and form the $(N+1)$-dimensional matrix \n",
    "\n",
    "$$T_{N+1}=\\begin{bmatrix}1&0&\\cdots&0\\\\0&&&\\\\\\vdots&&T_N&\\\\0&&&\\end{bmatrix}$$\n",
    "\n",
    "Show that $T_{N+1}$ is orthogonal.\n",
    "\n",
    "- This is annoying to type out in latex, but the idea is basically that the transpose is a matrix of the same form, but with $T_N^T$ instead of $T_N$ so that $T_{N+1}^TT_{N+1}=I_{N+1}$ after block multiplication which would imply that $T_{N+1}$ is also orthogonal. Intuitively, this is clear as we are simply appending an orthogonal basis to the collection of vectors within $T_N$.\n",
    "\n",
    "**7.** Show that if $T$ is orthogonal, $x=Ty$ implies that $y=T^Tx$.\n",
    "\n",
    "- To see this, note that by definition of orthogonality, \n",
    "\n",
    "$$x=Ty\\iff T^Tx=T^TTy\\iff T^Tx=y.$$\n",
    "\n",
    "**8.** If $AB=BA$, then $TAT^T$ and $TBT^T$ commute if $T$ is orthogonal. \n",
    "\n",
    "- Again, just applying the definition yields that if $T$ is orthogonal, then \n",
    "\n",
    "$$(TAT^T)(TBT^T)=TABT^T=TBAT^T=(TBT^T)(TAT^T).$$\n",
    "\n",
    "### 18. Unitary Matrices\n",
    "\n",
    "Skipped - similar to transpose section as unitary matrices are the complex equivalent of real orthogonal matrices.\n",
    "\n",
    "## Miscellaneous Exercises from Vectors and Matrices\n",
    "\n",
    "**1.** Not included because it's a latex pain - it's about the determinant for a particular kind of matrix by means of cofactor expansion...\n",
    "\n",
    "**2.** Let $I_{ij}$ denote the matrix obtained by interchanging the $i$th and $j$th rows of the unit matrix $I$. Prove that \n",
    "\n",
    "$$I_{ij}^2=I\\;\\;\\;\\;\\;I_{ik}I_{kj}I_{ji}=I_{kj}.$$\n",
    "\n",
    "- Letting $Y=I_{ij}^2$, then \n",
    "\n",
    "$$Y_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}(I_{ij})_{kn}.$$\n",
    "\n",
    "If $m=n$ and $(m,n)\\neq (i,j)$, then we have that $Y_{mn}=1$ as the rows and columns here were unchanged by the operation. If on the other hand, $m\\neq n$ and $m=i$ or $m=j$, then $Y_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}(I_{ij})_{kn}=0$ as the ones no longer get paired together. However if $m=n=i$ or $m=n=j$, then there is exactly one entry - notably the $(i,j)$th entry that is one and will get multiplied together in the dot product. Thus the resulting matrix is identity. Similar deduction can be applied to justify the second equality asked.\n",
    "\n",
    "**3.** Show that $I_{ij}A$ is a matrix identical to $A$ except that the $i$th and $j$th rows have been interchanged, while $AI_{ij}$ yields the interchange of the $i$th and $j$th columns.\n",
    "\n",
    "- Using the definition of matrix multiplication, we have that if $m\\notin \\{i,j\\}$, then \n",
    "\n",
    "$$(I_{ij}A)_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}A_{kn}=A_{mn}.$$\n",
    "\n",
    "On the other hand, if $m\\in\\{i,j\\}$, then the row vector of zeros and ones will be one for the swapped row, that is \n",
    "\n",
    "$$(I_{ij}A)_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}A_{kn}=\\begin{cases}A_{jn}&m=i\\\\A_{in}&m=j.\\end{cases}$$\n",
    "\n",
    "Had we instead commuted the matrices $I_{ij}$ and $A$, then we would end up with the columns being interchanged by similar analysis.\n",
    "\n",
    "**4.** Let $H_{ij}$ denote the matrix whose $ij$th element is $h$, and whose other elements are zero. Show that $(I+H_{ij})A$ yields a matrix identical with $A$ except that the $i$th row has been replaced by the $i$th row plus $h$ times the $j$th row, while $A(I+H_{ij})$ has a similar effect upon the columns. \n",
    "\n",
    "- By associativity of matrix multiplication, we have that $(I+H_{ij})A=A+H_{ij}A$ where \n",
    "\n",
    "$$(H_{ij}A)_{mn}=\\sum\\limits_{k=1}^N (H_{ij})_{mk}A_{kn}=\\begin{cases} 0&m\\neq i\\\\hA_{jn}&m=i\\end{cases}$$\n",
    "\n",
    "which means that $(I+H_{ij})A$ is simply $A$ but the $i$th row becomes the $i$th row plus a constant $h$ times the $j$th row - i.e. this is a form of row reduction we encounter.\n",
    "\n",
    "**5.** Let $H_r$ denote the matrix equal to $I$ except for the fact that the element one in the position $rr$ is equal to $k$. What are $H_rA$ and $AH_r$ equal to?\n",
    "\n",
    "- This is essentially the same as 4 where instead we consider $h=k-1$, so that $I+H_{rr}=H_r$, then we have that $(I+H_{rr})A=A+H_{rr}A$ which again is the same as $A$, but the $r$th row becomes the $r$th row plus $h-1$ times the $r$th row. The same effect occurs with the columns in the latter operation. \n",
    "\n",
    "**6.** If $A$ is real and $AA^T=0$, then $A=0$.\n",
    "\n",
    "- As $A$ is real, we know that \n",
    "\n",
    "$$(AA^T)_{ii}=\\sum\\limits_{k=1}^N A_{ik}A^T_{ki}=\\sum\\limits_{k=1}^N A_{ik}^2=0\\iff A_{ik}=0\\;\\;\\forall k\\in\\{1,\\dots,N\\}.$$ \n",
    "\n",
    "Thus as this holds for all rows $i\\in\\{1,\\dots,N\\}$, we have the desired result.\n",
    "\n",
    "**7.** Same as 6 but for the conjugate transpose equivalent - if $AA^*=0$, then $A=0$.\n",
    "\n",
    "- Not doing...\n",
    "\n",
    "**8.** Show that if $T$ is orthogonal, its elements are uniformly bounded. Similarly, if $U$ is unitary, its elements are uniformly bounded in absolute value.\n",
    "\n",
    "- This follows from similar reasoning to 6) - that is, since $$(AA^T)_{ii}=\\sum\\limits_{k=1}^N A_{ik}^2=1\\Rightarrow A_{ik}\\leq 1.$$\n",
    "\n",
    "and as the elements are real, we have that each element lies in $[0,1]$ as this holds for every row.\n",
    "\n",
    "**9.** Let $d_1$ denote the determinant of the system of linear homogenous equations derived from the relation \n",
    "\n",
    "$$\\left[\\sum\\limits_{j=1}^N a_{ij}x_j\\right]\\left[a_{kj}x_j\\right]=0\\;\\;\\;\\;\\;i,k=1,2,\\dots,N$$\n",
    "\n",
    "regarding the $N(N+1)/2$ quantities $x_ix_j,i,j=1,2,\\dots,N$, as unknowns. Then $d_1=|a_{ij}|^{\\frac{N(N+1)}{2}}$.\n",
    "\n",
    "- The goal I believe is to show that the system can be expressed as the matrix $A$ to the power $N$, but I am not sure how to get there...\n",
    "\n",
    "**10.** Show $\\lim\\limits_{n\\rightarrow\\infty} A^n$, $\\lim\\limits_{n\\rightarrow\\infty} B^n$ may exist as $n\\rightarrow\\infty$, without $\\lim\\limits_{n\\rightarrow\\infty} (AB)^n$ existing. It is sufficient to take $A$ and $B$ two-dimensional.\n",
    "\n",
    "- Let $A=\\begin{bmatrix}1&1\\\\0&0\\end{bmatrix}$ and $B=\\begin{bmatrix}0&1\\\\0&1\\end{bmatrix}$. Then clearly $\\lim\\limits_{n\\rightarrow\\infty} A^n=A$ and $\\lim\\limits_{n\\rightarrow\\infty} B^n=B$ however $$\\lim\\limits_{n\\rightarrow\\infty} (AB)^n=\\lim\\limits_{n\\rightarrow\\infty} \\begin{bmatrix}0&2\\\\0&0\\end{bmatrix}^n$$ which does not exist.\n",
    "\n",
    "**11.** Suppose that we are given the system of linear equations $\\sum\\limits_{j=1}^N a_{ij}x_j=b_i$, $i=1,2,\\dots,N$. If it is possible to obtain the equations $x_1=c_1,x_2=c_2,\\dots,x_N=c_N$, by forming linear combinations of the given equations, then these equations yield a solution of the given equations, and the only solution.\n",
    "\n",
    "- I am not even sure if this is an exercise - are we supposed to prove it's unique?\n",
    "\n",
    "**12.** Introduce the Jacobi bracket symbol $[A,B]=AB-BA$, the commutator of $A$ and $B$. Show, by direct calculation, that $$[A,[B,C]]+[B,[C,A]]+[C,[A,B]]=0.$$\n",
    "\n",
    "- By applying the definition, we obtain that \n",
    "\n",
    "$$[A,[B,C]]+[B,[C,A]]+[C,[A,B]]=[A,BC-CB]+[B,CA-AC]+[C,AB-BA]$$ \n",
    "\n",
    "$$=ABC-ACB-BCA+CBA+BCA-BAC-CAB+ACB+CAB-CBA-ABC+BAC=0$$\n",
    "\n",
    "**13.** Let $r_1=e^{2\\pi i/n}$ be an irreducible root of unity, and let $r_k=e^{2\\pi ik/n}$, $k=1,2,\\dots,n-1$. Consider the matrix \n",
    "\n",
    "$$T=\\begin{bmatrix}1&1&\\cdots&1\\\\1&r_1&\\cdots&r_1^{n-1}\\\\\\vdots&&\\ddots&\\\\1&r_{n-1}&\\cdots&r_{n-1}^{n-1}\\end{bmatrix}.$$\n",
    "\n",
    "Show that $\\frac{T}{\\sqrt{n}}$ is unitary. Hence, if $x_k=\\sum\\limits_{j=0}^{n-1} e^{2\\pi ikj/n}y_j$, then $y_k=\\frac{1}{n}\\sum\\limits_{j=0}^{n-1} e^{-2\\pi i k j/n}x_k$, and $\\sum\\limits_{k=0}^{n-1} |x_k|^2=n\\sum\\limits_{k=0}^{n-1} |y_k|^2.$ This transformation is called a finite Fourier transform. \n",
    "\n",
    "- Do Mark\n",
    "\n",
    "**14.** Suppose that \n",
    "\n",
    "$$\\sum\\limits_{i,j=1}^N a_{ij}x_iy_i=\\sum\\limits_{k=1}^N \\left(\\sum\\limits_{s=k}^N b_{kj}x_s\\right)\\left(\\sum\\limits_{t=k}^N d_{kt}y_t\\right)$$\n",
    "\n",
    "with $b_{kk}=d_{kk}=1$ for all $k$. Then $|a_{ij}|=\\prod\\limits_{k=1}^N c_k$.\n",
    "\n",
    "- Do Mark\n",
    "\n",
    "**15.** Consider the Gauss transform $B=(b_{ij})$ of the matrix $A=(a_{ij})$,\n",
    "\n",
    "$$b_{i1}=\\delta_{i1}a_{i1},\\;\\;\\;\\;b_{ik}=a_{11}^{-1}(a_{11}a_{ik}-a_{i1}a_{ik}),\\;\\;\\;\\;k>1$$\n",
    "\n",
    "Let $A_{11}=(a_{ij})$, $i,j=2,\\dots,N$. Show that \n",
    "\n",
    "$$|\\lambda I - B|=a_{11}^{-1}\\left[\\lambda\\left|\\lambda I - A_{11}\\right|-\\left|\\lambda I - A\\right|\\right].$$\n",
    "\n",
    "- Do Mark\n",
    "\n",
    "There are 14 more exercises in this section to do as well...\n",
    "\n",
    "\n",
    "## 3 - Diagonalization and Canonical Forms\n",
    "\n",
    "### 2. The Solution of Linear Homogenous Equations\n",
    "\n",
    "Here Bellman recapitulates that we desire to find the zeros (or stationary values) of the quadratic form $Q(x)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j$ on the unit sphere $x_1^2+x_2^2+\\cdots +x_N^2=1$ which led us in chapter one to find nontrivial solutions of the linear homogenous equations \n",
    "\n",
    "$$\\sum\\limits_{j=1}^N a_{ij} x_j=\\lambda x_i\\;\\;\\;\\;i=1,\\dots,N.$$\n",
    "\n",
    "From here, we had to digress from this to learn about the power of vectors and matrices - chapter 2. Now with that power in mind, he presents the following theorem that provides much utility in resolving our problem. \n",
    "\n",
    "The takeaway from this chapter is the statement and proof of the following lemma.\n",
    "\n",
    "Lemma: A necessary and sufficient condition that the linear system\n",
    "\n",
    "$$\\sum\\limits_{j=1}^N b_{ij}x_j=0\\;\\;\\;\\; i=1,2,\\dots,N$$\n",
    "\n",
    "possess a nontrivial (at least one $x_j$ is nonzero) solution is that we have the determinantal relation\n",
    "\n",
    "$$|b_{ij}|=0.$$\n",
    "\n",
    "**1.** Show that if $A$ is real, the equation $Ax=0$ always possesses a real nontrivial solution if it possesses any nontrivial solution.\n",
    "\n",
    "- Clearly as Gaussian elimination simply takes existing entries from the matrix $(b_{ij})$ to create solutions of the form $x_1=-\\sum\\limits_{j=2}^N b_{1j}x_j/b_{11}$, we have that the solution is a linear combination of real numbers which since the reals are a field means the result will still be real.\n",
    "\n",
    "### 3. Characteristic Roots and Vectors\n",
    "\n",
    "Here Bellman rants about the ugly word \"eigenvalue\" which was a bilingual compromise between the German word \"eigenverte\" and the English word \"characteristic value\". Aside from this, he explains that we can rewrite our system of equations in the form \n",
    "\n",
    "$$Ax=\\lambda x.$$\n",
    "\n",
    "From here, we know that from the previous section's lemma, that for there to exist a nontrivial $x$ satisfying the above system of equations, it is necessary that \n",
    "\n",
    "$$|A-\\lambda I|=0.$$\n",
    "\n",
    "This equation is known as the characteristic equation of $A$. The equation is polynomial in $\\lambda$ with $N$ roots called characteristic roots/values. If the roots are distinct, we denote them as simple as opposed to multiple. With each root, we have an associated vector $x$ known as a characteristic vector. \n",
    "\n",
    "**1.** $A$ and $A^T$ have the same characteristic values. \n",
    "\n",
    "- To show they share the same characteristic values, it suffices to show they share the same characteristic equation. This is trivially true as $\\det A=\\det A^T$, so:\n",
    "\n",
    "$$|A-\\lambda I|=|(A-\\lambda I)^T|=|A^T-\\lambda I|.$$\n",
    "\n",
    "**2.** $T^TAT-\\lambda I = T^T(A-\\lambda I) T$ if $T$ is orthogonal. Hence, $A$ and $T^TAT$ have the same characteristic values if $T$ is orthogonal.\n",
    "\n",
    "- Intuitively, this makes sense as $T^TAT$ is simply a rotation of $A$ ($T$ is orthogonal), so the determinant which loosely measures volume change does not change under a rotation with scaling equal to one. As for the computation, we just note that $T^T\\lambda IT=T^TT\\lambda I=\\lambda I$. Then notice that since $|T|=|T^T|=1$,\n",
    "\n",
    "$$|T^T(A-\\lambda I)T|=|T^T|\\cdot |A-\\lambda I|\\cdot |A^T|=|A-\\lambda I|.$$\n",
    "\n",
    "**3.** $A$ and $T^*AT$ have the same characteristic values if $T$ is unitary.\n",
    "\n",
    "- This is a restatement of the above for complex matrices...\n",
    "\n",
    "**4.** $SAT$ and $A$ have the same characteristic values if $ST=I$. \n",
    "\n",
    "- Note by a similar trick to 2., we have that \n",
    "\n",
    "$$|SAT-\\lambda I|=|S(A-\\lambda I)T|=|S|\\cdot |A-\\lambda I|\\cdot |T|=|ST|\\cdot |A-\\lambda I|=|A - \\lambda I|.$$\n",
    "\n",
    "**5 and 6.** Show by direct calculation or $A$ and $B$, $2\\times 2$ matrices, that $AB$ and $BA$ have the same characteristic equation. Does the result hold in general?\n",
    "\n",
    "- Let $A=\\begin{bmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{bmatrix}$ and $B=\\begin{bmatrix}b_{11}&b_{12}\\\\b_{21}&b_{22}\\end{bmatrix}$. Then the characteristic equation of $AB$ is \n",
    "\n",
    "$$|AB-\\lambda I|=\\left| \\begin{bmatrix} a_{11}b_{11}+a_{12}b_{12}-\\lambda & a_{11}b_{12}+a_{12}b_{22}\\\\b_{11}a_{21}+b_{21}a_{22}&b_{12}a_{21}+a_{22}b_{22}-\\lambda\\end{bmatrix}\\right|$$\n",
    "\n",
    "$$=\\left(a_{11}b_{11}+a_{12}b_{12}-\\lambda\\right)\\left(b_{12}a_{21}+a_{22}b_{22}-\\lambda\\right)-\\left(a_{11}b_{12}+a_{12}b_{22}\\right)\\left(b_{11}a_{21}+b_{21}a_{22}\\right).$$\n",
    "\n",
    "While for $BA$, we have \n",
    "\n",
    "$$|BA-\\lambda I|=\\left|\\begin{bmatrix} a_{11}b_{11}+a_{21}b_{12}-\\lambda & b_{11}a_{12}+a_{22}b_{12}\\\\a_{11}b_{21}+a_{21}b_{22}&a_{12}b_{21}+a_{22}b_{22}-\\lambda \\end{bmatrix}\\right|$$\n",
    "\n",
    "$$=\\left(a_{11}b_{11}+a_{21}b_{12}-\\lambda\\right)\\left(a_{12}b_{21}+a_{22}b_{22}-\\lambda\\right)-\\left(b_{11}a_{12}+a_{22}b_{12}\\right)\\left(a_{11}b_{21}+a_{21}b_{22}\\right)$$\n",
    "\n",
    "and the result follows these are exactly the same polynomials. To prove this result in general, note that if $A$ or $B$ is invertible then WLOG $B$ is invertible so\n",
    "\n",
    "$$|AB-\\lambda I|=|B|\\cdot |AB-\\lambda I|\\cdot |B^{-1}|=|B|\\cdot |ABB^{-1}-\\lambda I B^{-2}|=|BA-B\\lambda I B^{-1}|=|BA-\\lambda I|.$$\n",
    "\n",
    "If neither are invertible, then notice we have $A+\\zeta I$ is invertible for $\\zeta\\in(0,\\epsilon)$ and $\\epsilon > 0$ as $|A|$ is a monic polynomial with at most $N$ roots, so letting $\\zeta\\rightarrow 0$, we have that \n",
    "\n",
    "$$|(A+\\zeta I)B - \\lambda I|=|B(A+\\zeta I) - \\lambda I|\\Rightarrow |AB-\\lambda I|=|BA-\\lambda I|.$$\n",
    "\n",
    "**7.** Show that any scalar multiple apart from zero of a characteristic vector is also a characteristic vector. Hence, show that we can always choose a characteristic vector $x$ so that $(x,\\overline{x})=1$.\n",
    "\n",
    "- The point is that if we know $Ax=\\lambda x$ for some $x$ and $\\lambda$, then we can multiply both sides by $\\alpha\\in\\mathcal{F}$ so that \n",
    "\n",
    "$$\\alpha Ax=\\alpha \\lambda x\\iff A(\\alpha x)=\\lambda (\\alpha x)$$\n",
    "\n",
    "which implies that once we have a given $\\lambda$ that satisfies the characteristic equation, we can simply multiply both sides of the first equation by one and arrive at a scaled vector still satisfying the relation. \n",
    "\n",
    "**8.** Show, by considering $2\\times 2$ matrices, that the characteristic roots of $A+B$ cannot be obtained in general as sums of characteristic roots of $A$ and $B$.\n",
    "\n",
    "- Let $A=\\begin{bmatrix}1&0\\\\1&1\\end{bmatrix}$ and $B=\\begin{bmatrix}-1&-1\\\\0&-1\\end{bmatrix}$. Then \n",
    "\n",
    "$$|A-\\lambda_A I|=0\\iff \\lambda_A=1,\\;\\;\\;\\; |B-\\lambda_B I|=0\\iff \\lambda_B=-1.$$\n",
    "\n",
    "However, note that \n",
    "\n",
    "$$|A+B-\\lambda_{A+B}I|=0\\iff \\lambda = \\pm 1\\neq \\lambda_A + \\lambda_B=0.$$\n",
    "\n",
    "**9.** Show that a similar comment is true for the characteristic roots of $AB$.\n",
    "\n",
    "- Let $A=\\begin{bmatrix}1&1\\\\0&1\\end{bmatrix}$ and $B=\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}$ then we have that $\\lambda_A=1$ and $\\lambda_B=0$ while $\\lambda_{AB}$ is 0 or 1. \n",
    "\n",
    "**10.** For the $2\\times 2$ case, obtain the relation between the characteristic roots of $A$ and those of $A^2$.\n",
    "\n",
    "- Denoting $A$ by $\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}$, the characteristic equation for $A$ is given by \n",
    "\n",
    "$$\\lambda^2-(a+d)\\lambda + (ad - bc)=\\lambda^2-\\text{Tr}(A)\\lambda + |A|.$$\n",
    "\n",
    "Meanwhile for $A^2$, we obtain\n",
    "\n",
    "$$|A^2-\\lambda I|=(a^2+bc-\\lambda)(d^2+bc-\\lambda)-(ab+bd)(ac+cd)=\\lambda^2-(a^2+2bc+d^2)\\lambda + (ad-bc)^2$$\n",
    "\n",
    "$$=\\lambda^2-\\left[(a+d)^2-2(ad-bc)\\right]\\lambda + (ad-bc^2)=\\lambda^2-\\text{Tr}(A^2)\\lambda + |A|^2$$\n",
    "\n",
    "or equivalently, we could have \n",
    "\n",
    "$$|A^2-\\lambda I|=\\lambda^2-\\left(\\text{Tr}(A)^2-2|A|\\right)\\lambda+|A|^2.$$\n",
    "\n",
    "Anyhow, the important thing to realize is that $\\lambda_{A^2}=\\lambda_A^2$. \n",
    "\n",
    "**11.** Does a corresponding relatin hold for the characteristic roots of $A$ and $A^n$ for $n=2,3,4,\\dots$?\n",
    "\n",
    "- Yes. Notice that \n",
    "\n",
    "$$Ax=\\lambda x\\iff A^2x=A\\lambda x=\\lambda(Ax)=\\lambda^2 x$$\n",
    "\n",
    "so we have in general that squaring a matrix results in a matrix with the eigenvalues squared.\n",
    "\n",
    "### 4. Two Fundamental Properties of Symmetric Matrices.\n",
    "\n",
    "Here Bellman introduces two fundamental results upon which all of the analysis of real symmetric matrices is built upon - basically all of statistics you care about works with these kind of matrices.\n",
    "\n",
    "**Real Roots Theorem**: The characteristic roots of a real symmetric matrix are real.\n",
    "\n",
    "**Orthgonal Eigenboys Theorem**: Characteristic vectors associated with distinct characteristic roots of a real symmetric matrix $A$ are orthogonal. \n",
    "\n",
    "\n",
    "**1.** A characteristic vector cannot be associated with two distinct characteristic values.\n",
    "\n",
    "- Assume the contrary so then there are characteristic roots $\\lambda_1$ and $\\lambda_2$ not equal to each other corresponding to the same characteristic vector $x$ for some matrix $A$. Then since $Ax=\\lambda_1x$, $Ax=\\lambda_2x$, and $Ax-Ax=0=(\\lambda_1-\\lambda_2)x$, we have that $x$ must be zero. As the eigenvector is typically defined to exclude the zero vector (as there are infinitely many eigenvalues associated with it), we arrive at a contradiction as this is only plausible if $\\lambda_1=\\lambda_2$.\n",
    "\n",
    "**2.** Show by means of a $2\\times 2$ matrix, however, that two distinct vectors can be associated with the same characteristic root.\n",
    "\n",
    "- If $\\lambda=1$ i.e. the matrix $A=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}$, then we can have $x=\\begin{bmatrix}a&0\\end{bmatrix}^T$ or $x=\\begin{bmatrix}b&0\\end{bmatrix}^T$.\n",
    "\n",
    "**3.** Show by means of an example that there exist $2\\times 2$ symmetric matrices $A$ and $B$ with the property that $|A-\\lambda B|$ is identically zero. Hence, under what conditions on $B$ can we assert that all roots of $|A-\\lambda B|=0$ are real?\n",
    "\n",
    "- For the second part, if we let $A=\\begin{bmatrix}a&b\\\\b&c\\end{bmatrix}$ and $B=\\begin{bmatrix}d&e\\\\e&f\\end{bmatrix}$, then \n",
    "\n",
    "$$|A-\\lambda B|=(a-\\lambda d)(c - \\lambda f) - (b - \\lambda e)^2=(df - e^2)\\lambda^2 + (2be-cd-af)\\lambda + (ac - b^2).$$\n",
    "\n",
    "By the discriminant in the quadratic formula, we know that the roots of this quadratic are real provided that \n",
    "\n",
    "$$(2be-cd-af)^2-4(df - e^2)(ac - b^2)\\geq 0.$$\n",
    "\n",
    "From this, we can deduce the first part as the quadratic equation is identically zero if $ac=b^2$, $df=e^2$, and $2be-cd-af=0$ - as this would imply that for any $\\lambda$, the determinant is zero.\n",
    "\n",
    "**4.** Show that if $A$ and $B$ are real symmetric matrices, and if $B$ is positive definite, the roots of $|A-\\lambda B|=0$ are all real. \n",
    "\n",
    "- If $B$ is positive definite then all eigenvalues are real. If we are using the same matrices as in problem 3 above, we have the discriminant of the characteristic equation for $B$ must be greater than or equal to zero i.e.\n",
    "\n",
    "$$(d+f)^2-4(df-e^2)\\geq 0.$$\n",
    "\n",
    "From this, we use the inequality to plug into the inequality in 3 and deduce the discriminant of $|A - \\lambda B|$ is non-negative. **Insert algebra here.**\n",
    "\n",
    "**5.** Show that the characteristic roots of a Hermitian matrix are real and that the characteristic vectors corresponding to distinct characteristic roots are orthogonal using the generalized inner product $(x,\\bar{y})$.\n",
    "\n",
    "- I could do this, but...\n",
    "\n",
    "**6.** Let the elements $a_{ij}$ of $A$ depend upon a parameter $t$. Show that the derivatives of $|A|$ with respect to $t$ can be written as the sum of $N$ determinants, where the $k$th determinant is obtained by differentiating the elements of the $k$th row and leaving the others unaltered.\n",
    "\n",
    "- I will denote the $ij$th entry by the function $f_{ij}(t)$ to symbolize that the entry is a function of $t$. The our matrix $A=(f_{ij})$ and let us also denote $|A'_i|$ for the determinant of the matrix $A$ where we differentiate the $i$th row first. Additionally, we need to denote $|A_{ij}|$ as the determinant of the matrix formed by removing the $i$th row and $j$th column. $|(A_{ij})'_k|$ denotes the determinant of the matrix $A$ where the $i$th row and $j$th column is removed and then the $k$th column is differentiated and the result is computed (just layering syntax). Now we can begin - first with the $2\\times 2$ case:\n",
    "\n",
    "$$\\frac{d}{dt}\\left|\\begin{bmatrix}f_{11}(t)&f_{12}(t)\\\\f_{21}(t)&f_{22}(t)\\end{bmatrix}\\right|=\\frac{d}{dt}\\left[f_{11}(t)f_{22}(t)-f_{12}(t)f_{21}(t)\\right]$$\n",
    "\n",
    "$$=f'_{11}(t)f_{22}(t)+f_{11}(t)f'_{22}(t)-f'_{12}(t)f_{21}(t)-f'_{21}(t)f_{12}(t)=\\left|\\begin{bmatrix}f'_{11}(t)&f'_{12}(t)\\\\f_{21}(t)&f_{22}(t)\\end{bmatrix}\\right|+\\left|\\begin{bmatrix}f_{11}(t)&f_{12}(t)\\\\f'_{21}(t)&f'_{22}(t)\\end{bmatrix}\\right|$$\n",
    "\n",
    "$$=|A'_1+A'_2|.$$\n",
    "\n",
    "Now assume it holds for some $N\\times N$ matrix $A$ i.e. that $\\frac{d}{dt}|A|=\\sum\\limits_{i=1}^N |A'_i|$. Then for $A$ of size $(N+1)\\times (N+1)$, we have using cofactor expansion on the first row\n",
    "\n",
    "$$\\frac{d}{dt}|A|=\\frac{d}{dt}\\sum\\limits_{i=1}^{N+1} (-1)^{i-1}f_{1i}(t)|A_{1i}|.$$\n",
    "\n",
    "By chain rule, we have\n",
    "\n",
    "$$\\frac{d}{dt}|A| = \\sum\\limits_{i=1}^{N+1} (-1)^{i-1}\\left[f'_{1i}(t)\\times |A_{1i}|+f_{1i}(t)\\times \\frac{d}{dt}|A_{1i}|\\right]$$\n",
    "\n",
    "From here, note that our induction step yields a formula for the derivative of the determinant of $N\\times N$ matrices. As $A_{1i}$ is $N\\times N$ where we removed the $1$st row and $i$th column, we can apply the induction step and get\n",
    "\n",
    "$$\\frac{d}{dt}|A|=\\sum\\limits_{i=1}^{N+1} (-1)^{i-1} \\left(f'_{1i}(t)|A_{1i}|\\right)+\\sum\\limits_{i=1}^{N+1} (-1)^{i-1} \\left(f_{1i}(t)\\sum\\limits_{j=2}^{N+1} \\left|(A_{1i})'_j\\right|\\right).$$\n",
    "\n",
    "From here, we can finally deduce the result - how? Notice the first sum is simply the cofactor expansion for the $(N+1)\\times (N+1)$ matrix $A$ with the first row differentiated - it equals $|A'_1|$. The latter sum is simply the same thing except we perform cofactor expansion with every row unchanged except the $j$th row which is differentiated first - it equals $\\sum\\limits_{i=2}^{N+1} |A'_i|$. Thus adding the two together yields the equality for all $N\\in\\mathbb{N}$: \n",
    "\n",
    "$$\\frac{d}{dt}|A|=\\sum\\limits_{i=1}^N |A'_i|.$$\n",
    "\n",
    "**7.** Show that the derivative of $|A-\\lambda I|$ with respect to $\\lambda$ is equal to - $\\sum\\limits_{k=1}^N |A_k-\\lambda I|$, where $A_k$ is the $(N-1)\\times (N-1)$ matrix obtained from $A$ by striking out the $k$th row and column.\n",
    "\n",
    "- Again, we can proceed by induction as I don't see a better solution. Let me denote $A_{ij}$ again as the matrix with the $i$th row and $j$th column removed. For the base case, let $A=\\begin{bmatrix} a&b\\\\c&d\\end{bmatrix}$. Then\n",
    "\n",
    "$$\\frac{d}{d\\lambda}|A-\\lambda I|=\\frac{d}{d\\lambda}\\left[ad-(a+d)\\lambda+\\lambda^2\\right]=2\\lambda-(a+d)=-\\left[(a-\\lambda)+(d-\\lambda)\\right]=\\sum\\limits_{k=1}^2 |A_k-\\lambda I|.$$\n",
    "\n",
    "Thus our base holds and let us assume for it holds for $N\\times N$ matrices. Then for an $(N+1)\\times (N+1)$ matrix $A$, we have \n",
    "\n",
    "$$\\frac{d}{dt}\\left|A - \\lambda I\\right|=\\frac{d}{d\\lambda}\\sum\\limits_{k=1}^{N+1} (-1)^{k-1}(A-\\lambda I)_{1k}\\times |(A-\\lambda I)_{1k}|$$\n",
    "\n",
    "$$=\\sum\\limits_{i=1}^{N+1}(-1)^{k-1}\\left[\\frac{d}{d\\lambda} (A-\\lambda I)_{1k}\\times|(A-\\lambda I)_{1k}|+(A-\\lambda I)_{1k}\\times \\frac{d}{d\\lambda}\\left|(A-\\lambda I)_{1k}\\right|\\right].$$\n",
    "\n",
    "This step followed by chain rule (like 6) again. From here, we notice that $\\frac{d}{d\\lambda}(A-\\lambda I)_{1k}=0$ for $k\\neq 1$ and $-1$ otherwise. Additionally, in the second component of the chain rule, we have by the induction step that for $N\\times N$ matrices\n",
    "\n",
    "$$\\frac{d}{d\\lambda}\\left|(A-\\lambda I)_{1k}\\right|=-\\sum\\limits_{j=2}^{N+1} |\\left[(A-\\lambda I)_{1k}\\right]_j-\\lambda I|.$$\n",
    "\n",
    "Combining these steps yields \n",
    "\n",
    "$$\\frac{d}{dt}\\left|A - \\lambda I\\right| = -|(A-\\lambda I)_{11}|+\\sum\\limits_{k=1}^{N+1} (-1)^{k-1}(A-\\lambda I)_{1k}(-1)\\left(\\sum\\limits_{j=2}^{N+1} \\left|\\left[(A-\\lambda I)_{1k}\\right]_j-\\lambda I\\right|\\right)$$\n",
    "\n",
    "$$=-|A_1-\\lambda I|-\\sum\\limits_{j=2}^{N+1}\\sum\\limits_{k=1}^{N+1} (-1)^{k-1}(A-\\lambda I)_{1k}\\times \\left|\\left[(A-\\lambda I)_{1k}\\right]_j-\\lambda I\\right|=-\\sum\\limits_{j=1}^{N+1} |A_j-\\lambda I|.$$\n",
    "\n",
    "**8.** From this, conclude that if $\\lambda$ is a simple root of $A$, then at least one of the determinants $|A_k-\\lambda I|$ is nonzero.\n",
    "\n",
    "- This follows from above as if all $|A_k-\\lambda I|=0$, then the point is stationary w.r.t $\\lambda$ which would imply all eigenvalues are $\\lambda$ (i.e. we have multiple roots).\n",
    "\n",
    "**9.** Use this result to show that if $\\lambda$ is a simple root of $A$, a characteristic vector $x$ associated with $\\lambda$ can always be taken to be a vector whose components are polynomials in $\\lambda$ and the elements of $A$.\n",
    "\n",
    "- Not solved, but I found a post here on the question: https://math.stackexchange.com/questions/4343471/distinct-characteristic-roots-hamilton.\n",
    "\n",
    "### 5. Reduction to Diagonal Form - Distinct Characteristic Roots.\n",
    "\n",
    "In this section, Bellman describes how for matrices with all simple/distinct characteristic roots, we can form diagonal matrices. Let $A$ be an $N\\times N$ matrix with simple characteristic roots $\\lambda_1,\\dots,\\lambda_N$ along with the set of characteristic vectors $x_1,\\dots,x_N$ which have unit length $(x_i,x_i)=1$. Then let $T=\\begin{bmatrix}x_1&\\cdots&x_N\\end{bmatrix}$, so \n",
    "\n",
    "$$T^TT=I.$$\n",
    "\n",
    "Then notice the product \n",
    "\n",
    "$$AT=\\begin{bmatrix}Ax_1&\\cdots&Ax_N\\end{bmatrix}=\\begin{bmatrix}\\lambda_1x_1&\\cdots&\\lambda_N x_N\\end{bmatrix}.$$\n",
    "\n",
    "Similarly, it follows that \n",
    "\n",
    "$$T^TAT=\\begin{bmatrix}\\lambda_1\\\\\\vdots\\\\\\lambda_N\\end{bmatrix} I$$\n",
    "\n",
    "as $x_i x_j=0$ for $i\\neq j$ and 1 otherwise. From here, we again use the fact that $T^TT=I$, to arrive at \n",
    "\n",
    "$$A=T\\begin{bmatrix}\\lambda_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\lambda_N\\end{bmatrix}T^T$$\n",
    "\n",
    "which he describes the process as reduction to diagonal form. From here on out, he uses the notation\n",
    "\n",
    "$$\\Lambda=\\begin{bmatrix}\\lambda_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\lambda_N\\end{bmatrix}.$$\n",
    "\n",
    "**1.** Show that $\\Lambda^k=\\begin{bmatrix}\\lambda_1\\\\\\vdots\\\\\\lambda_N\\end{bmatrix}^k I$ and that $A^k=T\\Gamma^k T^T$ for $k\\in\\mathbb{N}$.\n",
    "\n",
    "- We trivially have that the product of the diagonal matrices with the same entries simply raises the diagonal terms to the $k$th power. For the second question, note that for $k=2$,\n",
    "\n",
    "$$A^k=(T\\Gamma T^T)(T\\Gamma T^T)=T\\Gamma I \\Gamma T^T=T\\Gamma^2 T^T$$\n",
    "\n",
    "and this argument naturally extends by induction for all $k$.\n",
    "\n",
    "**2.** Show that if $A$ has distinct characteristic roots, then $A$ satisfies its own characteristic equation. This is a particular case of a more general result we shall obtain later on. \n",
    "\n",
    "- Haha this is a simple case of the Cayley Hamilton theorem - thank you abstract algebra (for once).\n",
    "\n",
    "**3.** If $A$ has distinct characteristic roots, obtain the set of characteristic vectors asssociated with the characteristic roots of $A^k$ for $k\\in\\mathbb{N}$.\n",
    "\n",
    "- From the previous section, we learned that if $A$ has characteristic roots $\\lambda_1,\\dots,\\lambda_N$, then we have that $A^k$ has characteristic roots $\\lambda_1^k,\\dots,\\lambda_N^k$. Thus we simply want to find the null space of $(A^k-\\lambda^k I)=(T\\Gamma T^T-\\lambda^k I)$. Notice that for $i\\in\\{1,\\dots,N\\}$, we have that $Ax_i=\\lambda x_i\\iff A^kx_i=\\lambda^k x_i$. Thus the eigenvectors are equivalent to those of $A$. However, we have swept under the rug here that these are the only eigenvalues for $A^k$ - i.e. we have found some, but is it guaranteed that this makes up all of the spectra? Yes - this is one of the spectral theorems I think Hamilton will cover (analytical/smooth transformations of the matrix yield the resulting spectra is simply given by the function applied to the original spectra).\n",
    "\n",
    "### 6. Reduction of Quadratic Forms to Canonical Form.\n",
    "\n",
    "Now that we know how to diagonalize a matrix, we can let make a change of variables to simplify the quadratic forms we have been looking at to better understand the dynamics and behavior of the function. That is, let $z=T^Tx$ where $T^T=\\begin{bmatrix} x_1&\\cdots &x_N\\end{bmatrix}^T$ that represents the eigenvectors of a matrix $A$. Then we have \n",
    "\n",
    "$$Q(x)=(x,Ax)=(Tz, ATz)=(z, T^TATz)=(z,\\Gamma z)$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N x_i\\sum\\limits_{j=1}^N A_{ij}x_j=\\sum\\limits_{i=1}^N \\lambda_i z_i^2.$$\n",
    "\n",
    "**1.** Let $A$ have distinct characteristic roots which are all positive. Use the preceding result to compute the volume of the $N$-dimensional ellipsoid $(x,Ax)=1$.\n",
    "\n",
    "- Well, we know that \n",
    "\n",
    "$$(x,Ax)=(y,\\Gamma y)=\\sum\\limits_{i=1}^N \\lambda_i y_i^2=1.$$\n",
    "\n",
    "To find the volume of the quadratic form is a pain but enlightening, so I will explain. Notice that we can start by making the change of variables of the quadratic form such that \n",
    "\n",
    "$$z_i=\\sqrt{\\lambda_i}y_i$$\n",
    "\n",
    "then the Jacobian of our inverse transformation is given by \n",
    "\n",
    "$$S=\\begin{bmatrix}\\frac{1}{\\sqrt{\\lambda_1}}&0&\\cdots&0\\\\0&\\frac{1}{\\sqrt{\\lambda_2}}&\\cdots&0\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\cdots&\\frac{1}{\\sqrt{\\lambda_N}}\\end{bmatrix}$$\n",
    "\n",
    "whose determinant is given by \n",
    "\n",
    "$$|S|=\\prod\\limits_{i=1}^N \\frac{1}{\\sqrt{\\lambda_i}}.$$\n",
    "\n",
    "Then the volume of the N-dimensional ellipsoid is given by \n",
    "\n",
    "$$\\int\\cdots\\int\\limits_{\\sum\\limits_{i=1}^N z_i^2=1} |S|dz_1\\cdots dz_n$$\n",
    "\n",
    "Now, our task is simplified to finding the volume of an $N$-dimensional unit sphere - as the above is simply $|S|$ times the volume of a unit sphere. LET US USE THE POWA OF MULTIVARIATE GAUSSIANS!! Notably, from Aditya's copied lecture notes of \"An Intermediate Course in Probability\" by Gut, the joint density of a random vector $(X_1,\\dots,X_N)^T$ that is multivariate Gaussian with mean $\\mu$ and covariance $\\Sigma$ is given by \n",
    "\n",
    "$$P(X_1=x_1,\\dots,X_N=x_N)=P(X=x)=\\frac{1}{\\sqrt{2\\pi}^N\\sqrt{|\\Sigma|}}e^{\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}.$$\n",
    "\n",
    "In the case that $\\mu=0$ and $\\Sigma=I$, we have\n",
    "\n",
    "$$P(X=x)=\\frac{1}{\\sqrt{2\\pi}^N}e^{-\\frac{1}{2}x^Tx}=\\frac{1}{\\sqrt{2\\pi}^N}e^{\\frac{1}{2}\\sum\\limits_{i=1}^N x_i^2}.$$\n",
    "\n",
    "Our goal from here is to make this look as close to our original integral as possible to prove the volume. Notice that after a change of variables $z_i=\\frac{x_i}{\\sqrt{2}}$, we have the Jacobian of inverse transformation is given by $\\sqrt{2} I$, so the determinant is $\\sqrt{2}^N$ and\n",
    "\n",
    "$$(\\sqrt{2\\pi})^N\\times \\sqrt{2}^N=2^N\\sqrt{\\pi}^N=\\int_\\mathbb{R}\\cdots\\int_\\mathbb{R} e^{-\\sum\\limits_{i=1}^N z_i^2}dz_1\\cdots dz_N.$$\n",
    "\n",
    "Notice that this integrand is symmetric about zero (i.e. $e^{-(-1)^2}=e^{-(1)^2}$), so \n",
    "\n",
    "$$\\int\\limits_\\mathbb{R} e^{-x^2}dx=2\\int\\limits_0^\\infty e^{-x^2}dx$$\n",
    "\n",
    "which simplifies our expression to \n",
    "\n",
    "$$\\sqrt{\\pi}^N = \\int\\limits_0^\\infty\\cdots\\int\\limits_0^\\infty e^{-\\sum\\limits_{i=1}^N z_i^2}dz_1\\cdots dz_N.$$\n",
    "\n",
    "An equivalent formulation of the integral on the right hand side can be uncovered through yet another change of variables to spherical coordinates. The transformation we desire is given by the following mapping \n",
    "\n",
    "$$T(z_1,\\dots,z_N)=\\begin{bmatrix}r \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_{N-1}\\end{bmatrix} = \\begin{bmatrix} \\sqrt{\\sum\\limits_{i=1}^N z_i^2}\\\\ \\cos^{-1}\\left(\\frac{z_1^2}{\\sum\\limits_{i=1}^N z_i^2}\\right)\\\\ \\cos^{-1}\\left(\\frac{z_2^2}{\\sum\\limits_{i=2}^N z_i^2}\\right)\\\\ \\vdots\\\\ \\cos^{-1}\\left(\\frac{z_{N-1}}{\\sum\\limits_{i=N-1}^N z_i^2}\\right)\\end{bmatrix}.$$\n",
    "\n",
    "Then the inverse transformation $S(r,\\theta_1,\\dots,\\theta_{N-1})$ can be found as \n",
    "\n",
    "$$z_i = \\cos(\\theta_i)\\prod\\limits_{j=1}^{i-1} \\sin(\\theta_j)$$ \n",
    "\n",
    "for $i>1$ and for $i=1$, $z_1=\\cos(\\theta_1)$ and so the Jacobian is given by \n",
    "\n",
    "$$S=\\begin{bmatrix} \\cos(\\theta_1)&-r\\sin(\\theta_1)&\\cdots&0\\\\ \\sin(\\theta_1)\\cos(\\theta_2)& r\\cos(\\theta_1)\\cos(\\theta_2)&\\cdots&0\\\\\\vdots &\\vdots &\\ddots&\\vdots\\\\ \\sin(\\theta_1)\\sin(\\theta_2)\\cdots\\sin(\\theta_{n-1})&r\\cos(\\theta_1)\\sin(\\theta_2)\\cdots\\sin(\\theta_{N-1})&\\cdots&r\\sin(\\theta_1)\\sin(\\theta_2)\\cdots\\cos(\\theta_{N-1})\\end{bmatrix}$$\n",
    "\n",
    "Now you may be thinking no way am I computing that and you're right, I'm not as this is trivial by induction. In the case where $N=2$, you know this is just $r^2\\sin(\\theta_1)$ and by use of cofactor expansion like in the proofs in section 5 of this chapter I used earlier, we can inductively prove that\n",
    "\n",
    "$$|S_N|=r^{N-1}\\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i).$$\n",
    "\n",
    "Thus our integral becomes\n",
    "\n",
    "$$\\int\\limits_0^\\infty \\int\\limits_0^{2\\pi}\\cdots\\int_0^\\pi e^{-r^2}\\left[r^{N-1}\\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i)\\right]d\\theta_1\\cdots d\\theta_{N-1}dr.$$\n",
    "\n",
    "Notice that \n",
    "\n",
    "$$\\int\\limits_0^\\infty e^{-r^2}r^{N-1}dr=\\int\\limits_0^\\infty e^{-r^2}(r^2)^{\\frac{N-1}{2}}dr=\\frac{1}{2}\\Gamma(\\frac{N+1}{2})=\\Gamma(\\frac{N}{2}+1),$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\\frac{\\sqrt{\\pi}^N}{\\Gamma(\\frac{N}{2}+1)}=\\int\\limits_0^{2\\pi}\\cdots\\int\\limits_0^\\pi \\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i)d\\theta_1\\cdots d\\theta_{N-1}=\\int\\limits_0^1\\int\\limits_0^{2\\pi}\\cdots\\int\\limits_0^\\pi \\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i)d\\theta_1\\cdots d\\theta_{N-1}dr$$\n",
    "\n",
    "where the last step follows as $\\int_0^1 dr=1$, so we are just using a multiply by one trick. This trick helps us recognize the right-hand side as exactly the volume of the unit sphere (we changed variables and the radius goes from zero to one). Thus, we can finally combine our steps to get the volume of the N-dimensional ellipsoid:\n",
    "\n",
    "$$\\int\\cdots\\int\\limits_{\\sum\\limits_{i=1}^N z_i^2=1} |S|dz_1\\cdots dz_n=\\prod\\limits_{i=1}^N \\frac{1}{\\sqrt{\\lambda_i}}\\times \\frac{\\sqrt{\\pi}^N}{\\Gamma(\\frac{N}{2}+1)}.$$\n",
    "\n",
    "\n",
    "**2.** Prove along the preceding lines that the characteristic roots of Hermitian matrices are real and that characteristic vectors associated with distinct characteristic roots are orthogonal in terms of the notation $[x,y]$ of Sec. 16 of Chap. 2.\n",
    "\n",
    "- Not doing.\n",
    "\n",
    "**3.** Show that if the characteristic roots of a Hermitian matrix $A$ are distinct, we can find a unitary matrix $T$ such that $A=T\\Gamma T^*$. This again is a particular case of a more general result we shall prove in the following chapter.\n",
    "\n",
    "- If the characteristic roots $\\lambda_1,\\dots,\\lambda_N$ are distinct, let $(x_1,\\dots,x_N)$ denote the respective eigenvectors that are normalized ($x_i\\bar{x_i}=1$) which implies that $T$ is unitary as $T^*T=I$. Define $T=(x_1,\\dots,x_N)$, so then \n",
    "\n",
    "$$AT=(Ax_1,\\dots,Ax_N)=(\\lambda_1x_1,\\dots,\\lambda_Nx_N)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$T^*AT=(\\lambda_1(x_1,\\bar{x_1}),\\dots,\\lambda_N(x_1,\\bar{x_N}))=\\begin{bmatrix}\\lambda_1&&\\\\&\\ddots&\\\\&&\\lambda_N\\end{bmatrix}.$$\n",
    "\n",
    "From here, we multiply the left by $T$ and the right by $T^*$ to get that\n",
    "\n",
    "$$A=T\\begin{bmatrix}\\lambda_1&&\\\\&\\ddots&\\\\&&\\lambda_N\\end{bmatrix}T^*$$\n",
    "\n",
    "as desired.\n",
    "\n",
    "**4.** Let $A$ be a real matrix with the property that $A^T=-A$, a *skew-symmetric* matrix. Show that the characteristic roots are either zero or pure imaginary.\n",
    "\n",
    "- Clearly since \n",
    "\n",
    "$$A=S\\Gamma S^T=A^T=-A$$\n",
    "\n",
    "we have that $a_{ij}=-a_{ij}$ where $a_{ij}\\in\\mathbb{R}$ if the characteristic values are zero or pure imaginary. The former is easy to see, but for the latter, notice that we still have for pure imaginary numbers that the real part is zero, so $a_{ij}=-a_{ij}$ as the entries $a_{ij}$ only consist of real numbers.\n",
    "\n",
    "**5.** Let $T$ be an orthogonal matrix. Show that all characteristic roots have absolute value one.\n",
    "\n",
    "- If $T$ is orthogonal, then let $A$ denote the characteristic vectors normalized by one corresponding to eigenvalues $\\lambda_1,\\dots,\\lambda_K$. Then notice that\n",
    "\n",
    "$$T^TT=I=(A\\Gamma A^T)^2=A\\Gamma^2A^T$$\n",
    "\n",
    "which occurs iff $1=\\lambda_i^2\\iff \\lambda_i = \\pm 1$.\n",
    "\n",
    "**6.** Let $T$ be a unitary matrix. Show that all characteristic roots have absolute value one.\n",
    "\n",
    "- Same as 5.\n",
    "\n",
    "**7.** Let $A$ be a symmetric matrix without necessarily simple characteristic roots. Explain how to obtain the diagonal representation as in chapter 5 - $A=S\\Gamma S^T$.\n",
    "\n",
    "- To begin with, we can assert that we can always find a symmetric matrix $B$, with elements arbitrarily small, possessing the property that $A+B$ has simple characteristic roots. Let $\\mu_1,\\dots,\\mu_N$ be the characteristic roots of $A+B$. Then there exists an orthogonal matrix $S$ such that \n",
    "\n",
    "$$A+B=S\\begin{bmatrix}\\mu_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\mu_N\\end{bmatrix}S^T.$$\n",
    "\n",
    "As $S$ is an orthogonal matrix, its elements are uniformly bounded by one (we showed this earlier). Let $\\{B_n\\}_{n\\in\\mathbb{N}}$ be a sequence of matrices approaching 0 such that the corresponding sequence of orthogonal matrices $\\{S_n\\}_{n\\in\\mathbb{N}}$ converges. The limit matrix must be an orthogonal matrix, say $T$. Since $\\lim\\limits_{n\\rightarrow\\infty} \\mu_i=\\lambda_i$, we have \n",
    "\n",
    "$$A=\\lim\\limits_{n\\rightarrow\\infty} (A+B_n)=\\lim\\limits_{n\\rightarrow\\infty} S_n\\begin{bmatrix} \\lim\\limits_{n\\rightarrow\\infty} \\mu_1&&\\\\&\\vdots&\\\\&&\\lim\\limits_{n\\rightarrow\\infty} \\mu_N\\end{bmatrix}\\lim\\limits_{n\\rightarrow\\infty} S_n^T.$$\n",
    "\n",
    "As Bellman notes, this proof relies on core analytic concepts - the existence of such a $B$ to ensure $A+B$ have distinct simple roots and the existence of the two sequences of matrices $\\{B_n\\}$ and $\\{S_n\\}$ which are not yet fleshed out in the book. However, if you remember from second semester analysis, the Stone-Weierstrass theorem in full generality, the notion of algebras and compactness should be enough to convince you this should be true.\n",
    "\n",
    "### 7. Positive Definite Quadratic Forms and Matrices\n",
    "\n",
    "Here Bellman simply defines the concept of a posiitive definite quadratic form for $N$-dimensional quadratic forms. If $A$ is a real symmetric matrix, and \n",
    "\n",
    "$$Q_N(x)=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N A_{ij}x_ix_j>0$$\n",
    "\n",
    "for all real and nonzero $x_i$, we shall say that $Q_N(x)$ is positive definite and that $A$ is positive definite. If $Q_N(x)\\geq 0$, we shall say that $Q_N(x)$ and $A$ are positive indefinite. Similarly we have the same for complex matrices - if $B$ is Hermitian and\n",
    "\n",
    "$$P_N(x)=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N B_{ij}x_i\\bar{x}_j>0$$\n",
    "\n",
    "for all complex and nonzero $x_i$, we say that $P_N(x)$ and $B$ are positive definite.\n",
    "\n",
    "**1.** If $A$ is a symmetric matrix with distinct characteristic roots, obtain a set of necessary and sufficient conditions that $A$ be positive definite.\n",
    "\n",
    "- As before, let $y=T^Tx$ where $T$ consists of the eigenvectors of $A$, so then\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N A_{ij}=\\sum\\limits_{i=1}^n \\lambda_iy_i^2.$$\n",
    "\n",
    "As $y_i^2>0$ for all non-trivial $x$, a sufficient condition for $A$ to be positive definite is that all eigenvalues be positive (or most are zero, but at least one is positive). \n",
    "\n",
    "**2.** There exists a scalar $c_1$ such that $A+c_1I$ is positive definite, given any symmetric matrix $A$.\n",
    "\n",
    "- Let us assume the contrary - that $A$ is not positive definite. Then \n",
    "\n",
    "$$x^T(A+c_1I)x\\leq 0$$\n",
    "\n",
    "for all $x\\in\\mathbb{R}^N$. Notice that this also implies that\n",
    "\n",
    "$$a_{ii}+c_1\\leq 0$$\n",
    "\n",
    "for all $i\\in\\{1,\\dots,N\\}$ and $c_1\\in\\mathbb{R}$. This forms a contradiction as we can always let $c_1=-(a_{ii}+1)$ if $a_{ii}$ is negative which implies that $a_{ii}+c_1=1\\nleq 0$.\n",
    "\n",
    "**3.** Show that we can write $A$ in the form\n",
    "\n",
    "$$A=\\sum\\limits_{i=1}^N \\lambda_iE_i$$\n",
    "\n",
    "where the $E_i$ are non-negative definite matrices. Then\n",
    "\n",
    "$$A^k=\\sum\\limits_{i=1}^N \\lambda_i^k E_i$$\n",
    "\n",
    "for $k=1,2,\\dots$\n",
    "\n",
    "- By the spectral theorem (which hasn't been proven yet), we know that $A=\\sum\\limits_{i=1}^N \\lambda_i x_ix_i^T$ where we know that $x_ix_i^T$ is non-negative definite as for all $a\\in\\mathbb{R}^N$, we have\n",
    "\n",
    "$$a^T(x_ix_i^T)a=(a^Tx_i)(x_i^Ta)=(x_i^Ta)^T(x_i^Ta)\\geq 0.$$\n",
    "\n",
    "As for the latter part, notice that all the cross terms resulting from $\\left(\\sum\\limits_{i=1}^N \\lambda_i x_ix_i^T\\right)^k$ will vanish as the eigenvectors are orthonormal. On a similar note, for the diagonal terms, we have\n",
    "\n",
    "$$(\\lambda x_ix_i^T)^k=\\lambda^k(x_ix_i^T)(x_ix_i^T)\\cdots(x_ix_i^T)=\\lambda^k x_i(x_i^Tx_i)\\cdots(x_i^Tx_i)x_i^T=\\lambda^k x_ix_i^T$$\n",
    "\n",
    "and so $A^k=\\sum \\lambda_i^k x_ix_i^T$ as desired.\n",
    "\n",
    "**4.** If $p(\\lambda)$ is a polynomial in $\\lambda$ with scalar coefficients, $p(\\lambda)=\\sum\\limits_{k=0}^m c_k\\lambda^k$, let $p(A)$ denote the matrix $p(A)=\\sum\\limits_{k=0}^m c_k A^k$. Show that $p(A)=\\sum\\limits_{i=1}^N p(\\lambda_i)E_i$.\n",
    "\n",
    "- This follows trivially from question 3 as\n",
    "\n",
    "$$p(A)=\\sum\\limits_{k=0}^m c_k\\sum\\limits_{i=1}^N \\lambda_i^k E_i=\\sum\\limits_{i=1}^N \\sum\\limits_{k=0}^m c_k\\lambda_i^k E_i=\\sum\\limits_{i=1}^N p(\\lambda)E_i.$$\n",
    "\n",
    "## Miscellaneous Exercises from Diagonalization and Canonical Forms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702b0b5",
   "metadata": {},
   "source": [
    "# Some Competition Linear Algebra Problems & Answers \n",
    "\n",
    "Here are an assortment of easy competition problems - my solutions are not complete, but sufficient if we are talking about statistics for the sake of my sanity with latex.\n",
    "\n",
    "1. Putnam 2008 Problem A2: Alan and Barbara play a game in which they take turns filling entries of an initially empty $2008\\times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy?\n",
    "\n",
    "- There is a very neat solution/optimal strategy for Barbara to win every time. If Alan fills in a real number $x$ on the $a_{ij}$ entry, Barbara can fill in the same real number x in the $a_{ij+1}$ or $a_{ij-1}$ entry (sticking to either adjacent rows or columns for the entire game). Thus in the end, there will always be at least one pairing of identical columns. Barbara could also have done this pairing strategy by rows instead which also ensures the determinant of the resultant matrix is $0$.\n",
    "\n",
    "2.\\. IMC 2005 Day 1 Problem 1: Let $A$ be an $n\\times n$ matrix such that $A_{ij}=i+j$. Find the rank of $A$.\n",
    "\n",
    "- For $n=1$, we have that the rank is 1. For $n\\geq 2$, we can do the following: take the first row and subtract it from each of the subsequent rows. Then the second through $n$th row are all increasing multiples of each other. Hence we have just two rows that make up the rank for any matrix larger than $1\\times 1$.\n",
    "\n",
    "3.\\. Putnam 2011 Problem A4: For which positive integers n is there an n\\times n matrix with integer entries such that every dot product of a row with itself is even, while every dot product of two different rows is odd?\n",
    "\n",
    "- WLOG, we can just inspect matrices with entries in $\\mathbb{Z}/2\\mathbb{Z}$ as we are only concerned with whether or not the dot product is even/odd. For odd matrices, it is easy to see that if we consider a matrix with 0s along the diagonal and 1s everywhere else, then what we have is that each row as an even n-1 number of 1s whereas when we take the dot product with another row, we will always have 2 cases of $0*1$ and the rest are $1*1 – n-2$ is odd, so this works. This generalizes for all odd $n$, so now we turn attention to even $n$ where this simple argument doesn’t extend nicely. Instead, we can use a bit of linear algebra. If we have row vectors $V_1,\\dots,V_n$, our problem is characterized by trying to show that $\\forall i,\\;j\\in\\{1,\\dots,n\\}$: $i\\neq j,\\;V_i\\cdot V_j=1$ and $V_i\\cdot V_i=0$. As the dot product of a vector with itself is even, we must have that the matrix is not invertible. But, there’s a contradiction – left up to the reader 🙂\n",
    "\n",
    "## Other Comp Problems \n",
    "- https://artofproblemsolving.com/community - this is a goat resource to improve, look at contest, high school olympiad and college math forums.\n",
    "- https://href.li/?http://www.math.utoronto.ca/barbeau/putnamla.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f277c",
   "metadata": {},
   "source": [
    "# Exercises from \"Linear Statistical Inference and Its Applications\" by C.R Rao\n",
    "\n",
    "## 1a\n",
    "\n",
    "**1.** If $\\alpha_1,\\dots,\\alpha_k$ are $k$ independent vectors in a a vector space $\\mathcal{X}$, show that it can always be exptended to a basis of $\\mathcal{X}$, that is, we can add $\\alpha_{k+1},\\dots,\\alpha_r$ such that $\\alpha_1,\\dots,\\alpha_r$ form a basis of $\\mathcal{X}$, where $r=\\text{Dim}(\\mathcal{X})$.\n",
    "\n",
    "- do Mark\n",
    "\n",
    "**4.** If $pq$ elements $a_{ij}$, $i\\in\\{1,\\dots,p\\}$ and $j\\in\\{1,\\dots,q\\}$ are such that the tetra difference $a_{ij}+a_{rs}-a_{is}-a_{rj}=0$ for all $i,j,r,s$, then $a_{ij}$ can be expressed in the form $a_{ij}=a_i+b_j$ where $a_1,\\dots,a_p$ and $b_1,\\dots,b_q$ are properly chosen. \n",
    "\n",
    "- do Mark\n",
    "\n",
    "**5.** Find the number of independent vectors in the set ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38408ed9",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "- Introduction to Matrix Analysis by Richard Bellman\n",
    "    - I like the book for the problems and storytelling/smooth explanations. It assumes some first course linear algebra exposure and maybe differential equations for applications, but that is it.\n",
    "    \n",
    "    \n",
    "- Scientific Computing: An Introductory Survey by Heath\n",
    "    - I just bought this book. It seems full of applications and is used in an ML grad course at Stanford, so I'm all for it. This has epic problems just from eyeballing it.\n",
    "    \n",
    "    \n",
    "- Linear Statistical Inference and Its Applications by C.R Rao\n",
    "    - I really enjoy how he introduces linear algebra with statistics in mind. I have found his writing much better for getting exposure to linear algebra used frequently in heavier duty books like Keener's \"Theoretical Statistics\".\n",
    "   \n",
    "   \n",
    "- Linear Algebra by Hoffman and Kunze\n",
    "    - I used this undergrad for linear algebra. It's full of good examples and problems, but be warned this dude doesn't mess around and it's tough and he quickly abstracts to results in algebra - like Grassman manifolds which have no place in my heart tbh.\n",
    "    \n",
    "    \n",
    "- Mathematics for Machine Learning by Deisenroth et al.\n",
    "    - This is super light, but if like Rao's book but at a higher level, you want only the barebones, this might quench your thirst.\n",
    "\n",
    "\n",
    "- https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/thm_sed.html\n",
    "    - EECS 127 has a good proof of spectral theorem :) \n",
    "\n",
    "\n",
    "This weird econ dude has surprisingly good notes on the topic w/ visuals too - screw MIT, Iowa State is where it's at :)\n",
    "- http://www2.econ.iastate.edu/classes/econ500/hallam/Char_Vec.pdf\n",
    "- http://www2.econ.iastate.edu/classes/econ500/hallam/documents/Quad_Forms_000.pdf\n",
    "- http://www2.econ.iastate.edu/classes/econ500/hallam/documents/Geometry_Matrices_002.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
