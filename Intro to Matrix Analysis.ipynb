{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a39d7d",
   "metadata": {},
   "source": [
    "# Exposition\n",
    "\n",
    "The goal of this set of notes is to understand linear algebra as a whole better. The utility and applications of the subject is lucid, but not many really understand what the heck is going on. My goal is to do exactly that - find out what's going on with enough depth to traverse through topics in ML and high dimensional statistics with ease. To do so, I will be solving all the linear algebra questions from Bellman's \"Introduction to Matrix Analysis\", revisiting material from \"Linear Algebra\" by Hoffman & Kunze, as well as going through Rao's chapter on vector spaces specific to statistics. As always, the layout is notes, followed by exercises, followed by sources if you hate my writing or want more explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe017e",
   "metadata": {},
   "source": [
    "# Exercises from \"Introduction to Matrix Analysis\" by Richard Bellman\n",
    "\n",
    "In Bellman's words: \"Together with working out a number of the exercises, [Chapters 1-5 and Chapters 10-11] should cover a semester course at an undergraduate senior, or first-year graduate level.\" An impressive feature of the author (Bellman), is that he wrote it (and 100+ papers) after having a brain tumor removed that left him severely disabled.\n",
    "\n",
    "## 2 - Vectors and Matrices\n",
    "\n",
    "### 3. Vector Addition\n",
    "\n",
    "**1.** Show that we have commutativity, $x+y=y+x$, and associativity, $x+(y+z)=(x+y)+z$.\n",
    "\n",
    "- I assume our matrices are either integer, rational, real, or complex. In any of these cases, it trivially follows that since $x+y=y+x$ for $x,y\\in\\mathcal{F}$ that the same holds if we perform the addition elementwise as done in vector addition. On a similar note, we know these fields of numbers are associative, so associativity is also an inherent condition. \n",
    "\n",
    "**3.** Define subtraction of two vectors directly, and in terms of addition; that is, $x-y$ is a vector $z$ such that $y+z=x$.\n",
    "\n",
    "- Let $z=x+(-y)$, so $y+(x+(-y))=x+(y+(-y))=x$ by associativity.\n",
    "\n",
    "### 4. Scalar Multiplication\n",
    "\n",
    "**1.** Show that $(c_1+c_2)(x+y)=c_1x+c_1y+c_2x+c_2y$.\n",
    "\n",
    "- $(c_1+c_2)(x+y)=(c_1+c_2)x+(c_1+c_2)y=c_1x+c_2x+c_1y+c_2y$.\n",
    "\n",
    "**2.** Define the null vector, written 0, to be the vector all of whose components are zero. Show that it is uniquely determined as the vector 0 for which $x+0=x$ for all $x$.\n",
    "\n",
    "- Again, this question stems from our understanding of the field of numbers we are working with. Clearly $x+0=x$ for $x\\in\\mathbb{R}$ and this concept extends for each entry as we denote the zero vector by $n$ zeros in a column vector.\n",
    "\n",
    "### 5. The Inner Product of Two Vectors\n",
    "\n",
    "**1.** If $x$ is real, show that $(x,x)>0$ unless $x$ is $0$.\n",
    "\n",
    "- If $x\\neq 0$, $(x,x)=\\sum\\limits_{i=1}^n x_i^2>0$ for some component $x_i\\neq 0$ in $x$.\n",
    "\n",
    "**2.** Show that $(ux+vy,ux+vy)=u^2(x,x)+2uv(x,y)+v^2(y,y)$ is a non-negative definite quadratic form in the scalar variables $u$ and $v$ if $x$ and $y$ are real and hence that \n",
    "$$(x,y)^2\\leq (x,x)(y,y)$$\n",
    "for any two real vectors $x$ and $y$ (Cauchy's inequality).\n",
    "\n",
    "- $(ux+vy,ux+vy)=\\sum\\limits_{i=1}^n (ux_i+vy_i)^2$. This quantity is clearly non-negative provided the field of numbers is real or rational. To show Cauchy's inequality, we know $(ux+vy)(ux+vy)=u^2(x,x)+2uv(x,y)+v^2(y,y)$ by expanding out the inner product. If we consider $u=-\\sqrt{(y,y)}$ and $v=\\sqrt{(x,x)}$, then the above becomes\n",
    "\n",
    "$$\\left(-\\sqrt{y,y}\\right)^2(x,x)-2\\sqrt{(y,y)(x,x)}(x,y)+\\left(\\sqrt{x,x}\\right)^2(y,y)\\geq 0\\iff 2(x,x)(y,y)\\geq 2\\sqrt{(y,y)(x,x)}(x,y)$$\n",
    "\n",
    "$$\\iff (x,x)^2(y,y)^2\\geq (x,x)(y,y)(x,y)^2\\iff (x,x)(y,y)\\geq (x,y)^2.$$\n",
    "\n",
    "**3.** What is the geometric interpretation of this result?\n",
    "\n",
    "- The projection of a vector onto another does not have longer length than the vector itself.\n",
    "\n",
    "**4.** Using this result, show that for real vectors $x$ and $y$ that\n",
    "\n",
    "$$(x+y,x+y)^{1/2}\\leq (x,x)^{1/2} + (y,y)^{1/2}.$$\n",
    "\n",
    "- Using Cauchy's, $(x+y,x+y)=(x,x)+2(x,y)+(y,y)\\leq(x,x)+2\\sqrt{(x,x)(y,y)}+(y,y)=\\left((x,x)^{\\frac{1}{2}}+(y,y)^{\\frac{1}{2}}\\right)^2$. From here, we simply take the square root of both sides and call it a day. \n",
    "\n",
    "**5.** Why is this inequality called the \"triangle inequality\"?\n",
    "\n",
    "- Because in $\\mathbb{R}^2$, we have that a triangle's smallest two sides summed is greater than or equal to the hypotenuse - hence the name stemming from this geometric shape. \n",
    "\n",
    "### 6. Orthogonality\n",
    "\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 7. Matrices\n",
    "\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 8. Matrix Multiplication - Vector by Matrix\n",
    "\n",
    "**1.** Show that $(A+B)(x+y)=Ax+Ay+Bx+By$.\n",
    "\n",
    "- $(A+B)(x+y)=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N (a_{ij}+b_{ij})(x_j+y_j)=\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N a_{ij}x_j+a_{ij}y_j+b{ij}x_j+b_{ij}y_j=Ax+Ay+Bx+By$.\n",
    "\n",
    "### 9. Matrix Multiplication - Matrix by Matrix.\n",
    "\n",
    "**2.** If \n",
    "\n",
    "$$T(\\theta)=\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{bmatrix}$$\n",
    "\n",
    "show that \n",
    "\n",
    "$$T(\\theta_1)T(\\theta_2)=T(\\theta_2)T(\\theta_1)=T(\\theta_1+\\theta_2).$$\n",
    "\n",
    "- Using the identity $\\cos\\theta_1+\\theta_2=\\cos\\theta_1\\cos\\theta_2-\\sin\\theta_1\\sin\\theta_2$ and $\\sin\\theta_1+\\theta_2=\\cos\\theta_1\\sin\\theta_2+\\cos\\theta_2\\sin\\theta_1$, we can clearly expand out the linear operators to arrive at the equalities.\n",
    "\n",
    "**3.** Let $A$ be a matrix with the property that $a_{ij}=0$ if $j\\neq i$, a diagonal matrix, and let $B$ be a matrix of similar type. Show that $AB$ is again a diagonal matrix, and that $AB=BA$.\n",
    "\n",
    "- Let $C=AB$. $c_{ij}=\\sum\\limits_{k=1}^n a_{ik}b_{kj}=a_{ik}b_{kj}=a_{ii}b_{ii}\\iff i=j$ otherwise the sum is zero. Thus the matrix is diagonal. Similarly, letting $D=BA$ we obtain $d_{ij}=\\sum\\limits_{k=1}^n b_{ij}a_{jk}=b_{ik}a_{kj}=b_{ii}a_{ii}\\iff i=j$ otherwise the sum is zero. As The field is commutative, interchanging these elements is allowed and so $C=D$.\n",
    "\n",
    "**4.** Let $A$ be a matrix with the property that $a_{ij}=0$, $j>i$, a triangular or semi-diagonal matrix, and let $B$ be a matrix of similar type. Show that $AB$ is again a triangular matrix, but that $AB\\neq BA$ in general.\n",
    "\n",
    "- Let $C=AB$ so $c_{ij}=\\sum\\limits_{i=1}^n a_{ik}b_{kj}=0$ if $k>i$ or $j>k$ which is equivalent to saying $j>i$ which implies the matrix is triangular. If $A=\\begin{bmatrix}a&0\\\\b&c\\end{bmatrix}$ and $B=\\begin{bmatrix}x&0\\\\y&z\\end{bmatrix}$, $AB\\neq BA$ if we ensure $bx+cy\\neq ay+bz$.\n",
    "\n",
    "**6.** Use the relation $|AB|=|A||B|$ to show that \n",
    "\n",
    "$$(a_1^2+a_2^2)(b_1^2+b_2^2)=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2$$\n",
    "\n",
    "- If we let $A=\\begin{bmatrix}a_1&a_2\\\\-a_2&a_1\\end{bmatrix}$ and $B=\\begin{bmatrix}b_1&b_2\\\\-b_2&b_1\\end{bmatrix}$, then\n",
    "\n",
    "$$|AB|=(a_1b_1-a_2b_2)^2+(a_2b_1+a_1b_2)^2=|A||B|=(a_1^2+a_2^2)(b_1+b_2^2).$$\n",
    "\n",
    "**10.** Consider the linear fractional transformation \n",
    "\n",
    "$$w=\\frac{a_1z+b_1}{c_1z+d_1}=T_1(z)$$\n",
    "\n",
    "If $T_2(z)$ is a similar transformation, with coefficients $a_1,b_1,c_1,d_1$ replaced by $a_2,b_2,c_2,d_2$, show that $T_1(T_2(z))$ and $T_2(T_1(z))$ are again transformations of the same type.\n",
    "\n",
    "- I will just show the first case as the latter follows with the exact same argument in mind:\n",
    "\n",
    "$$T_1(T_2(z))=T_1\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)=\\frac{a\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)+b_1}{c_1\\left(\\frac{a_2z+b_2}{c_2z+d_2}\\right)+d_1}=\\frac{(a_1a_2+b_1c_2)z+(a_1b_2+b_1d_2)}{(a_2c_1+c_2d_1)z+(b_2c_1+d_1d_2)}.$$\n",
    "\n",
    "**11.** If the expression $a_1d_1-b_1c_1\\neq 0$, show that $T_1^{-1}(z)$ is a transformation of the same type. If $a_1d_1-b_1c_1=0$, what type of a transformation is $T_1(z)$?\n",
    "\n",
    "- Not solved\n",
    "\n",
    "**12.** Consider a correspondence between $T_1(z)$ and the matrix \n",
    "\n",
    "$$A_1=\\begin{bmatrix}a_1&b_1\\\\c_1&d_1\\end{bmatrix}$$\n",
    "\n",
    "written $A_1\\sim T_1(z)$. Show that if $A_1\\sim T_1(z)$ and $A_2\\sim T_2(z)$, then $A_1A_2\\sim T_1(T_2(z))$. What then is the condition that $T_1(T_2(z))=T_2(T_1(z))$ for all $z$?\n",
    "\n",
    "- If we let $A_1=\\begin{bmatrix}a_1&b_1\\\\c_1&d_1\\end{bmatrix}$ and $A_2=\\begin{bmatrix}a_2&b_2\\\\c_2&d_2\\end{bmatrix}$, then $A_1A_2=\\begin{bmatrix} a_1a_2+b_1c_2&a_1b_2+b_1d_2\\\\a_2c_1+c_2d_1&b_2c_1+d_1d_2\\end{bmatrix}$. From problem 10, we showed that\n",
    "\n",
    "$$T_1(T_2(z))=\\frac{(a_1a_2+b_1c_2)z+(a_1b_2+b_1d_2)}{(a_2c_1+c_2d_1)z+(b_2c_1+d_1d_2)},$$\n",
    "\n",
    "and expressed in similar correspondence, this is equivalent to $A_1A_2$. Thus our condition is that $A_1A_2=A_2A_1$.\n",
    "\n",
    "**14.** Show that \n",
    "\n",
    "$$\\begin{bmatrix} -1&-1\\\\1&0\\end{bmatrix}^3=I$$\n",
    "\n",
    "- The matrix represents a $120^\\circ$ rotation. Thus if we apply the transformation three times, we end up in the same spot.\n",
    "\n",
    "### 10. Noncommutativity\n",
    "\n",
    "Skipped - no interesting exercises\n",
    "\n",
    "### 11. Associativity\n",
    "\n",
    "**3.** Show that $A^m$ and $B^n$ commute if $A$ and $B$ commute.\n",
    "\n",
    "- If $AB=BA$, then $A^mB^n=A\\cdots A\\times B\\cdots B=A\\cdots B\\times A\\cdots B$ and iteratively, we continue this procedure to prove commutativity of the matrices raised to some power.\n",
    "\n",
    "**4.** Write \n",
    "\n",
    "$$X^n=\\begin{bmatrix}x_1(n)&x_2(n)\\\\x_3(n)&x_4(n)\\end{bmatrix}\\;\\;\\;\\;\\text{where}\\;\\;\\;\\; X=\\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix}$$\n",
    "\n",
    "is a given matrix. Using the relation $X^{n+1}=XX^n$, derive recurrence relations for the $x_i(n)$ and thus derive the analytic form of the $x_i(n)$.\n",
    "\n",
    "- $X^{n+1}=XX^n=\\begin{bmatrix}x_1(n)&x_2(n)\\\\x_3(n)&x_4(n)\\end{bmatrix}\\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix}=\\begin{bmatrix}x_1x_1(n)+x_3x_2(n)&x_2x_1(n)+x_4x_2(n)\\\\x_1x_3(n)+x_3x_4(n)&x_2x_3(n)+x_4x_4(n)\\end{bmatrix}$ which form our respective recurrence relations for the system. As for the analytic form, I believe we can leave the expression in matrix form (i.e. $X^n$), but for explicit solutions, we know $x_i(n+1)=x_i(0)^n+\\text{cross-terms}$ but not sure how useful that is.\n",
    "\n",
    "**6.** Use these relations to find explicit representations for the elements of $$X^n$$ where \n",
    "\n",
    "$$X=\\begin{bmatrix}\\lambda&1\\\\0&\\lambda\\end{bmatrix}\\;\\;\\;\\;\\;\\;\\;\\; X=\\begin{bmatrix}\\lambda&1&0\\\\0&\\lambda & 1\\\\0&0&\\lambda\\end{bmatrix}$$\n",
    "\n",
    "- For the 2x2 matrix, we have that \n",
    "\n",
    "$$\\begin{bmatrix}\\lambda&1\\\\0&\\lambda\\end{bmatrix}^3=\\begin{bmatrix}\\lambda^3&3\\lambda^2\\\\0&\\lambda^3\\end{bmatrix}$$ \n",
    "\n",
    "and proceeding by induction or inspection without using the earlier recurrence relations, we see that \n",
    "\n",
    "$$X^n=\\begin{bmatrix}\\lambda^n&n\\lambda^{n-1}\\\\0&\\lambda^n\\end{bmatrix}.$$\n",
    "\n",
    "As for the 3x3 matrix, we can similary proceed by first observing the first few powers and continuing with induction - that is\n",
    "\n",
    "$$X^n=\\begin{bmatrix}\\lambda^n&n\\lambda^{n-1}&\\frac{n(n-1)}{2}\\lambda^{n-2}\\\\0&\\lambda^n&n\\lambda^{n-1}\\\\0&0&\\lambda^n\\end{bmatrix}$$\n",
    "\n",
    "**7.** Find all 2x2 solutions of $X^2=X.$\n",
    "\n",
    "- This can be solved by algebra\n",
    "\n",
    "$$\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}^2=\\begin{bmatrix}a^2+bc&ab+bd\\\\ac+cd&d^2+bc\\end{bmatrix}$$\n",
    "\n",
    "Thus the first set of solutions is simply given by the class of diagonal matrices where $b=c=0.$ The second class can be found where $a=1-d$. In this solution space, we have that $b$ and $c$ are free variables in $\\mathbb{R}$ and thus for some fixed $bc\\in\\mathbb{R}$, we can find the diagonal entries as follows: \n",
    "\n",
    "$$d^2+bc=d\\iff d^2-d+bc=0\\iff d=\\frac{1\\pm\\sqrt{1-4bc}}{2}$$\n",
    "\n",
    "and thus \n",
    "\n",
    "$$a=\\frac{1\\mp\\sqrt{1-4bc}}{2}.$$\n",
    "\n",
    "**8.** Find all 2x2 solutions of $X^n=X$.\n",
    "\n",
    "- This solution is a bit more difficult to derive - I believe we can use the binomial theorem to expand this out however we can not end up with a nice description of the class of solutions as we were able to derive above. \n",
    "\n",
    "### 12. Invariant Vectors\n",
    "\n",
    "Skipped - no exercises\n",
    "\n",
    "### 13. Quadratic Forms as Inner Products\n",
    "\n",
    "**1.** Does $(x,Ax)=(x,Bx)$ for all $x$ imply $A=B$?\n",
    "\n",
    "- If for all $x$, we have \n",
    "\n",
    "$$(x,Ax)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j=\\sum\\limits_{i,j=1}^N b_{ij}x_ix_j=(x,Bx),$$ then the intuitive justifiication is that if under every vector in the space, we have the two matrices/transformations map to the same point, then it is necessarily the case that the two transformations are equivalent. I'm not sure how to carry out this same justification rigorously to be honest.\n",
    "\n",
    "**2.** Under what conditions does $(x,Ax)=0$ for all $x$?\n",
    "\n",
    "- This is only the case for the zero matrix - where every entry is defined by zero. This can be taken for granted if we are working with the real or complex field of numbers.\n",
    "\n",
    "### 14. The Transpose Matrix\n",
    "\n",
    "Skipped - no exercises.\n",
    "\n",
    "### 15. Symmetric Matrices\n",
    "\n",
    "**1.** Show that $(A^T)^T=A$.\n",
    "\n",
    "- As $A^T=\\sum\\limits_{i,j=1}^N a_{ij}$, then clearly $(A^T)^T=\\sum\\limits_{i,j=1}^N a_{ij}=A$.\n",
    "\n",
    "**2.** Show that $(A+B)^T=A^T+B^T$, $(AB^T)=B^TA^T$, $(A_1A_2\\cdots A_n)^T=A_n^T\\cdots A_2^TA_1^T$.\n",
    "\n",
    "- $(A+B)_{ij}=a_{ij}+b_{ij}\\Rightarrow (A+B)^T_{ij}=a_{ji}+b_{ji}$, so $(A+B)^T=A^T+B^T$. For the second equality to show, we have that \n",
    "\n",
    "$$(B^TA^T)_{ij}=\\sum\\limits_{k=1}^N (B^T)_{ik}(A^T)_{kj}=\\sum\\limits_{k=1}^N B_{kj}A_{jk}=(AB)_{ji}=\\left((AB)^T\\right)_{ij}.$$\n",
    "\n",
    "For the third, let $B=(A_1\\cdots A_{n-1})$, so then from the second equality, $(BA_n)^T=A_n^TB^T$ and we can iteratively repeat this procedure up till $B=A_1$ to arrive at the conclusion. As for the fourth equality, we trivially have after lettting $A_1=\\cdots A_n=A$, that $(A^n)^T=(A^T)^n$.\n",
    "\n",
    "**3.** Show that $AB$ is not necessarily symmetric if $A$ and $B$ are. \n",
    "\n",
    "- Let $A=\\begin{bmatrix}-1&0\\\\0&0\\end{bmatrix}$ and $B=\\begin{bmatrix}1&-1\\\\-1&1\\end{bmatrix}$, so $AB=\\begin{bmatrix}-1&1\\\\0&0\\end{bmatrix}$ which is no longer symmetric.\n",
    "\n",
    "**4.** Show that $A^TBA$ is symmetric if $B$ is symmetric.\n",
    "\n",
    "- Let $Y=A^TBA$, then \n",
    "\n",
    "$$Y_{ij}=\\sum\\limits_{k=1}^N (A^T)_{ik}(BA)_{kj}=\\sum\\limits_{k=1}^N A_{ji}\\left(\\sum\\limits_{l=1}^N B_{kl}A_{lj}\\right)$$\n",
    "\n",
    "while \n",
    "\n",
    "$$(Y^T)_{ji}=A^TB^TA_{ij}=\\sum\\limits_{k=1}^N (A^T)_{ik}(B^TA)_{kj}=\\sum\\limits_{k=1}^N (A^T)_{ik}\\left(\\sum\\limits_{l=1}^N (B^T)_{kl}A_{lj}\\right)$$\n",
    "\n",
    "where we see the two are equivalent as $(B^T)_{ij}=B_{ij}$. The implication of this result is great in statistics where we often are dealing with dispersion or covariance matrices (which are symmetric generally) and have scalers or noise inputted in. \n",
    "\n",
    "**5.** Show that $(Ax,By)=(x,A^TBy)$\n",
    "\n",
    "- We can just apply the definition to get the result alongside the fact that we can freely interchange the finite sums - \n",
    "\n",
    "$$(x,A^TBy)=\\sum\\limits_{i=1}^N x_i\\left(\\sum\\limits_{j=1}^N (A^TB)_{ij}y_j\\right)=\\sum\\limits_{i=1}^N x_i\\left(\\sum\\limits_{j=1}^N\\left(\\sum\\limits_{k=1}^N A_{ki}B_{kj}\\right)y_j\\right)$$\n",
    "\n",
    "$$=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N \\sum\\limits_{k=1}^N (A_{ki}x_i)(B_{kj}y_j)=\\sum\\limits_{k=1}^N \\left(\\sum\\limits_{i=1}^N A_{ki}x_i\\right)\\left(\\sum\\limits_{j=1}^N B_{kj}y_j\\right)=\\sum\\limits_{k=1}^N (Ax)_k(By)_k=(Ax,By)$$\n",
    "\n",
    "**6.** Show that $\\det A=\\det A^T$.\n",
    "\n",
    "- I will first provide an intuitve justification here and dive more into the details after covering \"Linear Algebra\" by Hoffman and Kunze. The intuitive justification is through the geometric definition of the determinant - the volume of a parallelpiped formed by the column vectors. From this definition, you can intuitively see that if the matrix is square, then these will preserve the same mass of the parallelpiped. At a slightly more computational level, you can see this as cofactor expansion to compute the determinant does not matter if I traverse through the rows or columns - of course if you know the proof of this theorem, then you can use it. I'll cover proof of the latter and a broader generalization of the definition taught in first course linear algebra above.\n",
    "\n",
    "**7.** Show that when we write $Q(x)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j$, there is no loss of generality in assuming that $a_{ij}=a_{ji}$.\n",
    "\n",
    "- This question is masked a bit on the outside but still very interesting. It is really asking for the reader to show that for any real quadratic form, the matrix $A$ is symmetric. To realize this, it boils down to an age old linear algebra trick which I will derive here. For any $N\\times N$ matrix $A$, we can express $A$ as the sum of it's symmetric and non-symmetric parts - \n",
    "\n",
    "$$A=\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T).$$\n",
    "\n",
    "The first weighted matrix is symmetric trivially - \n",
    "\n",
    "$$\\left(A+A^T\\right)^T=A^T+A=A+A^T$$\n",
    "\n",
    "while the second matrix is not - \n",
    "\n",
    "$$\\left(A-A^T\\right)^T=A^T-A=-(A-A^T).$$ Now returning to the problem at hand, we have that $$Q(x)=(x,Ax)=\\left(x,\\left(\\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T)\\right)x\\right)=\\left(x,\\frac{1}{2}(A+A^T)x\\right)+\\left(x,\\frac{1}{2}(A-A^T)x\\right)$$\n",
    "\n",
    "$$=\\left(x,\\frac{1}{2}(A+A^T)x\\right)$$ since $$Q^*(x)=\\left(x,\\frac{1}{2}(A-A^T)x\\right)=\\left(x,-\\frac{1}{2}(A^T-A)^T x\\right)=-\\left(x,\\frac{1}{2}(A-A^T)\\right)=-Q^*(x)\\iff Q^*(x)=0.$$\n",
    "\n",
    "which finally proves the statement in full breadth.\n",
    "\n",
    "### 16. Hermitian Matrices\n",
    "\n",
    "Bellman uses the notation that $\\overline{A^T}=A^*$.\n",
    "\n",
    "**1.** A real Hermitian matrix is symmetric. \n",
    "\n",
    "- This is trivially true as $\\bar{A^T}=A^T$ if $A$ is real, so $A=\\overline{A^T}=A^T$ which implies that $A$ is symmetric.\n",
    "\n",
    "**2.** Show $(A^*)^*=A$ and $(AB)^*=B^*A^*$.\n",
    "\n",
    "- Just following definition and using associativity of transpose under conjugation yields the answers.\n",
    "\n",
    "**3.** If $A+iB$ is Hermitian, $A, B$ real, then $A'=A,B'=-B$.\n",
    "\n",
    "- If $A+iB$ is Hermitian, then $A+iB=\\overline{A^T+iB^T}=A^T-iB^T\\Rightarrow A=A^T$ and $B=-B^T$.\n",
    "\n",
    "### 17. Invariance of Distance - Orthogonal Matrices\n",
    "\n",
    "Bellman defines a real matrix $T$ with the property that $T^TT=I$ as being orthogonal - this gives us a sense of what linear transformations leave the magnitude ($(x,x)=\\lVert x\\rVert_2^2 $) unchanged - i.e. they are exclusively rotating the matrix around the $N$-dimensional sphere of length $(x,x)$.\n",
    "\n",
    "**1.** Show that $T^T$ is orthogonal whenever $T$ is. \n",
    "\n",
    "- If $T$ is orthogonal, then \n",
    "\n",
    "$$0=T-T=T-TI=T-T(T^TT)=(I-TT^T)T\\iff I-TT^T=0$$\n",
    "\n",
    "since $T^TT=I$ by assumption so $T$ must have rank $N$ to begin with in order to claim the product equals the identity which has rank $N$. From here, we have that \n",
    "\n",
    "$$I-TT^T=0\\iff TT^T=I\\iff T^T \\text{ is orthogonal.}$$\n",
    "\n",
    "**2.** Show that every $2\\times 2$ orthogonal matrix with determinant $+1$ can be written in the form \n",
    "\n",
    "$$T(\\theta)=\\begin{bmatrix}\\cos\\theta&-\\sin\\theta\\\\\\sin\\theta&\\cos\\theta\\end{bmatrix}$$\n",
    "\n",
    "What is the geometric significance of this result?\n",
    "\n",
    "- $T(\\theta)\\begin{bmatrix}x\\\\y\\end{bmatrix}=\\begin{bmatrix}x\\cos\\theta - y\\sin\\theta\\\\x\\sin\\theta+y\\cos\\theta\\end{bmatrix}$ from which we can see the geomtric interpretation - this matrix is a transformation that simply rotates a vector by $\\theta^\\circ$! As for proving the statement, I will try to stick with simple algebra instead of relying on linear algebra experience outside of the bounds of this book. Let us denote our orthogonal matrix $A$ as \n",
    "\n",
    "$$A=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}$$\n",
    "\n",
    "where by definition we have that \n",
    "\n",
    "$$A^TA=\\begin{bmatrix}a^2+c^2&ab+cd\\\\ab+cd&b^2+d^2\\end{bmatrix}=\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}.$$\n",
    "\n",
    "Additionally, we have by assumption that $\\det A=ad-bc=1$. If we now make the substitution $a=\\cos\\theta, b=-\\sin\\theta,c=\\sin\\theta$ and $d=\\cos\\theta$, then we satisfy all four equations in our system.\n",
    "\n",
    "**3.** Show that the columns of $T$ are orthogonal vectors.\n",
    "\n",
    "- This problem is solved if we show using the matrix given in the previous problem - $a^2+b^2=\\cos^2\\theta+\\sin^2\\theta=1$ and $c^2+d^2=\\cos^2\\theta+(-\\sin\\theta)^2=1$. Note that if instead, we have that $ad-bc=-1$, then we simply make $c=-\\sin\\theta$ and $b=\\sin\\theta$. The determinant is always $\\pm 1$ as I will show in a later exercise.\n",
    "\n",
    "**4.** Show that the product of two orthogonal matrices is again an orthogonal matrix.\n",
    "\n",
    "- Trivial: $(AB)^TAB=B^TA^TAB=B^TB=I$ for orthogonal $A$ and $B$.\n",
    "\n",
    "**5.** Show that the determinant of an orthogonal matrix is $\\pm 1$. \n",
    "\n",
    "- From exercise 2, we have that for an arbitrary orthogonal matrix $A$, that \n",
    "\n",
    "$$A^TA=I\\iff \\det A^TA=\\det I=1\\iff \\left(\\det A\\right)^2=1\\iff \\det A=\\pm 1.$$\n",
    "\n",
    "**6.** Let $T_N$ be an orthogonal matrix of dimension $N$, and form the $(N+1)$-dimensional matrix \n",
    "\n",
    "$$T_{N+1}=\\begin{bmatrix}1&0&\\cdots&0\\\\0&&&\\\\\\vdots&&T_N&\\\\0&&&\\end{bmatrix}$$\n",
    "\n",
    "Show that $T_{N+1}$ is orthogonal.\n",
    "\n",
    "- This is annoying to type out in latex, but the idea is basically that the transpose is a matrix of the same form, but with $T_N^T$ instead of $T_N$ so that $T_{N+1}^TT_{N+1}=I_{N+1}$ after block multiplication which would imply that $T_{N+1}$ is also orthogonal. Intuitively, this is clear as we are simply appending an orthogonal basis to the collection of vectors within $T_N$.\n",
    "\n",
    "**7.** Show that if $T$ is orthogonal, $x=Ty$ implies that $y=T^Tx$.\n",
    "\n",
    "- To see this, note that by definition of orthogonality, \n",
    "\n",
    "$$x=Ty\\iff T^Tx=T^TTy\\iff T^Tx=y.$$\n",
    "\n",
    "**8.** If $AB=BA$, then $TAT^T$ and $TBT^T$ commute if $T$ is orthogonal. \n",
    "\n",
    "- Again, just applying the definition yields that if $T$ is orthogonal, then \n",
    "\n",
    "$$(TAT^T)(TBT^T)=TABT^T=TBAT^T=(TBT^T)(TAT^T).$$\n",
    "\n",
    "### 18. Unitary Matrices\n",
    "\n",
    "Skipped - similar to transpose section as unitary matrices are the complex equivalent of real orthogonal matrices.\n",
    "\n",
    "### Miscellaneous Exercises from Vectors and Matrices\n",
    "\n",
    "**1.** Not included because it's a latex pain - it's about the determinant for a particular kind of matrix by means of cofactor expansion...\n",
    "\n",
    "**2.** Let $I_{ij}$ denote the matrix obtained by interchanging the $i$th and $j$th rows of the unit matrix $I$. Prove that \n",
    "\n",
    "$$I_{ij}^2=I\\;\\;\\;\\;\\;I_{ik}I_{kj}I_{ji}=I_{kj}.$$\n",
    "\n",
    "- Letting $Y=I_{ij}^2$, then \n",
    "\n",
    "$$Y_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}(I_{ij})_{kn}.$$\n",
    "\n",
    "If $m=n$ and $(m,n)\\neq (i,j)$, then we have that $Y_{mn}=1$ as the rows and columns here were unchanged by the operation. If on the other hand, $m\\neq n$ and $m=i$ or $m=j$, then $Y_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}(I_{ij})_{kn}=0$ as the ones no longer get paired together. However if $m=n=i$ or $m=n=j$, then there is exactly one entry - notably the $(i,j)$th entry that is one and will get multiplied together in the dot product. Thus the resulting matrix is identity. Similar deduction can be applied to justify the second equality asked.\n",
    "\n",
    "**3.** Show that $I_{ij}A$ is a matrix identical to $A$ except that the $i$th and $j$th rows have been interchanged, while $AI_{ij}$ yields the interchange of the $i$th and $j$th columns.\n",
    "\n",
    "- Using the definition of matrix multiplication, we have that if $m\\notin \\{i,j\\}$, then \n",
    "\n",
    "$$(I_{ij}A)_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}A_{kn}=A_{mn}.$$\n",
    "\n",
    "On the other hand, if $m\\in\\{i,j\\}$, then the row vector of zeros and ones will be one for the swapped row, that is \n",
    "\n",
    "$$(I_{ij}A)_{mn}=\\sum\\limits_{k=1}^N (I_{ij})_{mk}A_{kn}=\\begin{cases}A_{jn}&m=i\\\\A_{in}&m=j.\\end{cases}$$\n",
    "\n",
    "Had we instead commuted the matrices $I_{ij}$ and $A$, then we would end up with the columns being interchanged by similar analysis.\n",
    "\n",
    "**4.** Let $H_{ij}$ denote the matrix whose $ij$th element is $h$, and whose other elements are zero. Show that $(I+H_{ij})A$ yields a matrix identical with $A$ except that the $i$th row has been replaced by the $i$th row plus $h$ times the $j$th row, while $A(I+H_{ij})$ has a similar effect upon the columns. \n",
    "\n",
    "- By associativity of matrix multiplication, we have that $(I+H_{ij})A=A+H_{ij}A$ where \n",
    "\n",
    "$$(H_{ij}A)_{mn}=\\sum\\limits_{k=1}^N (H_{ij})_{mk}A_{kn}=\\begin{cases} 0&m\\neq i\\\\hA_{jn}&m=i\\end{cases}$$\n",
    "\n",
    "which means that $(I+H_{ij})A$ is simply $A$ but the $i$th row becomes the $i$th row plus a constant $h$ times the $j$th row - i.e. this is a form of row reduction we encounter.\n",
    "\n",
    "**5.** Let $H_r$ denote the matrix equal to $I$ except for the fact that the element one in the position $rr$ is equal to $k$. What are $H_rA$ and $AH_r$ equal to?\n",
    "\n",
    "- This is essentially the same as 4 where instead we consider $h=k-1$, so that $I+H_{rr}=H_r$, then we have that $(I+H_{rr})A=A+H_{rr}A$ which again is the same as $A$, but the $r$th row becomes the $r$th row plus $h-1$ times the $r$th row. The same effect occurs with the columns in the latter operation. \n",
    "\n",
    "**6.** If $A$ is real and $AA^T=0$, then $A=0$.\n",
    "\n",
    "- As $A$ is real, we know that \n",
    "\n",
    "$$(AA^T)_{ii}=\\sum\\limits_{k=1}^N A_{ik}A^T_{ki}=\\sum\\limits_{k=1}^N A_{ik}^2=0\\iff A_{ik}=0\\;\\;\\forall k\\in\\{1,\\dots,N\\}.$$ \n",
    "\n",
    "Thus as this holds for all rows $i\\in\\{1,\\dots,N\\}$, we have the desired result.\n",
    "\n",
    "**7.** Same as 6 but for the conjugate transpose equivalent - if $AA^*=0$, then $A=0$.\n",
    "\n",
    "- Not doing...\n",
    "\n",
    "**8.** Show that if $T$ is orthogonal, its elements are uniformly bounded. Similarly, if $U$ is unitary, its elements are uniformly bounded in absolute value.\n",
    "\n",
    "- This follows from similar reasoning to 6) - that is, since $$(AA^T)_{ii}=\\sum\\limits_{k=1}^N A_{ik}^2=1\\Rightarrow A_{ik}\\leq 1.$$\n",
    "\n",
    "and as the elements are real, we have that each element lies in $[0,1]$ as this holds for every row.\n",
    "\n",
    "**9.** Let $d_1$ denote the determinant of the system of linear homogenous equations derived from the relation \n",
    "\n",
    "$$\\left[\\sum\\limits_{j=1}^N a_{ij}x_j\\right]\\left[a_{kj}x_j\\right]=0\\;\\;\\;\\;\\;i,k=1,2,\\dots,N$$\n",
    "\n",
    "regarding the $N(N+1)/2$ quantities $x_ix_j,i,j=1,2,\\dots,N$, as unknowns. Then $d_1=|a_{ij}|^{\\frac{N(N+1)}{2}}$.\n",
    "\n",
    "- The goal I believe is to show that the system can be expressed as the matrix $A$ to the power $N$, but I am not sure how to get there...\n",
    "\n",
    "**10.** Show $\\lim\\limits_{n\\rightarrow\\infty} A^n$, $\\lim\\limits_{n\\rightarrow\\infty} B^n$ may exist as $n\\rightarrow\\infty$, without $\\lim\\limits_{n\\rightarrow\\infty} (AB)^n$ existing. It is sufficient to take $A$ and $B$ two-dimensional.\n",
    "\n",
    "- Let $A=\\begin{bmatrix}1&1\\\\0&0\\end{bmatrix}$ and $B=\\begin{bmatrix}0&1\\\\0&1\\end{bmatrix}$. Then clearly $\\lim\\limits_{n\\rightarrow\\infty} A^n=A$ and $\\lim\\limits_{n\\rightarrow\\infty} B^n=B$ however $$\\lim\\limits_{n\\rightarrow\\infty} (AB)^n=\\lim\\limits_{n\\rightarrow\\infty} \\begin{bmatrix}0&2\\\\0&0\\end{bmatrix}^n$$ which does not exist.\n",
    "\n",
    "**11.** Suppose that we are given the system of linear equations $\\sum\\limits_{j=1}^N a_{ij}x_j=b_i$, $i=1,2,\\dots,N$. If it is possible to obtain the equations $x_1=c_1,x_2=c_2,\\dots,x_N=c_N$, by forming linear combinations of the given equations, then these equations yield a solution of the given equations, and the only solution.\n",
    "\n",
    "- I am not even sure if this is an exercise - are we supposed to prove it's unique?\n",
    "\n",
    "**12.** Introduce the Jacobi bracket symbol $[A,B]=AB-BA$, the commutator of $A$ and $B$. Show, by direct calculation, that $$[A,[B,C]]+[B,[C,A]]+[C,[A,B]]=0.$$\n",
    "\n",
    "- By applying the definition, we obtain that \n",
    "\n",
    "$$[A,[B,C]]+[B,[C,A]]+[C,[A,B]]=[A,BC-CB]+[B,CA-AC]+[C,AB-BA]$$ \n",
    "\n",
    "$$=ABC-ACB-BCA+CBA+BCA-BAC-CAB+ACB+CAB-CBA-ABC+BAC=0$$\n",
    "\n",
    "**13.** Let $r_1=e^{2\\pi i/n}$ be an irreducible root of unity, and let $r_k=e^{2\\pi ik/n}$, $k=1,2,\\dots,n-1$. Consider the matrix \n",
    "\n",
    "$$T=\\begin{bmatrix}1&1&\\cdots&1\\\\1&r_1&\\cdots&r_1^{n-1}\\\\\\vdots&&\\ddots&\\\\1&r_{n-1}&\\cdots&r_{n-1}^{n-1}\\end{bmatrix}.$$\n",
    "\n",
    "Show that $\\frac{T}{\\sqrt{n}}$ is unitary. Hence, if $x_k=\\sum\\limits_{j=0}^{n-1} e^{2\\pi ikj/n}y_j$, then $y_k=\\frac{1}{n}\\sum\\limits_{j=0}^{n-1} e^{-2\\pi i k j/n}x_k$, and $\\sum\\limits_{k=0}^{n-1} |x_k|^2=n\\sum\\limits_{k=0}^{n-1} |y_k|^2.$ This transformation is called a finite Fourier transform. \n",
    "\n",
    "- asdf\n",
    "\n",
    "**14.** Suppose that \n",
    "\n",
    "$$\\sum\\limits_{i,j=1}^N a_{ij}x_iy_i=\\sum\\limits_{k=1}^N \\left(\\sum\\limits_{s=k}^N b_{kj}x_s\\right)\\left(\\sum\\limits_{t=k}^N d_{kt}y_t\\right)$$\n",
    "\n",
    "with $b_{kk}=d_{kk}=1$ for all $k$. Then $|a_{ij}|=\\prod\\limits_{k=1}^N c_k$.\n",
    "\n",
    "- If this is the case, then $A$ is triangular. If $A$ is triangular, then by cofactor expansion we know the determinant is simply the product of the diagonal terms. In our case if $b_{kk}=d_{kk}=1$, then $c_kb_{kk}d_{kk}=c_k$ which constitute our diagonal terms from which the result follows that $|A|=\\prod c_k$.\n",
    "\n",
    "**15.** Consider the Gauss transform $B=(b_{ij})$ of the matrix $A=(a_{ij})$,\n",
    "\n",
    "$$b_{i1}=\\delta_{i1}a_{i1},\\;\\;\\;\\;b_{ik}=a_{11}^{-1}(a_{11}a_{ik}-a_{i1}a_{ik}),\\;\\;\\;\\;k>1$$\n",
    "\n",
    "Let $A_{11}=(a_{ij})$, $i,j=2,\\dots,N$. Show that \n",
    "\n",
    "$$|\\lambda I - B|=a_{11}^{-1}\\left[\\lambda\\left|\\lambda I - A_{11}\\right|-\\left|\\lambda I - A\\right|\\right].$$\n",
    "\n",
    "- In similar fashion to above, the key is to recognize the importance of cofactor expansion here. As $b_{i1}=0$ for $i\\neq 1$, we know the determinant becomes $\\lamdba - a_{11}$ times the determinant of $\\lambda I - B$ with the first row and column removed. Then ....asdf\n",
    "\n",
    "**16.** Show that the matrices \n",
    "\n",
    "$$A=\\begin{bmatrix}1&a_1&b_1\\\\0&1&0\\\\0&0&1\\end{bmatrix},\\;\\;\\;\\; B=\\begin{bmatrix} 1&-a_1&-b_1\\\\0&1&0\\\\0&0&1\\end{bmatrix}$$\n",
    "\n",
    "satisfy the relation $AB=I$.\n",
    "\n",
    "- Clearly the algebra checks off and you can easily verify it. Why Bellman brings the matrix identity into light is another question I'm not sure - they're not symmetric, yet their product is the identity or ...\n",
    "\n",
    "**17.** Show that \n",
    "\n",
    "$$\\left|\\begin{bmatrix}x_1&y_1&z_1\\\\z_1&x_1&y_1\\\\y_1&z_1&x_1\\end{bmatrix}\\right|=(x_1+y_1+z_1)(x_1+wy_1+w^2z_1)(x_1+w^2y_1+wz_1)$$\n",
    "\n",
    "$$=x_1^3+y_1^3+z_1^3-3x_1y_1z_1$$\n",
    "\n",
    "where $w$ is a cube root of unity.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**18.** Hence, show that if $Q(x)=x_1^3x_2^3+x_3^3-3x_1x_2x_3$, then $Q(x)Q(y)=Q(z)$, where $z$ is a bilinear form in the $x_i$ and $y_i$, that is, $z_i=\\sum a_{ij}x_jy_k$, where the coefficients $a_{ijk}$ are independent of the $x_i$ and $y_j$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**19.** Latex pain in the @ss, skipping\n",
    "\n",
    "- No solution...\n",
    "\n",
    "**20.** Prove that a matrix $A$ whose elements are given by the relations\n",
    "\n",
    "$$a_{ij}=\\begin{cases} (-1)^{j-1}\\binom{j-1}{i-1}&i<j\\\\(-1)^{i-1}&i=j\\\\0&i>j\\end{cases}$$\n",
    "\n",
    "satisfies the relation $A^2=I$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**21.** Let $y_i=y_i(x_1x_2,\\dots,x_N)$ be a set of $N$ functions of the $x_i$, $i=1,2,\\dots,N$. The matrix $J=J(y,x)=(\\partial y_i / \\partial x_j)$ is called the Jacobian matrix and its determinant is called the Jacobian of the transformation. Show that\n",
    "\n",
    "$$J(z,y)J(y,x)=J(z,x).$$\n",
    "\n",
    "- asdf\n",
    "\n",
    "**22.** Consider the relation between the $N^2$ variables $y_{ij}$ and the $N^2$ variables $z_{ij}$ given by $Y=AXB$, where $A$ and $B$ are constant matrices. Show that $|J(y,x)|=|A|^N|B|^N$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**23.** If $Y=XX^T$, where $X$ is triangular, then $|J(y,x)|=2^N\\prod\\limits_{i=1}^N x_{ii}^{N-i+1}$.\n",
    "\n",
    "- Holy sh\\*t, this exercise is incredibly useful/related to chapter 6 in the next section and the first problem with the spherical change of variables...\n",
    "\n",
    "**24.** Show that the problem of determining the maximum of the function $(x,Ax)-2(x,b)$ leads to the vector equation $Ax=b$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "There are 10 more exercises in this section to latex and add as well...\n",
    "\n",
    "\n",
    "## 3 - Diagonalization and Canonical Forms\n",
    "\n",
    "### 2. The Solution of Linear Homogenous Equations\n",
    "\n",
    "Here Bellman recapitulates that we desire to find the zeros (or stationary values) of the quadratic form $Q(x)=\\sum\\limits_{i,j=1}^N a_{ij}x_ix_j$ on the unit sphere $x_1^2+x_2^2+\\cdots +x_N^2=1$ which led us in chapter one to find nontrivial solutions of the linear homogenous equations \n",
    "\n",
    "$$\\sum\\limits_{j=1}^N a_{ij} x_j=\\lambda x_i\\;\\;\\;\\;i=1,\\dots,N.$$\n",
    "\n",
    "From here, we had to digress from this to learn about the power of vectors and matrices - chapter 2. Now with that power in mind, he presents the following theorem that provides much utility in resolving our problem. \n",
    "\n",
    "The takeaway from this chapter is the statement and proof of the following lemma.\n",
    "\n",
    "Lemma: A necessary and sufficient condition that the linear system\n",
    "\n",
    "$$\\sum\\limits_{j=1}^N b_{ij}x_j=0\\;\\;\\;\\; i=1,2,\\dots,N$$\n",
    "\n",
    "possess a nontrivial (at least one $x_j$ is nonzero) solution is that we have the determinantal relation\n",
    "\n",
    "$$|b_{ij}|=0.$$\n",
    "\n",
    "**1.** Show that if $A$ is real, the equation $Ax=0$ always possesses a real nontrivial solution if it possesses any nontrivial solution.\n",
    "\n",
    "- Clearly as Gaussian elimination simply takes existing entries from the matrix $(b_{ij})$ to create solutions of the form $x_1=-\\sum\\limits_{j=2}^N b_{1j}x_j/b_{11}$, we have that the solution is a linear combination of real numbers which since the reals are a field means the result will still be real.\n",
    "\n",
    "### 3. Characteristic Roots and Vectors\n",
    "\n",
    "Here Bellman rants about the ugly word \"eigenvalue\" which was a bilingual compromise between the German word \"eigenverte\" and the English word \"characteristic value\". Aside from this, he explains that we can rewrite our system of equations in the form \n",
    "\n",
    "$$Ax=\\lambda x.$$\n",
    "\n",
    "From here, we know that from the previous section's lemma, that for there to exist a nontrivial $x$ satisfying the above system of equations, it is necessary that \n",
    "\n",
    "$$|A-\\lambda I|=0.$$\n",
    "\n",
    "This equation is known as the characteristic equation of $A$. The equation is polynomial in $\\lambda$ with $N$ roots called characteristic roots/values. If the roots are distinct, we denote them as simple as opposed to multiple. With each root, we have an associated vector $x$ known as a characteristic vector. \n",
    "\n",
    "**1.** $A$ and $A^T$ have the same characteristic values. \n",
    "\n",
    "- To show they share the same characteristic values, it suffices to show they share the same characteristic equation. This is trivially true as $\\det A=\\det A^T$, so:\n",
    "\n",
    "$$|A-\\lambda I|=|(A-\\lambda I)^T|=|A^T-\\lambda I|.$$\n",
    "\n",
    "**2.** $T^TAT-\\lambda I = T^T(A-\\lambda I) T$ if $T$ is orthogonal. Hence, $A$ and $T^TAT$ have the same characteristic values if $T$ is orthogonal.\n",
    "\n",
    "- Intuitively, this makes sense as $T^TAT$ is simply a rotation of $A$ ($T$ is orthogonal), so the determinant which loosely measures volume change does not change under a rotation with scaling equal to one. As for the computation, we just note that $T^T\\lambda IT=T^TT\\lambda I=\\lambda I$. Then notice that since $|T|=|T^T|=1$,\n",
    "\n",
    "$$|T^T(A-\\lambda I)T|=|T^T|\\cdot |A-\\lambda I|\\cdot |A^T|=|A-\\lambda I|.$$\n",
    "\n",
    "**3.** $A$ and $T^*AT$ have the same characteristic values if $T$ is unitary.\n",
    "\n",
    "- This is a restatement of the above for complex matrices...\n",
    "\n",
    "**4.** $SAT$ and $A$ have the same characteristic values if $ST=I$. \n",
    "\n",
    "- Note by a similar trick to 2., we have that \n",
    "\n",
    "$$|SAT-\\lambda I|=|S(A-\\lambda I)T|=|S|\\cdot |A-\\lambda I|\\cdot |T|=|ST|\\cdot |A-\\lambda I|=|A - \\lambda I|.$$\n",
    "\n",
    "**5 and 6.** Show by direct calculation or $A$ and $B$, $2\\times 2$ matrices, that $AB$ and $BA$ have the same characteristic equation. Does the result hold in general?\n",
    "\n",
    "- Let $A=\\begin{bmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{bmatrix}$ and $B=\\begin{bmatrix}b_{11}&b_{12}\\\\b_{21}&b_{22}\\end{bmatrix}$. Then the characteristic equation of $AB$ is \n",
    "\n",
    "$$|AB-\\lambda I|=\\left| \\begin{bmatrix} a_{11}b_{11}+a_{12}b_{12}-\\lambda & a_{11}b_{12}+a_{12}b_{22}\\\\b_{11}a_{21}+b_{21}a_{22}&b_{12}a_{21}+a_{22}b_{22}-\\lambda\\end{bmatrix}\\right|$$\n",
    "\n",
    "$$=\\left(a_{11}b_{11}+a_{12}b_{12}-\\lambda\\right)\\left(b_{12}a_{21}+a_{22}b_{22}-\\lambda\\right)-\\left(a_{11}b_{12}+a_{12}b_{22}\\right)\\left(b_{11}a_{21}+b_{21}a_{22}\\right).$$\n",
    "\n",
    "While for $BA$, we have \n",
    "\n",
    "$$|BA-\\lambda I|=\\left|\\begin{bmatrix} a_{11}b_{11}+a_{21}b_{12}-\\lambda & b_{11}a_{12}+a_{22}b_{12}\\\\a_{11}b_{21}+a_{21}b_{22}&a_{12}b_{21}+a_{22}b_{22}-\\lambda \\end{bmatrix}\\right|$$\n",
    "\n",
    "$$=\\left(a_{11}b_{11}+a_{21}b_{12}-\\lambda\\right)\\left(a_{12}b_{21}+a_{22}b_{22}-\\lambda\\right)-\\left(b_{11}a_{12}+a_{22}b_{12}\\right)\\left(a_{11}b_{21}+a_{21}b_{22}\\right)$$\n",
    "\n",
    "and the result follows these are exactly the same polynomials. To prove this result in general, note that if $A$ or $B$ is invertible then WLOG $B$ is invertible so\n",
    "\n",
    "$$|AB-\\lambda I|=|B|\\cdot |AB-\\lambda I|\\cdot |B^{-1}|=|B|\\cdot |ABB^{-1}-\\lambda I B^{-2}|=|BA-B\\lambda I B^{-1}|=|BA-\\lambda I|.$$\n",
    "\n",
    "If neither are invertible, then notice we have $A+\\zeta I$ is invertible for $\\zeta\\in(0,\\epsilon)$ and $\\epsilon > 0$ as $|A|$ is a monic polynomial with at most $N$ roots, so letting $\\zeta\\rightarrow 0$, we have that \n",
    "\n",
    "$$|(A+\\zeta I)B - \\lambda I|=|B(A+\\zeta I) - \\lambda I|\\Rightarrow |AB-\\lambda I|=|BA-\\lambda I|.$$\n",
    "\n",
    "**7.** Show that any scalar multiple apart from zero of a characteristic vector is also a characteristic vector. Hence, show that we can always choose a characteristic vector $x$ so that $(x,\\overline{x})=1$.\n",
    "\n",
    "- The point is that if we know $Ax=\\lambda x$ for some $x$ and $\\lambda$, then we can multiply both sides by $\\alpha\\in\\mathcal{F}$ so that \n",
    "\n",
    "$$\\alpha Ax=\\alpha \\lambda x\\iff A(\\alpha x)=\\lambda (\\alpha x)$$\n",
    "\n",
    "which implies that once we have a given $\\lambda$ that satisfies the characteristic equation, we can simply multiply both sides of the first equation by one and arrive at a scaled vector still satisfying the relation. \n",
    "\n",
    "**8.** Show, by considering $2\\times 2$ matrices, that the characteristic roots of $A+B$ cannot be obtained in general as sums of characteristic roots of $A$ and $B$.\n",
    "\n",
    "- Let $A=\\begin{bmatrix}1&0\\\\1&1\\end{bmatrix}$ and $B=\\begin{bmatrix}-1&-1\\\\0&-1\\end{bmatrix}$. Then \n",
    "\n",
    "$$|A-\\lambda_A I|=0\\iff \\lambda_A=1,\\;\\;\\;\\; |B-\\lambda_B I|=0\\iff \\lambda_B=-1.$$\n",
    "\n",
    "However, note that \n",
    "\n",
    "$$|A+B-\\lambda_{A+B}I|=0\\iff \\lambda = \\pm 1\\neq \\lambda_A + \\lambda_B=0.$$\n",
    "\n",
    "**9.** Show that a similar comment is true for the characteristic roots of $AB$.\n",
    "\n",
    "- Let $A=\\begin{bmatrix}1&1\\\\0&1\\end{bmatrix}$ and $B=\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}$ then we have that $\\lambda_A=1$ and $\\lambda_B=0$ while $\\lambda_{AB}$ is 0 or 1. \n",
    "\n",
    "**10.** For the $2\\times 2$ case, obtain the relation between the characteristic roots of $A$ and those of $A^2$.\n",
    "\n",
    "- Denoting $A$ by $\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}$, the characteristic equation for $A$ is given by \n",
    "\n",
    "$$\\lambda^2-(a+d)\\lambda + (ad - bc)=\\lambda^2-\\text{Tr}(A)\\lambda + |A|.$$\n",
    "\n",
    "Meanwhile for $A^2$, we obtain\n",
    "\n",
    "$$|A^2-\\lambda I|=(a^2+bc-\\lambda)(d^2+bc-\\lambda)-(ab+bd)(ac+cd)=\\lambda^2-(a^2+2bc+d^2)\\lambda + (ad-bc)^2$$\n",
    "\n",
    "$$=\\lambda^2-\\left[(a+d)^2-2(ad-bc)\\right]\\lambda + (ad-bc^2)=\\lambda^2-\\text{Tr}(A^2)\\lambda + |A|^2$$\n",
    "\n",
    "or equivalently, we could have \n",
    "\n",
    "$$|A^2-\\lambda I|=\\lambda^2-\\left(\\text{Tr}(A)^2-2|A|\\right)\\lambda+|A|^2.$$\n",
    "\n",
    "Anyhow, the important thing to realize is that $\\lambda_{A^2}=\\lambda_A^2$. \n",
    "\n",
    "**11.** Does a corresponding relatin hold for the characteristic roots of $A$ and $A^n$ for $n=2,3,4,\\dots$?\n",
    "\n",
    "- Yes. Notice that \n",
    "\n",
    "$$Ax=\\lambda x\\iff A^2x=A\\lambda x=\\lambda(Ax)=\\lambda^2 x$$\n",
    "\n",
    "so we have in general that squaring a matrix results in a matrix with the eigenvalues squared.\n",
    "\n",
    "### 4. Two Fundamental Properties of Symmetric Matrices.\n",
    "\n",
    "Here Bellman introduces two fundamental results upon which all of the analysis of real symmetric matrices is built upon - basically all of statistics you care about works with these kind of matrices.\n",
    "\n",
    "**Real Roots Theorem**: The characteristic roots of a real symmetric matrix are real.\n",
    "\n",
    "**Orthgonal Eigenboys Theorem**: Characteristic vectors associated with distinct characteristic roots of a real symmetric matrix $A$ are orthogonal. \n",
    "\n",
    "\n",
    "**1.** A characteristic vector cannot be associated with two distinct characteristic values.\n",
    "\n",
    "- Assume the contrary so then there are characteristic roots $\\lambda_1$ and $\\lambda_2$ not equal to each other corresponding to the same characteristic vector $x$ for some matrix $A$. Then since $Ax=\\lambda_1x$, $Ax=\\lambda_2x$, and $Ax-Ax=0=(\\lambda_1-\\lambda_2)x$, we have that $x$ must be zero. As the eigenvector is typically defined to exclude the zero vector (as there are infinitely many eigenvalues associated with it), we arrive at a contradiction as this is only plausible if $\\lambda_1=\\lambda_2$.\n",
    "\n",
    "**2.** Show by means of a $2\\times 2$ matrix, however, that two distinct vectors can be associated with the same characteristic root.\n",
    "\n",
    "- If $\\lambda=1$ i.e. the matrix $A=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}$, then we can have $x=\\begin{bmatrix}a&0\\end{bmatrix}^T$ or $x=\\begin{bmatrix}b&0\\end{bmatrix}^T$.\n",
    "\n",
    "**3.** Show by means of an example that there exist $2\\times 2$ symmetric matrices $A$ and $B$ with the property that $|A-\\lambda B|$ is identically zero. Hence, under what conditions on $B$ can we assert that all roots of $|A-\\lambda B|=0$ are real?\n",
    "\n",
    "- For the second part, if we let $A=\\begin{bmatrix}a&b\\\\b&c\\end{bmatrix}$ and $B=\\begin{bmatrix}d&e\\\\e&f\\end{bmatrix}$, then \n",
    "\n",
    "$$|A-\\lambda B|=(a-\\lambda d)(c - \\lambda f) - (b - \\lambda e)^2=(df - e^2)\\lambda^2 + (2be-cd-af)\\lambda + (ac - b^2).$$\n",
    "\n",
    "By the discriminant in the quadratic formula, we know that the roots of this quadratic are real provided that \n",
    "\n",
    "$$(2be-cd-af)^2-4(df - e^2)(ac - b^2)\\geq 0.$$\n",
    "\n",
    "From this, we can deduce the first part as the quadratic equation is identically zero if $ac=b^2$, $df=e^2$, and $2be-cd-af=0$ - as this would imply that for any $\\lambda$, the determinant is zero.\n",
    "\n",
    "**4.** Show that if $A$ and $B$ are real symmetric matrices, and if $B$ is positive definite, the roots of $|A-\\lambda B|=0$ are all real. \n",
    "\n",
    "- If $B$ is positive definite then all eigenvalues are real. If we are using the same matrices as in problem 3 above, we have the discriminant of the characteristic equation for $B$ must be greater than or equal to zero i.e.\n",
    "\n",
    "$$(d+f)^2-4(df-e^2)\\geq 0.$$\n",
    "\n",
    "From this, we use the inequality to plug into the inequality in 3 and deduce the discriminant of $|A - \\lambda B|$ is non-negative. **Insert algebra here.**\n",
    "\n",
    "**5.** Show that the characteristic roots of a Hermitian matrix are real and that the characteristic vectors corresponding to distinct characteristic roots are orthogonal using the generalized inner product $(x,\\bar{y})$.\n",
    "\n",
    "- I could do this, but...\n",
    "\n",
    "**6.** Let the elements $a_{ij}$ of $A$ depend upon a parameter $t$. Show that the derivatives of $|A|$ with respect to $t$ can be written as the sum of $N$ determinants, where the $k$th determinant is obtained by differentiating the elements of the $k$th row and leaving the others unaltered.\n",
    "\n",
    "- I will denote the $ij$th entry by the function $f_{ij}(t)$ to symbolize that the entry is a function of $t$. The our matrix $A=(f_{ij})$ and let us also denote $|A'_i|$ for the determinant of the matrix $A$ where we differentiate the $i$th row first. Additionally, we need to denote $|A_{ij}|$ as the determinant of the matrix formed by removing the $i$th row and $j$th column. $|(A_{ij})'_k|$ denotes the determinant of the matrix $A$ where the $i$th row and $j$th column is removed and then the $k$th column is differentiated and the result is computed (just layering syntax). Now we can begin - first with the $2\\times 2$ case:\n",
    "\n",
    "$$\\frac{d}{dt}\\left|\\begin{bmatrix}f_{11}(t)&f_{12}(t)\\\\f_{21}(t)&f_{22}(t)\\end{bmatrix}\\right|=\\frac{d}{dt}\\left[f_{11}(t)f_{22}(t)-f_{12}(t)f_{21}(t)\\right]$$\n",
    "\n",
    "$$=f'_{11}(t)f_{22}(t)+f_{11}(t)f'_{22}(t)-f'_{12}(t)f_{21}(t)-f'_{21}(t)f_{12}(t)=\\left|\\begin{bmatrix}f'_{11}(t)&f'_{12}(t)\\\\f_{21}(t)&f_{22}(t)\\end{bmatrix}\\right|+\\left|\\begin{bmatrix}f_{11}(t)&f_{12}(t)\\\\f'_{21}(t)&f'_{22}(t)\\end{bmatrix}\\right|$$\n",
    "\n",
    "$$=|A'_1+A'_2|.$$\n",
    "\n",
    "Now assume it holds for some $N\\times N$ matrix $A$ i.e. that $\\frac{d}{dt}|A|=\\sum\\limits_{i=1}^N |A'_i|$. Then for $A$ of size $(N+1)\\times (N+1)$, we have using cofactor expansion on the first row\n",
    "\n",
    "$$\\frac{d}{dt}|A|=\\frac{d}{dt}\\sum\\limits_{i=1}^{N+1} (-1)^{i-1}f_{1i}(t)|A_{1i}|.$$\n",
    "\n",
    "By chain rule, we have\n",
    "\n",
    "$$\\frac{d}{dt}|A| = \\sum\\limits_{i=1}^{N+1} (-1)^{i-1}\\left[f'_{1i}(t)\\times |A_{1i}|+f_{1i}(t)\\times \\frac{d}{dt}|A_{1i}|\\right]$$\n",
    "\n",
    "From here, note that our induction step yields a formula for the derivative of the determinant of $N\\times N$ matrices. As $A_{1i}$ is $N\\times N$ where we removed the $1$st row and $i$th column, we can apply the induction step and get\n",
    "\n",
    "$$\\frac{d}{dt}|A|=\\sum\\limits_{i=1}^{N+1} (-1)^{i-1} \\left(f'_{1i}(t)|A_{1i}|\\right)+\\sum\\limits_{i=1}^{N+1} (-1)^{i-1} \\left(f_{1i}(t)\\sum\\limits_{j=2}^{N+1} \\left|(A_{1i})'_j\\right|\\right).$$\n",
    "\n",
    "From here, we can finally deduce the result - how? Notice the first sum is simply the cofactor expansion for the $(N+1)\\times (N+1)$ matrix $A$ with the first row differentiated - it equals $|A'_1|$. The latter sum is simply the same thing except we perform cofactor expansion with every row unchanged except the $j$th row which is differentiated first - it equals $\\sum\\limits_{i=2}^{N+1} |A'_i|$. Thus adding the two together yields the equality for all $N\\in\\mathbb{N}$: \n",
    "\n",
    "$$\\frac{d}{dt}|A|=\\sum\\limits_{i=1}^N |A'_i|.$$\n",
    "\n",
    "**7.** Show that the derivative of $|A-\\lambda I|$ with respect to $\\lambda$ is equal to - $\\sum\\limits_{k=1}^N |A_k-\\lambda I|$, where $A_k$ is the $(N-1)\\times (N-1)$ matrix obtained from $A$ by striking out the $k$th row and column.\n",
    "\n",
    "- Again, we can proceed by induction as I don't see a better solution. Let me denote $A_{ij}$ again as the matrix with the $i$th row and $j$th column removed. For the base case, let $A=\\begin{bmatrix} a&b\\\\c&d\\end{bmatrix}$. Then\n",
    "\n",
    "$$\\frac{d}{d\\lambda}|A-\\lambda I|=\\frac{d}{d\\lambda}\\left[ad-(a+d)\\lambda+\\lambda^2\\right]=2\\lambda-(a+d)=-\\left[(a-\\lambda)+(d-\\lambda)\\right]=\\sum\\limits_{k=1}^2 |A_k-\\lambda I|.$$\n",
    "\n",
    "Thus our base holds and let us assume for it holds for $N\\times N$ matrices. Then for an $(N+1)\\times (N+1)$ matrix $A$, we have \n",
    "\n",
    "$$\\frac{d}{dt}\\left|A - \\lambda I\\right|=\\frac{d}{d\\lambda}\\sum\\limits_{k=1}^{N+1} (-1)^{k-1}(A-\\lambda I)_{1k}\\times |(A-\\lambda I)_{1k}|$$\n",
    "\n",
    "$$=\\sum\\limits_{i=1}^{N+1}(-1)^{k-1}\\left[\\frac{d}{d\\lambda} (A-\\lambda I)_{1k}\\times|(A-\\lambda I)_{1k}|+(A-\\lambda I)_{1k}\\times \\frac{d}{d\\lambda}\\left|(A-\\lambda I)_{1k}\\right|\\right].$$\n",
    "\n",
    "This step followed by chain rule (like 6) again. From here, we notice that $\\frac{d}{d\\lambda}(A-\\lambda I)_{1k}=0$ for $k\\neq 1$ and $-1$ otherwise. Additionally, in the second component of the chain rule, we have by the induction step that for $N\\times N$ matrices\n",
    "\n",
    "$$\\frac{d}{d\\lambda}\\left|(A-\\lambda I)_{1k}\\right|=-\\sum\\limits_{j=2}^{N+1} |\\left[(A-\\lambda I)_{1k}\\right]_j-\\lambda I|.$$\n",
    "\n",
    "Combining these steps yields \n",
    "\n",
    "$$\\frac{d}{dt}\\left|A - \\lambda I\\right| = -|(A-\\lambda I)_{11}|+\\sum\\limits_{k=1}^{N+1} (-1)^{k-1}(A-\\lambda I)_{1k}(-1)\\left(\\sum\\limits_{j=2}^{N+1} \\left|\\left[(A-\\lambda I)_{1k}\\right]_j-\\lambda I\\right|\\right)$$\n",
    "\n",
    "$$=-|A_1-\\lambda I|-\\sum\\limits_{j=2}^{N+1}\\sum\\limits_{k=1}^{N+1} (-1)^{k-1}(A-\\lambda I)_{1k}\\times \\left|\\left[(A-\\lambda I)_{1k}\\right]_j-\\lambda I\\right|=-\\sum\\limits_{j=1}^{N+1} |A_j-\\lambda I|.$$\n",
    "\n",
    "**8.** From this, conclude that if $\\lambda$ is a simple root of $A$, then at least one of the determinants $|A_k-\\lambda I|$ is nonzero.\n",
    "\n",
    "- This follows from above as if all $|A_k-\\lambda I|=0$, then the point is stationary w.r.t $\\lambda$ which would imply all eigenvalues are $\\lambda$ (i.e. we have multiple roots).\n",
    "\n",
    "**9.** Use this result to show that if $\\lambda$ is a simple root of $A$, a characteristic vector $x$ associated with $\\lambda$ can always be taken to be a vector whose components are polynomials in $\\lambda$ and the elements of $A$.\n",
    "\n",
    "- Not solved, but I found a post here on the question: https://math.stackexchange.com/questions/4343471/distinct-characteristic-roots-hamilton.\n",
    "\n",
    "### 5. Reduction to Diagonal Form - Distinct Characteristic Roots.\n",
    "\n",
    "In this section, Bellman describes how for matrices with all simple/distinct characteristic roots, we can form diagonal matrices. Let $A$ be an $N\\times N$ matrix with simple characteristic roots $\\lambda_1,\\dots,\\lambda_N$ along with the set of characteristic vectors $x_1,\\dots,x_N$ which have unit length $(x_i,x_i)=1$. Then let $T=\\begin{bmatrix}x_1&\\cdots&x_N\\end{bmatrix}$, so \n",
    "\n",
    "$$T^TT=I.$$\n",
    "\n",
    "Then notice the product \n",
    "\n",
    "$$AT=\\begin{bmatrix}Ax_1&\\cdots&Ax_N\\end{bmatrix}=\\begin{bmatrix}\\lambda_1x_1&\\cdots&\\lambda_N x_N\\end{bmatrix}.$$\n",
    "\n",
    "Similarly, it follows that \n",
    "\n",
    "$$T^TAT=\\begin{bmatrix}\\lambda_1\\\\\\vdots\\\\\\lambda_N\\end{bmatrix} I$$\n",
    "\n",
    "as $x_i x_j=0$ for $i\\neq j$ and 1 otherwise. From here, we again use the fact that $T^TT=I$, to arrive at \n",
    "\n",
    "$$A=T\\begin{bmatrix}\\lambda_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\lambda_N\\end{bmatrix}T^T$$\n",
    "\n",
    "which he describes the process as reduction to diagonal form. From here on out, he uses the notation\n",
    "\n",
    "$$\\Lambda=\\begin{bmatrix}\\lambda_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\lambda_N\\end{bmatrix}.$$\n",
    "\n",
    "**1.** Show that $\\Lambda^k=\\begin{bmatrix}\\lambda_1\\\\\\vdots\\\\\\lambda_N\\end{bmatrix}^k I$ and that $A^k=T\\Gamma^k T^T$ for $k\\in\\mathbb{N}$.\n",
    "\n",
    "- We trivially have that the product of the diagonal matrices with the same entries simply raises the diagonal terms to the $k$th power. For the second question, note that for $k=2$,\n",
    "\n",
    "$$A^k=(T\\Gamma T^T)(T\\Gamma T^T)=T\\Gamma I \\Gamma T^T=T\\Gamma^2 T^T$$\n",
    "\n",
    "and this argument naturally extends by induction for all $k$.\n",
    "\n",
    "**2.** Show that if $A$ has distinct characteristic roots, then $A$ satisfies its own characteristic equation. This is a particular case of a more general result we shall obtain later on. \n",
    "\n",
    "- Haha this is a simple case of the Cayley Hamilton theorem - thank you abstract algebra (for once).\n",
    "\n",
    "**3.** If $A$ has distinct characteristic roots, obtain the set of characteristic vectors asssociated with the characteristic roots of $A^k$ for $k\\in\\mathbb{N}$.\n",
    "\n",
    "- From the previous section, we learned that if $A$ has characteristic roots $\\lambda_1,\\dots,\\lambda_N$, then we have that $A^k$ has characteristic roots $\\lambda_1^k,\\dots,\\lambda_N^k$. Thus we simply want to find the null space of $(A^k-\\lambda^k I)=(T\\Gamma T^T-\\lambda^k I)$. Notice that for $i\\in\\{1,\\dots,N\\}$, we have that $Ax_i=\\lambda x_i\\iff A^kx_i=\\lambda^k x_i$. Thus the eigenvectors are equivalent to those of $A$. However, we have swept under the rug here that these are the only eigenvalues for $A^k$ - i.e. we have found some, but is it guaranteed that this makes up all of the spectra? Yes - this is one of the spectral theorems I think Hamilton will cover (analytical/smooth transformations of the matrix yield the resulting spectra is simply given by the function applied to the original spectra).\n",
    "\n",
    "### 6. Reduction of Quadratic Forms to Canonical Form.\n",
    "\n",
    "Now that we know how to diagonalize a matrix, we can let make a change of variables to simplify the quadratic forms we have been looking at to better understand the dynamics and behavior of the function. That is, let $z=T^Tx$ where $T^T=\\begin{bmatrix} x_1&\\cdots &x_N\\end{bmatrix}^T$ that represents the eigenvectors of a matrix $A$. Then we have \n",
    "\n",
    "$$Q(x)=(x,Ax)=(Tz, ATz)=(z, T^TATz)=(z,\\Gamma z)$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N x_i\\sum\\limits_{j=1}^N A_{ij}x_j=\\sum\\limits_{i=1}^N \\lambda_i z_i^2.$$\n",
    "\n",
    "**1.** Let $A$ have distinct characteristic roots which are all positive. Use the preceding result to compute the volume of the $N$-dimensional ellipsoid $(x,Ax)=1$.\n",
    "\n",
    "- Well, we know that \n",
    "\n",
    "$$(x,Ax)=(y,\\Gamma y)=\\sum\\limits_{i=1}^N \\lambda_i y_i^2=1.$$\n",
    "\n",
    "To find the volume of the quadratic form is a pain but enlightening, so I will explain. Notice that we can start by making the change of variables of the quadratic form such that \n",
    "\n",
    "$$z_i=\\sqrt{\\lambda_i}y_i$$\n",
    "\n",
    "then the Jacobian of our inverse transformation is given by \n",
    "\n",
    "$$S=\\begin{bmatrix}\\frac{1}{\\sqrt{\\lambda_1}}&0&\\cdots&0\\\\0&\\frac{1}{\\sqrt{\\lambda_2}}&\\cdots&0\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\cdots&\\frac{1}{\\sqrt{\\lambda_N}}\\end{bmatrix}$$\n",
    "\n",
    "whose determinant is given by \n",
    "\n",
    "$$|S|=\\prod\\limits_{i=1}^N \\frac{1}{\\sqrt{\\lambda_i}}.$$\n",
    "\n",
    "Then the volume of the N-dimensional ellipsoid is given by \n",
    "\n",
    "$$\\int\\cdots\\int\\limits_{\\sum\\limits_{i=1}^N z_i^2=1} |S|dz_1\\cdots dz_n$$\n",
    "\n",
    "Now, our task is simplified to finding the volume of an $N$-dimensional unit sphere - as the above is simply $|S|$ times the volume of a unit sphere. LET US USE THE POWA OF MULTIVARIATE GAUSSIANS!! Notably, from Aditya's copied lecture notes of \"An Intermediate Course in Probability\" by Gut, the joint density of a random vector $(X_1,\\dots,X_N)^T$ that is multivariate Gaussian with mean $\\mu$ and covariance $\\Sigma$ is given by \n",
    "\n",
    "$$P(X_1=x_1,\\dots,X_N=x_N)=P(X=x)=\\frac{1}{\\sqrt{2\\pi}^N\\sqrt{|\\Sigma|}}e^{\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}.$$\n",
    "\n",
    "In the case that $\\mu=0$ and $\\Sigma=I$, we have\n",
    "\n",
    "$$P(X=x)=\\frac{1}{\\sqrt{2\\pi}^N}e^{-\\frac{1}{2}x^Tx}=\\frac{1}{\\sqrt{2\\pi}^N}e^{\\frac{1}{2}\\sum\\limits_{i=1}^N x_i^2}.$$\n",
    "\n",
    "Our goal from here is to make this look as close to our original integral as possible to prove the volume. Notice that after a change of variables $z_i=\\frac{x_i}{\\sqrt{2}}$, we have the Jacobian of inverse transformation is given by $\\sqrt{2} I$, so the determinant is $\\sqrt{2}^N$ and\n",
    "\n",
    "$$(\\sqrt{2\\pi})^N\\times \\sqrt{2}^N=2^N\\sqrt{\\pi}^N=\\int_\\mathbb{R}\\cdots\\int_\\mathbb{R} e^{-\\sum\\limits_{i=1}^N z_i^2}dz_1\\cdots dz_N.$$\n",
    "\n",
    "Notice that this integrand is symmetric about zero (i.e. $e^{-(-1)^2}=e^{-(1)^2}$), so \n",
    "\n",
    "$$\\int\\limits_\\mathbb{R} e^{-x^2}dx=2\\int\\limits_0^\\infty e^{-x^2}dx$$\n",
    "\n",
    "which simplifies our expression to \n",
    "\n",
    "$$\\sqrt{\\pi}^N = \\int\\limits_0^\\infty\\cdots\\int\\limits_0^\\infty e^{-\\sum\\limits_{i=1}^N z_i^2}dz_1\\cdots dz_N.$$\n",
    "\n",
    "An equivalent formulation of the integral on the right hand side can be uncovered through yet another change of variables to spherical coordinates. The transformation we desire is given by the following mapping \n",
    "\n",
    "$$T(z_1,\\dots,z_N)=\\begin{bmatrix}r \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_{N-1}\\end{bmatrix} = \\begin{bmatrix} \\sqrt{\\sum\\limits_{i=1}^N z_i^2}\\\\ \\cos^{-1}\\left(\\frac{z_1^2}{\\sum\\limits_{i=1}^N z_i^2}\\right)\\\\ \\cos^{-1}\\left(\\frac{z_2^2}{\\sum\\limits_{i=2}^N z_i^2}\\right)\\\\ \\vdots\\\\ \\cos^{-1}\\left(\\frac{z_{N-1}}{\\sum\\limits_{i=N-1}^N z_i^2}\\right)\\end{bmatrix}.$$\n",
    "\n",
    "Then the inverse transformation $S(r,\\theta_1,\\dots,\\theta_{N-1})$ can be found as \n",
    "\n",
    "$$z_i = \\cos(\\theta_i)\\prod\\limits_{j=1}^{i-1} \\sin(\\theta_j)$$ \n",
    "\n",
    "for $i>1$ and for $i=1$, $z_1=\\cos(\\theta_1)$ and so the Jacobian is given by \n",
    "\n",
    "$$S=\\begin{bmatrix} \\cos(\\theta_1)&-r\\sin(\\theta_1)&\\cdots&0\\\\ \\sin(\\theta_1)\\cos(\\theta_2)& r\\cos(\\theta_1)\\cos(\\theta_2)&\\cdots&0\\\\\\vdots &\\vdots &\\ddots&\\vdots\\\\ \\sin(\\theta_1)\\sin(\\theta_2)\\cdots\\sin(\\theta_{n-1})&r\\cos(\\theta_1)\\sin(\\theta_2)\\cdots\\sin(\\theta_{N-1})&\\cdots&r\\sin(\\theta_1)\\sin(\\theta_2)\\cdots\\cos(\\theta_{N-1})\\end{bmatrix}$$\n",
    "\n",
    "Now you may be thinking no way am I computing that and you're right, I'm not as this is trivial by induction. In the case where $N=2$, you know this is just $r^2\\sin(\\theta_1)$ and by use of cofactor expansion like in the proofs in section 5 of this chapter I used earlier, we can inductively prove that\n",
    "\n",
    "$$|S_N|=r^{N-1}\\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i).$$\n",
    "\n",
    "Thus our integral becomes\n",
    "\n",
    "$$\\int\\limits_0^\\infty \\int\\limits_0^{2\\pi}\\cdots\\int_0^\\pi e^{-r^2}\\left[r^{N-1}\\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i)\\right]d\\theta_1\\cdots d\\theta_{N-1}dr.$$\n",
    "\n",
    "Notice that \n",
    "\n",
    "$$\\int\\limits_0^\\infty e^{-r^2}r^{N-1}dr=\\int\\limits_0^\\infty e^{-r^2}(r^2)^{\\frac{N-1}{2}}dr=\\frac{1}{2}\\Gamma(\\frac{N+1}{2})=\\Gamma(\\frac{N}{2}+1),$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\\frac{\\sqrt{\\pi}^N}{\\Gamma(\\frac{N}{2}+1)}=\\int\\limits_0^{2\\pi}\\cdots\\int\\limits_0^\\pi \\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i)d\\theta_1\\cdots d\\theta_{N-1}=\\int\\limits_0^1\\int\\limits_0^{2\\pi}\\cdots\\int\\limits_0^\\pi \\prod\\limits_{i=1}^{N-2} \\sin^{n-i-1}(\\theta_i)d\\theta_1\\cdots d\\theta_{N-1}dr$$\n",
    "\n",
    "where the last step follows as $\\int_0^1 dr=1$, so we are just using a multiply by one trick. This trick helps us recognize the right-hand side as exactly the volume of the unit sphere (we changed variables and the radius goes from zero to one). Thus, we can finally combine our steps to get the volume of the N-dimensional ellipsoid:\n",
    "\n",
    "$$\\int\\cdots\\int\\limits_{\\sum\\limits_{i=1}^N z_i^2=1} |S|dz_1\\cdots dz_n=\\prod\\limits_{i=1}^N \\frac{1}{\\sqrt{\\lambda_i}}\\times \\frac{\\sqrt{\\pi}^N}{\\Gamma(\\frac{N}{2}+1)}.$$\n",
    "\n",
    "\n",
    "**2.** Prove along the preceding lines that the characteristic roots of Hermitian matrices are real and that characteristic vectors associated with distinct characteristic roots are orthogonal in terms of the notation $[x,y]$ of Sec. 16 of Chap. 2.\n",
    "\n",
    "- Not doing.\n",
    "\n",
    "**3.** Show that if the characteristic roots of a Hermitian matrix $A$ are distinct, we can find a unitary matrix $T$ such that $A=T\\Gamma T^*$. This again is a particular case of a more general result we shall prove in the following chapter.\n",
    "\n",
    "- If the characteristic roots $\\lambda_1,\\dots,\\lambda_N$ are distinct, let $(x_1,\\dots,x_N)$ denote the respective eigenvectors that are normalized ($x_i\\bar{x_i}=1$) which implies that $T$ is unitary as $T^*T=I$. Define $T=(x_1,\\dots,x_N)$, so then \n",
    "\n",
    "$$AT=(Ax_1,\\dots,Ax_N)=(\\lambda_1x_1,\\dots,\\lambda_Nx_N)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$T^*AT=(\\lambda_1(x_1,\\bar{x_1}),\\dots,\\lambda_N(x_1,\\bar{x_N}))=\\begin{bmatrix}\\lambda_1&&\\\\&\\ddots&\\\\&&\\lambda_N\\end{bmatrix}.$$\n",
    "\n",
    "From here, we multiply the left by $T$ and the right by $T^*$ to get that\n",
    "\n",
    "$$A=T\\begin{bmatrix}\\lambda_1&&\\\\&\\ddots&\\\\&&\\lambda_N\\end{bmatrix}T^*$$\n",
    "\n",
    "as desired.\n",
    "\n",
    "**4.** Let $A$ be a real matrix with the property that $A^T=-A$, a *skew-symmetric* matrix. Show that the characteristic roots are either zero or pure imaginary.\n",
    "\n",
    "- Clearly since \n",
    "\n",
    "$$A=S\\Gamma S^T=A^T=-A$$\n",
    "\n",
    "we have that $a_{ij}=-a_{ij}$ where $a_{ij}\\in\\mathbb{R}$ if the characteristic values are zero or pure imaginary. The former is easy to see, but for the latter, notice that we still have for pure imaginary numbers that the real part is zero, so $a_{ij}=-a_{ij}$ as the entries $a_{ij}$ only consist of real numbers.\n",
    "\n",
    "**5.** Let $T$ be an orthogonal matrix. Show that all characteristic roots have absolute value one.\n",
    "\n",
    "- If $T$ is orthogonal, then let $A$ denote the characteristic vectors normalized by one corresponding to eigenvalues $\\lambda_1,\\dots,\\lambda_K$. Then notice that\n",
    "\n",
    "$$T^TT=I=(A\\Gamma A^T)^2=A\\Gamma^2A^T$$\n",
    "\n",
    "which occurs iff $1=\\lambda_i^2\\iff \\lambda_i = \\pm 1$.\n",
    "\n",
    "**6.** Let $T$ be a unitary matrix. Show that all characteristic roots have absolute value one.\n",
    "\n",
    "- Same as 5.\n",
    "\n",
    "**7.** Let $A$ be a symmetric matrix without necessarily simple characteristic roots. Explain how to obtain the diagonal representation as in chapter 5 - $A=S\\Gamma S^T$.\n",
    "\n",
    "- To begin with, we can assert that we can always find a symmetric matrix $B$, with elements arbitrarily small, possessing the property that $A+B$ has simple characteristic roots. Let $\\mu_1,\\dots,\\mu_N$ be the characteristic roots of $A+B$. Then there exists an orthogonal matrix $S$ such that \n",
    "\n",
    "$$A+B=S\\begin{bmatrix}\\mu_1&\\cdots&0\\\\\\vdots&\\ddots&\\vdots\\\\0&\\cdots&\\mu_N\\end{bmatrix}S^T.$$\n",
    "\n",
    "As $S$ is an orthogonal matrix, its elements are uniformly bounded by one (we showed this earlier). Let $\\{B_n\\}_{n\\in\\mathbb{N}}$ be a sequence of matrices approaching 0 such that the corresponding sequence of orthogonal matrices $\\{S_n\\}_{n\\in\\mathbb{N}}$ converges. The limit matrix must be an orthogonal matrix, say $T$. Since $\\lim\\limits_{n\\rightarrow\\infty} \\mu_i=\\lambda_i$, we have \n",
    "\n",
    "$$A=\\lim\\limits_{n\\rightarrow\\infty} (A+B_n)=\\lim\\limits_{n\\rightarrow\\infty} S_n\\begin{bmatrix} \\lim\\limits_{n\\rightarrow\\infty} \\mu_1&&\\\\&\\vdots&\\\\&&\\lim\\limits_{n\\rightarrow\\infty} \\mu_N\\end{bmatrix}\\lim\\limits_{n\\rightarrow\\infty} S_n^T.$$\n",
    "\n",
    "As Bellman notes, this proof relies on core analytic concepts - the existence of such a $B$ to ensure $A+B$ have distinct simple roots and the existence of the two sequences of matrices $\\{B_n\\}$ and $\\{S_n\\}$ which are not yet fleshed out in the book. However, if you remember from second semester analysis, the Stone-Weierstrass theorem in full generality, the notion of algebras and compactness should be enough to convince you this should be true.\n",
    "\n",
    "### 7. Positive Definite Quadratic Forms and Matrices\n",
    "\n",
    "Here Bellman simply defines the concept of a posiitive definite quadratic form for $N$-dimensional quadratic forms. If $A$ is a real symmetric matrix, and \n",
    "\n",
    "$$Q_N(x)=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N A_{ij}x_ix_j>0$$\n",
    "\n",
    "for all real and nonzero $x_i$, we shall say that $Q_N(x)$ is positive definite and that $A$ is positive definite. If $Q_N(x)\\geq 0$, we shall say that $Q_N(x)$ and $A$ are positive indefinite. Similarly we have the same for complex matrices - if $B$ is Hermitian and\n",
    "\n",
    "$$P_N(x)=\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N B_{ij}x_i\\bar{x}_j>0$$\n",
    "\n",
    "for all complex and nonzero $x_i$, we say that $P_N(x)$ and $B$ are positive definite.\n",
    "\n",
    "**1.** If $A$ is a symmetric matrix with distinct characteristic roots, obtain a set of necessary and sufficient conditions that $A$ be positive definite.\n",
    "\n",
    "- As before, let $y=T^Tx$ where $T$ consists of the eigenvectors of $A$, so then\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^N A_{ij}=\\sum\\limits_{i=1}^n \\lambda_iy_i^2.$$\n",
    "\n",
    "As $y_i^2>0$ for all non-trivial $x$, a sufficient condition for $A$ to be positive definite is that all eigenvalues be positive (or most are zero, but at least one is positive). \n",
    "\n",
    "**2.** There exists a scalar $c_1$ such that $A+c_1I$ is positive definite, given any symmetric matrix $A$.\n",
    "\n",
    "- Let us assume the contrary - that $A$ is not positive definite. Then \n",
    "\n",
    "$$x^T(A+c_1I)x\\leq 0$$\n",
    "\n",
    "for all $x\\in\\mathbb{R}^N$. Notice that this also implies that\n",
    "\n",
    "$$a_{ii}+c_1\\leq 0$$\n",
    "\n",
    "for all $i\\in\\{1,\\dots,N\\}$ and $c_1\\in\\mathbb{R}$. This forms a contradiction as we can always let $c_1=-(a_{ii}+1)$ if $a_{ii}$ is negative which implies that $a_{ii}+c_1=1\\nleq 0$.\n",
    "\n",
    "**3.** Show that we can write $A$ in the form\n",
    "\n",
    "$$A=\\sum\\limits_{i=1}^N \\lambda_iE_i$$\n",
    "\n",
    "where the $E_i$ are non-negative definite matrices. Then\n",
    "\n",
    "$$A^k=\\sum\\limits_{i=1}^N \\lambda_i^k E_i$$\n",
    "\n",
    "for $k=1,2,\\dots$\n",
    "\n",
    "- By the spectral theorem (which hasn't been proven yet), we know that $A=\\sum\\limits_{i=1}^N \\lambda_i x_ix_i^T$ where we know that $x_ix_i^T$ is non-negative definite as for all $a\\in\\mathbb{R}^N$, we have\n",
    "\n",
    "$$a^T(x_ix_i^T)a=(a^Tx_i)(x_i^Ta)=(x_i^Ta)^T(x_i^Ta)\\geq 0.$$\n",
    "\n",
    "As for the latter part, notice that all the cross terms resulting from $\\left(\\sum\\limits_{i=1}^N \\lambda_i x_ix_i^T\\right)^k$ will vanish as the eigenvectors are orthonormal. On a similar note, for the diagonal terms, we have\n",
    "\n",
    "$$(\\lambda x_ix_i^T)^k=\\lambda^k(x_ix_i^T)(x_ix_i^T)\\cdots(x_ix_i^T)=\\lambda^k x_i(x_i^Tx_i)\\cdots(x_i^Tx_i)x_i^T=\\lambda^k x_ix_i^T$$\n",
    "\n",
    "and so $A^k=\\sum \\lambda_i^k x_ix_i^T$ as desired.\n",
    "\n",
    "**4.** If $p(\\lambda)$ is a polynomial in $\\lambda$ with scalar coefficients, $p(\\lambda)=\\sum\\limits_{k=0}^m c_k\\lambda^k$, let $p(A)$ denote the matrix $p(A)=\\sum\\limits_{k=0}^m c_k A^k$. Show that $p(A)=\\sum\\limits_{i=1}^N p(\\lambda_i)E_i$.\n",
    "\n",
    "- This follows trivially from question 3 as\n",
    "\n",
    "$$p(A)=\\sum\\limits_{k=0}^m c_k\\sum\\limits_{i=1}^N \\lambda_i^k E_i=\\sum\\limits_{i=1}^N \\sum\\limits_{k=0}^m c_k\\lambda_i^k E_i=\\sum\\limits_{i=1}^N p(\\lambda)E_i.$$\n",
    "\n",
    "## Miscellaneous Exercises from Diagonalization and Canonical Forms\n",
    "\n",
    "**1.** Let $A$ and $B$ be two symmetric matrices. Then the roots of $|A-\\lambda B|=0$ are all real if $B$ is positive definite. What can be said of the vectors satisfying the relations $Ax=\\lambda_i Bx$.\n",
    "\n",
    "- Let us consider two distinct roots $\\lambda_1$ and $\\lambda_2$ of the system $|A-\\lambda B|$. Then, we have that $Ax=\\lambda_1Bx$ and $Ay=\\lambda_2By$ for some vectors $x$ and $y$. This implies that\n",
    "\n",
    "$$(y,Ax)=(y,\\lambda_1Bx)=\\lambda_1(y,Bx)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$(x,Ay)=(x,\\lambda_2 By)=\\lambda_2(x,By)$$\n",
    "\n",
    "so \n",
    "\n",
    "$$(y,Ax)-(x,Ay)=0=\\lambda_1(y,Bx)-\\lambda_2(x,By)=(\\lambda_1-\\lambda_2)(x,By)\\iff (x,By=0).$$\n",
    "\n",
    "This is slightly different of course from the proof of why characteristic vectors corresponding to distinct characteristic values are orthogonal. Instead here, we could consider $B$ to be the identity matrix, in which case we would have $(x,y)=0$. \n",
    "\n",
    "\n",
    "**2.** Every matrix is uniquely expressible in the form $A=H+S$, where $H$ is Hermitian and $S$ is skew-Hermitian, that is, $S^*=-S$.\n",
    "\n",
    "- Let $H=\\frac{1}{2}(A+A^*)$ and $S=\\frac{1}{2}(A-A^*)$. Then we have\n",
    "\n",
    "$$H^*=\\overline{\\left(\\frac{1}{2}(A+\\overline{A^T})\\right)^T}=\\overline{\\frac{1}{2}(A^T+(\\overline{A^T})^T)}=\\frac{1}{2}(\\overline{A^T+\\overline{A}})=\\frac{1}{2}(A^*+A)=H.$$\n",
    "\n",
    "Letting $S=\\frac{1}{2}(A-A^*)$ so that $A=H+S$, we have\n",
    "\n",
    "$$S^*=\\overline{\\left(\\frac{1}{2}(A)\\right)^T}=\\frac{1}{2}\\overline{A^T-\\overline{A}}=\\frac{1}{2}(A^*-A)=-S.$$\n",
    "\n",
    "\n",
    "**3.** As an extension of the Real Roots theorem, show that if $\\lambda$ is a characteristic root of a real matrix $A$, then\n",
    "\n",
    "$$|Im(\\lambda)|\\leq \\left(\\max\\limits_{1\\leq i,j\\leq N} |\\frac{a_{ij}-a_{ji}}{2}|\\right)\\left(\\frac{N(N+1)}{2}\\right)^{1/2}.$$\n",
    "\n",
    "- This is solved in 4 as we can decompose a real matrix into the sum of a symmetric and skew-symmetric matrix from which we can bound the real and imaginary parts of the eigenvalues.\n",
    "\n",
    "**4.** Generally, let $A$ be complex; then if \n",
    "\n",
    "$$d_1=\\max\\limits_{1\\leq i,j\\leq N} |a_{ij}|,\\;\\;\\;\\; d_2=\\max\\limits_{1\\leq i,j\\leq N} \\frac{|a_{ij}+\\bar{a_{ji}}|}{2}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$d_3=\\max\\limits_{1\\leq i,j\\leq N} \\frac{|a_{ij}-\\bar{a_{ji}}|}{2},$$\n",
    "\n",
    "we have \n",
    "\n",
    "$$|\\lambda |\\leq Nd_1,\\;\\;\\;|Re(\\lambda)|\\leq Nd_2|,\\;\\;\\;|Im(\\lambda)|\\leq Nd_3.$$\n",
    "\n",
    "- For the first inequality, let $x$ be a normalized eigenvector then notice that \n",
    "\n",
    "$$Ax=\\lambda x\\iff |Ax|=|\\lambda x| \\iff \\left|\\sum\\limits_{i=1}^N A_{ij}x_j\\right|=\\left|\\lambda x\\right|$$\n",
    "\n",
    "$$\\iff |\\lambda|=\\left|\\sum\\limits_{j=1}^N A_{ij}x_j\\right|\\leq \\sum\\limits_{j=1}^N |A_{ij}|\\leq N\\times\\max\\limits_{1\\leq i,j\\leq N} |A_{ij}|.$$\n",
    "\n",
    "For the latter two inequalities, notice that for any complex matrix, we showed above, that we can express it in the form $A=H+S$ where $H$ is Hermitian and $S$ is skew-Hermitian. Thus, it suffices to show that for a complex eigenvalue $\\lambda=a+bi$, we have $Hx=ax$ and $Sx=(bi)x$, so that $Ax=(H+S)x=\\lambda x$. Then we use the first inequality to bound the real and complex parts by the matrices $H$ and $S$ respectively. Notice that\n",
    "\n",
    "$$Hx=\\frac{1}{2}(A+A^*)x=\\frac{1}{2}\\lambda x+\\frac{1}{2}\\overline{\\lambda}x=\\text{Re }(\\lambda) x$$\n",
    "\n",
    "and\n",
    "\n",
    "$$Sx=\\frac{1}{2}(A-A^*)=\\text{Im }(\\lambda) x$$\n",
    "\n",
    "so then we have proven the latter two inequalties using the same method for the first with the Hermitian and skew-Hermitian matrices instead. \n",
    "\n",
    "\n",
    "**5.** Show that for any complex matrix $A$, we have the inequalities\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N |\\lambda_i|^2\\leq \\sum\\limits_{i,j=1}^N |a_{ij}|^2,$$\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N |Re(\\lambda_i)|^2\\leq \\sum\\limits_{i,j=1}^N |\\frac{a_{ij}-\\bar{a_{ji}}}{2}|^2,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\sum\\limits_{i=1}^N |Im(\\lambda_i)|^2\\leq \\sum\\limits_{i,j=1}^N |\\frac{a_{ij}-\\bar{a_{ji}}}{2}|^2.$$\n",
    "\n",
    "- This follows from similar reasoning to the prior inequality. If we prove the first inequality, then the latter two follow immediately by expressing the matrix as the sum of it's Hermitian and skew-Hermitian parts. Principally, we have for each eigenvalue and normalized eigenvector pair $(\\lambda_i, x_i)$, that\n",
    "\n",
    "$$|Ax|=|\\lambda x|\\iff |\\lambda x|^2=|Ax|^2\\iff |\\lambda|\\leq \\left(\\sum\\limits_{j=1}^N |A_{ij}|\\right)^2=\\sum_j |A_{ij}|^2+\\sum\\limits_{j\\neq k} A_{ij}A_{ik}$$\n",
    "\n",
    "$$\\leq \\sum\\limits_{1\\leq i,j\\leq N} |A_ij|^2.$$\n",
    "\n",
    "Again, similar to 4 - from here we simply decompose $A$ into $H$ and $S$ to obtain the latter two inequalities.\n",
    "\n",
    "**6.** So far, we have not dismissed the possibility that a characteristic root may have several associated characteristic vectors, not all multiples of a particular characteristic vector. As we shall see, this can happen if $A$ has multiple characteristic roots. For the case of distinct characteristic roots, this cannot occur. Although the simplest proof uses concepts of the succeeding chapter, consider a proof along the following lines:\n",
    "\n",
    "(a) Let $x^1$ and $y$ be two characteristic vectors associated with $\\lambda_1$ and suppose that $y\\neq c_1x^1$ fir sine scalar $c_1$. Then $x^1$ and $z=y-\\frac{x^1(x^1,y)}{(x^1,x^1)}$ are characteristic vectors and $x^1$ and $z$ are orthogonal.\n",
    "\n",
    "(b) Let $z^1$ be the normalized multiple of $z$. Then \n",
    "\n",
    "$$S=\\begin{bmatrix}x^1&z&x^3&\\cdots&x^N\\end{bmatrix}$$\n",
    "\n",
    "is an orthogonal transformation.\n",
    "\n",
    "(c) $A=SDS^T$, where\n",
    "\n",
    "$$D=\\begin{bmatrix} \\lambda_1&0&0&\\cdots&0\\\\ 0&\\lambda_1&0&\\cdots&0\\\\ 0&0&\\lambda_3&\\cdots&0\\\\\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\ 0&0&0&\\cdots&\\lambda_N\\end{bmatrix}$$\n",
    "\n",
    "(d) It follows that the transformation $x=Sy$ changes $(x,Ax)$ into $(y,Sy)$.\n",
    "\n",
    "(e) Assume that $A$ is positive definite (if not, consider $A+c_1I$) then, on one hand, the volume of the ellipsoid $(x,Ax)=1$ is equal to the volume of the ellipsoid\n",
    "\n",
    "$$\\sum\\limits_{i=1}^N \\lambda_i y_i^2=1,$$\n",
    "\n",
    "and, on the other hand, from what has just preceded, is equal to the volume of \n",
    "\n",
    "$$\\lambda_1y_1^2+\\lambda_1y_1^2+\\cdots+\\lambda_Ny_N^2=1$$\n",
    "\n",
    "This is a contradiction if $\\lambda_1\\neq \\lambda_2$.\n",
    "\n",
    "- For (a), we must show that $z$ is a characteristic vector and the two are orthogonal. Note that\n",
    "\n",
    "$$Az=A\\left(y-x^1\\frac{(x^1,y)}{(x^1,x^1)}\\right)=\\lambda y - \\lambda x^1\\frac{(x^1,y)}{(x^1,x^1)}=\\lambda z$$\n",
    "\n",
    "so $\\lambda$ is the eigenvalue corresponding to the eigenvector $z$. Now to see orthogonality,\n",
    "\n",
    "$$(x^1,z)=(x^1,y)-\\frac{(x^1,y)}{(x^1,x^1)}(x^1,x^1)=(x^1,y)-(x^1,y^1)=0.$$\n",
    "\n",
    "For step (b), we know from the eigenboys theorem that the eigenvectors are orthogonal, and so the transformation $S$ consisting of $N$ eigenvectors is indeed an orthogonal transformation by definition.\n",
    "\n",
    "For step (c), we know the transformation can be normalized from whence we can diagonalize $A$ in similar fashion to section 5 of this chapter - $SDS^T$.\n",
    "\n",
    "For step (d), Bellman makes a **major** typo :0 - even goats make mistakes. Anyways, we end up with \n",
    "\n",
    "$$(x,Ax)=(Sy, ASy)=(y, S^TASy)=(y,DY)\\neq (y,Sy).$$\n",
    "\n",
    "For step (e), clearly $(y,Sy)= \\lambda_1^2y_1^2+\\lambda_1^2y_1^2+\\cdots +\\lambda_Ny_N^2$. I do not think the first case is necessarily possible as not all positive definite matrices have $N$ distinct characteristic roots - take the identity matrix for instance. Thus, this proof is garbage - he needs a better editor ;). \n",
    "\n",
    "## Reduction of General Symmetric Matrices to Diagonal Form\n",
    "\n",
    "### 2. Linear Dependence\n",
    "\n",
    "Let $x^1,\\dots,x^k$ be a set of $k$ $N$-dimensional vectors. If a set of scalars, $c_1,\\dots,c_k$, where at least one $c_i$ is nonzero with the property that\n",
    "\n",
    "$$c_1x^1+\\cdots + c_ix^i+\\cdots +c_kx^k=0$$\n",
    "\n",
    "we say that the vectors are linearly dependent. If no such set of scalars exist satisfying the above property, the we say the vectors are linearly independent.\n",
    "\n",
    "**1.** Show that any set of mutually orthogonal nontrivial vectors is linearly independent.\n",
    "\n",
    "- Consider $N$ vectors, $x_1,\\dots,x_N$, that are mutually orthogonal. Assume the contrary - that there are vectors $x_i$ and $x_N$ that have nonzero coefficients $a_i$ and $a_N$ such that $\\sum a_i x_i=0$. Then $x_i=-\\frac{a_Nx_N}{a_i}$, but note that this implies that\n",
    "\n",
    "$$(x_i,x_N)=(-\\frac{a_Nx_N}{a_i}, x_N)=-\\frac{a_N}{a_i}(x_N, x_N)\\neq 0$$\n",
    "\n",
    "which is a contradiction as it was assumed the $N$ vectors were mutually orthogonal. \n",
    "\n",
    "**2.** Given any nontrivial vector in $N$-dimensional space, we can always find $N-1$ vectors which together with the given $N$-dimensional vector constitute a linearly independent set.\n",
    "\n",
    "- We can via Gram-Schmidt as discussed below. Another method would be as follows: 1) initialize $x_1$, the first vector. Then 2) find a nonzero $x_2$ such that $ax_1\\neq x_2$, there are many, many of these. 3) Find a nonzero $x_3$ such that $ax_1+bx_2\\neq x_3$ and iteratively repeat this process. To find the $x_i$, we can use Gaussian elimination to ensure the system has no solution or we could also consider the matrix $\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_i\\end{bmatrix}$ and ensure that the determinant is nonzero by cofactor expansion along the last row.\n",
    "\n",
    "### 3. Gram-Schmidt Orthogonalization\n",
    "\n",
    "This method takes as an input $N$ linearly independent vectors and outputs a set of $N$ mutually orthogonal vectors (which are of course also linearly independent). Let $x^1,\\dots , x^N$ denote a set of $N$ real linearly independent vectors. Now let me describe the process:\n",
    "\n",
    "- First, take one of the vectors, say $x^1$ and assign it to be $y^1$.\n",
    "\n",
    "- Now we want to create a second vector $y^2$ to be orthogonal to $y^1$ (i.e. perpendicular/dot product/inner product is zero), so let $y^2=a_{11}x^1+x^2$ such that\n",
    "\n",
    "$$(y^1,y^2)=(x^1, a_{11}x^1+x^2)=0\\iff a_{11}=-\\frac{(x^1,x^2)}{(x^1,x^1)}.$$\n",
    "\n",
    "- Again, now we have a set of two vectors that are orthogonal derived from the linearly independent set - notice that this $a_{11}$ is closely intertwined with the OLS solution as it is simply the projection of $y$ onto $x$. Anyhow, let $y^3=a_{21}x^1+a_{22}x^2+x^3$. Then we want to ensure that this vector is orthogonal to $y^1$ and $y^2$, so we require that\n",
    "\n",
    "$$(y^1,y^3)=(x^1,y^3)=0\\iff (x^1,a_{22}x^2)+(x^1,a_{21}x^1)+(x^1,x^3)=0$$\n",
    "\n",
    "and \n",
    "\n",
    "$$(y^2,y^3)=(a_{11}x^1+x^2,y^3)=0\\iff^* (x^2,y^3)=0\\iff (x^2,a_{22}x^2)+(x^2,a_{21}x^1)+(x^2,x^3)=0.$$\n",
    "\n",
    "where $\\iff^*$ follows as the first requirement, $(y^1,y^3)=0$, shows this. This is a system of two equations with two unknowns $a_{21}$ and $a_{22}$. To find these, we simply express the system in matrix notation\n",
    "\n",
    "$$S_2=\\begin{bmatrix} (x^1,x^1)&(x^1,x^2)\\\\(x^1,x^2)&(x^2,x^2)\\end{bmatrix}$$\n",
    "\n",
    "which is solvable provided \n",
    "\n",
    "$$|S|=(x^1,x^1)(x^2,x^2)-(x^1,x^2)^2\\neq 0\\iff (x^1,x^1)(x^2,x^2)\\neq (x^1,x^2)^2$$\n",
    "\n",
    "which true as $x^1$ and $x^2$ are linearly independent. Thus, row reducing yields that\n",
    "\n",
    "$$a_{21}=-\\frac{(x^2,x^3)+\\frac{(x^1,x^3)(x^2,x^2)}{(x^1,x^2)}}{(x^1,x^2)-\\frac{(x^1,x^1)(x^2,x^2)}{(x^1,x^2)}}$$\n",
    "\n",
    " and \n",
    " \n",
    " $$a_{22}=-\\frac{(x^1,x^3)+(a_{21}(x^1,x^1))}{(x^1,x^2)}.$$\n",
    " \n",
    " We continue this iterative process (which as you can imagine is quite slow) until we end up with $N$ mutually orthogonal vectors $y^1,\\dots,y^N$. We can then normalize this set of orthogonal vectors - this is known as an orthonormal set of vectors. Additionally, we denote |S_k|, the determinant of the system, as the Gramians.\n",
    "\n",
    "\n",
    "**1.** Consider the interval $[-1,1]$ and define the inner product of two real functions $f(t)$ and $g(t)$ in the following way:\n",
    "\n",
    "$$(f,g)=\\int\\limits_{-1}^1 f(t)g(t)\\;dt$$\n",
    "\n",
    "Let $P_0(t)=0.5$ and define the other elements of the sequence of real polynomials $|P_n(t)|$ by the condition that $P_n(t)$ is of degree $n$ and $(P_n,P_m)=0$, $m\\neq n$, $(P_n,P_n)=1$. Prove that we have $(P_n,t^m)=0$, for $0\\leq m\\leq n-1$, and construct the first few members of the sequence in this way. \n",
    "\n",
    "- Notice that we are constructing an orthonormal basis for the space of polynomials defined under this particular inner product. With this in mind, we see that \n",
    "\n",
    "$$t^m=\\alpha_1 P_m(t) + \\alpha_2 P_{m-1}(t) + \\cdots +\\alpha_{m+1} P_0(t)$$\n",
    "\n",
    "for some constants $\\alpha_1,\\dots,\\alpha_{m+1}$ and so\n",
    "\n",
    "$$(P_n,t^m)=\\left(P_n,\\alpha_1 P_m + \\alpha_2 P_{m-1} + \\cdots +\\alpha_{m+1} P_0\\right)=\\sum\\limits_{i=1}^{m+1} \\alpha_i (P_n,P_{m+1-i})=0.$$\n",
    "\n",
    "Now we would like to concretely construct the bases. First, we have for $P_1$ that\n",
    "\n",
    "$$(P_0,P_1)=\\int\\limits_{-1}^1 \\frac{1}{2}(a_0t^0+a_1t^1)\\;dt=\\frac{1}{2}\\left[\\right]_{-1}^1=a_0=0$$\n",
    "\n",
    "and \n",
    "\n",
    "$$(P_1,P_1)=\\int\\limits_{-1}^1 (a_1t^1)^2\\;dt=1\\iff a_1=\\pm\\sqrt{\\frac{3}{2}}$$\n",
    "\n",
    "which determines our polynomial of degree 1 - $P_1(t)=\\sqrt{\\frac{3}{2}}t$. For the next polynomial, we must check that $(P_0,P_2)=(P_1,P_2)=0$ and $(P_2,P_2)=1$. We have\n",
    "\n",
    "$$\\int\\limits_{-1}^1 \\frac{1}{2}\\left(a_0t^0+a_1t^1+a_2t^2\\right)\\;dt=\\frac{1}{2}\\left(2a_0+0+\\frac{2}{3}a_2\\right)=0\\iff a_0=-\\frac{a_2}{3}.$$\n",
    "\n",
    "We also have\n",
    "\n",
    "$$\\int\\limits_{-1}^1 \\sqrt{\\frac{3}{2}}t\\left(-\\frac{a_2}{3}+a_1t+a_2t^2\\right)\\;dt=\\left[-\\frac{a_2}{4\\sqrt{3}}t^2+\\frac{a_1}{\\sqrt{6}}t^3+\\frac{\\sqrt{3}}{4\\sqrt{2}}a_2t^4\\right]_{-1}^1=0\\iff a_1=0.$$\n",
    "\n",
    "Finally we also impose the normalization of the polynomial under the inner product \n",
    "\n",
    "$$(P_2,P_2)=\\int\\limits_{-1}^1 (-\\frac{a_2}{3}+a_2t)^2\\;dt=1\\iff a_2^2(\\frac{4}{45})=\\frac{1}{2}\\iff a_2=\\pm \\sqrt{\\frac{45}{8}}$$\n",
    "\n",
    "and so $a_0=-\\frac{\\sqrt{45/8}}{3}=-\\sqrt{\\frac{5}{8}}$ if we let $a_2$ be positive. Then $P_2(t)=-\\sqrt{\\frac{5}{8}}+\\sqrt{\\frac{45}{8}}t^2$.\n",
    "\n",
    "**2.** With the same definition of an inner product as above, prove that \n",
    "\n",
    "$$\\left(\\left(\\frac{d}{dt}\\right)^n (1-t^2)^n, t^m\\right)=0,$$\n",
    "\n",
    "$0\\leq m\\leq n-1$, and thus express $P_n$ in terms of the expression $\\left(\\frac{d}{dt}\\right)^n (1-t^2)^n$. These polyonomials are, apart from constant factors, the classical Legendre polynomials. \n",
    "\n",
    "- This is actually a really cool problem. By the binomial theorem, we know \n",
    "\n",
    "$$(1-t^2)^n=\\sum\\limits_{k=0}^n (-1)^{n-k} \\binom{n}{k} t^{2(n-k)}$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\\left(\\frac{d}{dt}\\right)^n (1-t^2)^n=\\left(\\frac{d}{dt}\\right)^n \\sum\\limits_{k=0}^n (-1)^{n-k} \\binom{n}{k} t^{2n-2k}$$\n",
    "\n",
    "$$=\\sum\\limits_{k=0}^{\\lfloor \\frac{n}{2}\\rfloor} (-1)^{n-k}\\binom{n}{k}(2n-2k)\\cdots (2n-2k-n+1) t^{n-2k}$$\n",
    "\n",
    "$$=\\sum\\limits_{k=0}^{\\lfloor \\frac{n}{2}\\rfloor} (-1)^{n-k}\\binom{n}{k}\\binom{2n-2k}{n}(n-2k)!t^{n-2k}.$$\n",
    "\n",
    "From here, we want to show \n",
    "\n",
    "$$P_n(t)=\\alpha\\left(\\frac{d}{dt}\\right)^n (1-t^2)^n$$\n",
    "\n",
    "for some constant $\\alpha$. As $\\{P_n\\}$ is a sequence of Legendre polynomials, we know that \n",
    "\n",
    "$$P_n(t)=\\frac{1}{2^n}\\sum\\limits_{k=0}^{\\lfloor \\frac{n}{2}\\rfloor} (-1)^{k}\\binom{n}{k}\\binom{2n-2k}{n}t^{n-2k}$$\n",
    "\n",
    "From here, it seems as though \n",
    "\n",
    "$$\\left(\\frac{d}{dt}\\right)^n (1-t^2)^n=(-2)^n \\xi P_n(t)$$\n",
    "\n",
    "for some constant $\\xi$, then notice that \n",
    "\n",
    "$$\\left(\\left(\\frac{d}{dt}\\right)^n (1-t^2)^n, t^m\\right)=(-2)^n \\xi\\left(P_n,t^m\\right)=0$$\n",
    "\n",
    "as we showed in the prior problem! Damn son, that's a nice problem.\n",
    "\n",
    "**3.** Consider the interval $(0,\\infty)$ and define the inner product $(f,g)=\\int\\limits_0^\\infty e^{-t}f(t)g(t)dt$ for any two real polynomials $f(t)$ and $g(t)$. Let the sequence of polynomials $\\{L_n(t)\\}$ be determined by the conditions $L_0(t)=1$, $L_n(t)$ is a polynomial of degree $n$, and $(L_n,L_m)=0$, $n\\neq m$, $(L_n,L_n)=1$. Prove that these conditions imply that $(L_n,t^m)=0$, $0\\leq m\\leq n-1$, and construct the first few terms of the sequence using these relations.\n",
    "\n",
    "- Notice that again the set $\\{P_0,\\dots,P_m\\}$ form a basis for the space of polynomials of degree $m$ as they are a linearly independent set and can thus be combined to form any vector in the space - including $t^m$. As $P_n$ is orthogonal to each of these $P_i$, again this implies that the inner product of $P_n$ and $t^m$ is simply zero. As for the first few terms of the sequence, note that\n",
    "\n",
    "$$(P_0,P_1)=\\int\\limits_0^\\infty e^{-t}(a_0+a_1t)\\;dt=-a_0\\left[e^{-t}\\right]_0^\\infty+a_1\\int\\limits_0^\\infty te^{-t}dt=a_0+a_1\\Gamma (2)\\iff a_0=-a_1.$$\n",
    "\n",
    "Additionally, \n",
    "\n",
    "$$(P_1,P_1)=\\int\\limits_0^\\infty e^{-t}(-a_1+a_1t)^2\\;dt=a_1^2-2a_1^2\\Gamma (2)+a_1^2\\Gamma (3)=1\\iff a_1=\\pm 1$$\n",
    "\n",
    "and so $a_0=\\mp 1$ and so $P_1=\\mp 1\\pm t$. Let us take $1-t$ as the polynomial. Then for $P_2$, we have\n",
    "\n",
    "$$\\int\\limits_0^\\infty e^{-t}(a_0+a_1t+a_2t^2)\\;dt=a_0+a_1\\Gamma (2)+a_2\\Gamma (3)=0\\iff a_0=-(a_1+2a_2).$$\n",
    "\n",
    "and \n",
    "\n",
    "$$(P_1,P_2)=\\int\\limits_0^\\infty e^{-t}(1-t)(-a_1-2a_2+a_1t+a_2t^2)\\;dt=-(a_1+4a_2)=0\\iff a_1=-4a_2.$$\n",
    "\n",
    "Then finally, we have $a_0=-(a_1+2a_2)=2a_2$, so\n",
    "\n",
    "$$(P_2,P_2)=\\int\\limits_0^\\infty (2a_2-4a_2t+a_2t^2)^2\\;dt=4a_2^2-16a_2^2+8a_2^2+32a_2^2-48a_2^2+24a_2^2=4a_2^2=1$$\n",
    "\n",
    "$$\\iff a_2=\\pm\\sqrt{\\frac{1}{4}}.$$\n",
    "\n",
    "Our second polynomial is thus given by $P_2(t)=\\frac{1}{2}-t+\\frac{1}{4}t^2$.\n",
    "\n",
    "**4.** Prove that $\\left(e^t\\left(\\frac{d}{dt}\\right)^n (e^{-t}t^n), t^m\\right)=0$, $0\\leq m\\leq n-1$, and thus express $L_n$ in terms of $e^t\\left(\\frac{d}{dt}\\right)^n (e^{-t}t^n)$. These polynomials are apart from the constant factors the classical Laguerre polynomials.\n",
    "\n",
    "- These are some really interesting problem - stats is stupid. Anyhow, by Taylor expansion, we get\n",
    "\n",
    "$$(e^{-t}t^n)=\\sum\\limits_{k=0}^\\infty \\frac{(-1)^k}{k!}t^{n+k}\\Rightarrow \\left(\\frac{d}{dt}\\right)^n (e^{-t}t^n)$$ \n",
    "\n",
    "$$= \\sum\\limits_{k=0}^\\infty (-1)^k \\binom{n+k}{k}t^k=\\sum\\limits_{k=0}^\\infty \\binom{-n-1}{k}t^k=\\left(\\frac{1}{1+t}\\right)^{n+1}.$$\n",
    "\n",
    "Now for the inner product, we have\n",
    "\n",
    "$$\\left(e^t\\left(\\frac{d}{dt}\\right)^n (e^{-t}t^n), t^m\\right)=\\int\\limits_0^\\infty \\frac{t^m}{(1+t)^{n+1}}\\;dt$$\n",
    "\n",
    "and as $\\frac{t^m}{(1+t)^{n+1}}\\leq \\frac{1}{t^2}$, we have by the comparison test that this integral is indeed finite. Thus, we can find a mapping $e^t\\left(\\frac{d}{dt}\\right)^n (e^{-t}t^n)\\mapsto P_n$ from which we know the inner product is zero from problem 3 above.\n",
    "\n",
    "**5.** Consider the interval $(-\\infty,\\infty)$ and define the inner product \n",
    "\n",
    "$$(f,g)=\\int\\limits_{-\\infty}^\\infty e^{-t^2}f(t)g(t)dt$$\n",
    "\n",
    "for any two real polynomials $f(t)$ and $g(t)$. Let the sequence of polynomials $\\{H_n(t)\\}$ be determined as follows. $H_0(t)=1$, $H_n(t)$ is a polynomial of degree $n$, and $(H_m,H_n)=0$, $m\\neq n$, $(H_n,H_n)=1$. Prove that $(H_n,t^m)=0$, $0\\leq m\\leq n-1$, and construct the first few terms of the sequence in this fashion.\n",
    "\n",
    "- Another sick problem - establishing orthogonality trivially follows from the same reasoning as the prior problems - it's by construction. As for the first few terms of the sequence, we can use the fact that $\\int x^k e^{-x^2}=\\mathbb{E}(X^k)$ where $X$ is a standard normal random variable. Notice that $\\mathbb{E}(X^k)=\\left(\\frac{d}{dt}\\right)^k e^{\\frac{t^2}{2}}(0)$. I don't feel like doing the algebra - good practice for you :).\n",
    "\n",
    "**6.** Prove that $\\left(e^{t^2} \\left(\\frac{d}{dt}\\right)^n (e^{-t^2}t^n), t^m\\right) =0$, $0\\leq m\\leq n-1$ and hence express $H_n$ in terms of $e^{t^2} \\left(\\frac{d}{dt}\\right)^n (e^{-t^2}t^n)$. These polynomials are apart from the constant factors the classical Hermite polynomials.\n",
    "\n",
    "- One way of seeing this is recognizing the inner product as being bounded above by $\\mathbb{E}(X^{n+2})$ where $n\\geq 1$ and so we are looking at the third or higher moment of a standard normal variables - this is always zero and the inner product is always non-negative, so this gives us that the two are orthgonal.\n",
    "\n",
    "**7.** Show that given a real $N$-dimensional vector $x^1$, normalized by the condition that $(x^1,x^1)=1$, we can find $(N-1)$ additional vectors $x^2,\\dots,x^N$ with the property that the matrix $T=(x^1,x^2,\\dots,x^N)$ is orthogonal. \n",
    "\n",
    "- By Gram-Schmidt, we can form an orthonormal basis for the $N$-space as we can set $y^1=x^1$ and find the other $N-1$ orthonormal vectors. A matrix of orthonormal vectors is in fact orthonormal by definition, so I am not sure what the point of this question is.\n",
    "\n",
    "**8.** Obtain the analogue of the Gram-Schmidt method for complex vectors\n",
    "\n",
    "- We consider $(x,\\bar{y})$ instead of $(x,y)$ when solving the system of equations.\n",
    "\n",
    "### 4. On the Positivity of the $D_k$\n",
    "\n",
    "No exercises. \n",
    "\n",
    "This section is concerned with showing $|D_k|>0$ as we only cared about if $|D_k|\\neq 0$ to show that a solution exists for the Gram-Schmidt process. There are several ways of establishing this fact. The first is to show that $D_k$ is positive definite, then we know that for positive definite matrices, all eigenvalues are positive, and the product of eigenvalues is the determinant, and so the determinant must be positive. This is simple if you know these tactics, but if you don't, Bellman proposes the following. Define\n",
    "\n",
    "$$P(\\lambda)=\\lambda Q+(1-\\lambda)\\sum\\limits_{i=1}^N u_i^2$$\n",
    "\n",
    "where \n",
    "\n",
    "$$Q=\\sum\\limits_{i=1}^N u_i\\left(\\sum\\limits_{j=1}^N a_{ij}u_j\\right).$$\n",
    "\n",
    "Then notice that if $\\lambda = 0$, we have $P(\\lambda)=I$ and so $|P(\\lambda)|=1$ which is clearly positive. As the determinant of $P(\\lambda)$ is continuous for $\\lambda\\in[0,1]$, and never zero, it follows that positivity at $\\lambda = 0$ implies positivity at $\\lambda = 1$. Why this implication - well notice that if it is never zero, by intermediate value theorem, we know that $P(\\lambda)>0$ for all $\\lambda$ on which it is continuous - nifty basic analysis for ya. Oh yeah, I forgot - another question you may be asking: why Mark must the determinant be continuous. Well, that's a good question - notice that the determinant is the characteristic equation and we showed earlier that this is a polynomial of degree $N$ and the composition of a simple function and a polynomial is still continuous so bam it's still continuous - thanks Rudin.\n",
    "\n",
    "### 5. An Identity\n",
    "\n",
    "No exercises.\n",
    "\n",
    "The section provides a theorem that proves useful for yet another alternative to proving the determinant of the Gramian, $D_k$, is positive. \n",
    "\n",
    "Grammy Whammy Theorem: Let $x^i$, $i=1,\\dots,k$ be a set of $N$-dimensional vectors $N\\geq k$. Then \n",
    "\n",
    "$$|(x^i,x^j)|_{i,j=1,2,\\dots,k}=\\frac{1}{k!}\\sum\\limits_{\\{i\\}} \\left|\\begin{bmatrix} x_{i_1}^1&x_{i_2}^1&\\cdots &x_{i_k}^1\\\\ x_{i_1}^2&x_{i_2}^2&\\cdots & x_{i_k}^2\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{i_1}^k&x_{i_2}^k&\\cdots & x_{i_k}^k\\end{bmatrix}\\right|^2$$\n",
    "\n",
    "where the sum is over all sets of integers $\\{i_k\\}$ with $1\\leq i_1\\leq i_2\\leq \\cdots \\leq i_k\\leq N$.\n",
    "\n",
    "### 6. The Diagonalization of General Symmetric Matrices - Two Dimensional\n",
    "\n",
    "No exercises.\n",
    "\n",
    "The section is denoted to inductively proving that we can diagonalize a symmetric matrix without necessarily distinct roots. Consider the symmetric matrix \n",
    "\n",
    "$$A=\\begin{bmatrix} a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{bmatrix}=\\begin{bmatrix} a^1\\\\a^2\\end{bmatrix}.$$\n",
    "\n",
    "Then denote $\\lambda_1$ and $x^1$ as an associated characteristic root and vector pair (so $Ax^1=\\lambda_1 x^1$ or $(a^1,x^1)=\\lambda_1x_{11}$ and $(a^2,x^2)=\\lambda_1 x_{12}$ where $x=[x_{11}, x_{12}]^T$. By Gram-Schmidt, we can form an orthogonal matrix $T_2$ where one column consists of $x^1$ and the other let's denote by $x^2$ so that $T_2^TT_2=I$. As $T_2^2AT_2$ is symmetric and $T_2$ is orthogonal, we trivially have that \n",
    "\n",
    "$$T_2^T A T_2=\\begin{bmatrix}\\lambda_1 & 0\\\\0&b_{22}\\end{bmatrix}$$\n",
    "\n",
    "for some constant $b_{22}$ we do not know yet. But we do, as \n",
    "\n",
    "$$|T_2^T A T_2|=|T_2^T|\\cdot |A|\\cdot |T_2|=|T_2^T|\\cdot |T_2|\\cdot |A|=|I|\\cdot |A|=|A|$$\n",
    "\n",
    "which imply these two share the same characteristic equation and thus the same characteristic values. As $T_2^T A T_2$ is diagonal, that means the characteristic values are placed along the diagonal, so $b_{22}=\\lambda_2$ (not necessarily distinct from $\\lambda_1$ as desired). This completes the base case for the proof.\n",
    "\n",
    "\n",
    "### 7. N-dimensional Case\n",
    "\n",
    "The section extends the inductive proof from the above. The logic is very much the same with the difference being how we utilize the inductive step to get the orthogonal matrix is $T_1S_{N+1}$ instead. I do not feel like typing this section out in latex though, so you can read pages 53-54 or try sketching it yourself from the $2\\times 2$ case. There is a theorem that is proposed as a result of this induction panning out:\n",
    "\n",
    "General Diagonalization Theorem: Let $A$ be a real symmetric matrix. Then it may be transformed into a diagonal form by means of an orthogonal transformation, which is to say, there is an orthogonal matrix $T$ such that \n",
    "\n",
    "$$T^TAT=\\text{Diag}(\\lambda_1,\\dots,\\lambda_N)$$\n",
    "\n",
    "where $\\lambda_i$ are the not necessarily simple roots of $A$. \n",
    "\n",
    "**1.** What can be said if $A$ is a complex symmetric matrix?\n",
    "\n",
    "- Notice that \n",
    "\n",
    "$$|A|=\\left|\\begin{bmatrix} 1 & i\\\\ i & -1\\end{bmatrix}\\right|=0$$\n",
    "\n",
    "so the characteristic values are zero, but this doesn't make sense in the context of diagonalization as that would imply that $A=S0S^T=0$, but $A$ is non-zero.\n",
    "\n",
    "### 8. A Necessary and Sufficient Condition for Positive Definiteness\n",
    "\n",
    "Positive Definite GRoots Theorem: A necessary and sufficient condition that $A$ be positive definite is that all characteristic roots of $A$ be positive.\n",
    "\n",
    "Why? Well notice that by diagonalization of $A$ for some orthogonal matrix $T$, we have that \n",
    "\n",
    "$$x^TAx=(x,Ax)=\\sum\\limits_{i=1}^N \\lambda_i y_i^2$$\n",
    "\n",
    "which is strictly positive provided $\\lambda_i>0$ for all $i$. If you thought maybe one of the $\\lambda_j$ could be zero and it would still work - think about the definition. The definition is that it applies to all $x$, and so we could consider the $x$ where the $j$th entry is non zero but the rest are, but this would imply the sum is zero...\n",
    "\n",
    "**1.** $A=BB^T$ is positive definite if $B$ is real and $|B|\\neq 0$.\n",
    "\n",
    "- Clearly for any $x\\neq 0$, we have\n",
    "\n",
    "$$x^TAx = x^T BB^Tx=(B^Tx)^T (B^T x)\\geq 0.$$\n",
    "\n",
    "This quantity is zero if $x=0$ or $B^T$ is not of full rank - the former is not allowed under our definition of positive definiteness and the latter is false as $|B|\\neq 0$. Thus we have that $A$ is positive definite and the inequality above is strict.\n",
    "\n",
    "**2.** $H=CC^*$ is positive definite if $|C|\\neq 0$.\n",
    "\n",
    "- This follows very similar reasoning - as $(C^* x)^T (C^* x)= 0$ iff $C^*$ is not full rank or $x=0$ and both are not true, so $C$ is positive definite.\n",
    "\n",
    "**3.** If $A$ is symmetric, then $I+\\epsilon A$ is positive definite if $\\epsilon$ is sufficiently small.\n",
    "\n",
    "- Notice that we have for $x\\neq 0$ that letting $x=Sy$ where $S$ is the matrix of normalized eigenvectors\n",
    "\n",
    "$$\\frac{x^TAx}{x^Tx}=\\frac{(Sy)^T A(Sy)}{(Sy)^T(Sy)}=\\frac{y^T\\Lambda y}{y^Ty}=\\frac{\\sum\\limits_{i=1}^N \\lambda_i y_i^2}{\\sum\\limits_{i=1}^N y_i^2}\\leq \\max\\{|\\lambda_1|,\\dots,|\\lambda_N|\\}.$$\n",
    "\n",
    "Then for our original expression, we have\n",
    "\n",
    "$$x^T(I+\\epsilon A)x=x^Tx+\\epsilon x^TAx>0\\iff x^Tx>-\\epsilon x^TAx\\iff 1>-\\epsilon\\frac{x^TAx}{x^Tx}$$\n",
    "\n",
    "and so we set $\\epsilon < \\frac{1}{\\max\\{\\lambda_1,\\dots,\\lambda_N\\}}$ to achieve this strict inequality which verifies that $I+\\epsilon A$ is indeed positive definite for sufficiently small $\\epsilon$.\n",
    "\n",
    "### 9. Characteristic Vectors Associated with Multiple Characteristic Roots\n",
    "\n",
    "We showed earlier that characteristic vectors associated with simple characteristic roots (all unique/different) are orthgonal to each other. That begs the question - what happens if we have one root of multiplicity $k>1$. Well, then let $\\lambda_1=\\lambda_2=\\cdots=\\lambda_k$ and $\\lambda_1\\neq \\lambda_i$ for $i>k$. Then we have \n",
    "\n",
    "$$AT=T\\text{Diag}(\\lambda_1,\\dots,\\lambda_N)$$\n",
    "\n",
    "from which it follows that by the General Diagonalization theorem that $T$ is orthogonal and so each of its column vectors must be orthogonal. These column vectors are simply the eigenvectors corresponding to the same repeated characteristic value and so we have $k$ linearly independent characteristic vectors for the same characteristic value. \n",
    "\n",
    "### 10. The Cayley-Hamilton Theorem for Symmetric Matrices\n",
    "\n",
    "For any polynomial $p(\\lambda)$ we have the relation\n",
    "\n",
    "$$p(A)=T\\begin{bmatrix}p(\\lambda_1)&&&\\\\&p(\\lambda_2)&&\\\\&&\\ddots&\\\\&&&p(\\lambda_N)\\end{bmatrix}T^T$$\n",
    "\n",
    "and if we let $p(\\lambda)=|A-\\lambda I|$, then $p(A)=0$. This lends us to a special case of the Cayley-Hamilton Theorem whose proof is given in the sources below:\n",
    "\n",
    "Cayley Hamilton Theorem: Every symmetric matrix satisfies its characteristic equation.\n",
    "\n",
    "**1.** Use the method of continuity to derive the Cayley-Hamilton theorem for general symmetric matrices from the result for symmetric matrices for simple roots.\n",
    "\n",
    "- This question is proved in exercise 7 of section 6 - Reduction of Quadratic Forms to Canonical Form. We showed that the limiting behavior of a symmetric matrix without necessarily all simple roots still behaves the same way and tends to a matrix with all simple roots. \n",
    "\n",
    "### 11. Simultaneous Reduction to Diagonal Form\n",
    "\n",
    "Simultaneous Diagonalization Theorem: A necessary and sufficient condition that there exist an orthogonal matrix $T$ with the property that \n",
    "\n",
    "$$T^TAT=\\text{Diag}(\\lambda_1,\\dots,\\lambda_N)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$T^TBT=\\text{Diag}(\\mu_1,\\dots,\\mu_N)$$\n",
    "\n",
    "is that $A$ and $B$ commute - $AB=BA$.\n",
    "\n",
    "The proof of the above theorem given in the book is pretty convoluted and ugly, but this concept is very important in optimization. The proof is written very elegantly at the top of page three in the pdf I attached below.\n",
    "\n",
    "No exercises\n",
    "\n",
    "### 12. Simultaneous Reduction to Sum of Squares\n",
    "\n",
    "Simultaneous Reduction to Squares Theorem: Given two real symmetric matrices, $A$ and $B$, with $A$ positive definite, there exists a nonsingular matrix $H$ such that\n",
    "\n",
    "$$A=HH^T$$\n",
    "\n",
    "and\n",
    "\n",
    "$$B=H\\text{Diag}(\\mu_1,\\dots,\\mu_N)H^T.$$\n",
    "\n",
    "Proof: Let $T$ be an orthogonal matrix such that $A=T\\Lambda T^T$. Then let $x=Ty$, so that we have \n",
    "\n",
    "$$(x,Ax)=(Ty,ATy)=(y,T^TSTy)=(y,\\Lambda y).$$\n",
    "\n",
    "Additionally, we have that $(x,Bx)=(Ty,BTy)=(y,T^TBTy)$. From here, we make another transformation by setting\n",
    "\n",
    "$$\\Gamma = \\begin{bmatrix}\\frac{1}{\\sqrt{\\lambda_1}}&&\\\\&\\ddots &\\\\&&\\frac{1}{\\sqrt{\\lambda_N}}\\end{bmatrix}$$\n",
    "\n",
    "so that $y=\\Gamma z$. Then we have that \n",
    "\n",
    "$$(y,\\Lambda y)=(\\Gamma z, \\Lambda \\Gamma z)=(z,\\Gamma^T\\Lambda\\Gamma z)=(z,z).$$\n",
    "\n",
    "For the second quadratic form, we have that \n",
    "\n",
    "$$(y,T^TBTy)=(\\Gamma z, T^TBT\\Gamma z)=(z,\\Gamma^T T^TBT\\Gamma z)=(z, \\Gamma T^TBT\\Gamma z).$$\n",
    "\n",
    "Call this matrix $\\Gamma T^TBT\\Gamma = C$. Notice that \n",
    "\n",
    "$$C^T=\\Gamma T^TB^TT\\Gamma=\\Gamma T^TBT\\Gamma$$ \n",
    "\n",
    "as $B$ is symmetric. Thus, we can diagonalize $C$. Call $S$ the matrix that is orthonormal and diagonalizes $C$ and whose eigenvalue diagonal matrix is denoted by $\\Sigma$. Then we have that $(z,Cz)$. Then akin to our first transformation, let $z=Sw$ so that \n",
    "\n",
    "$$(z,Cz)=(Sw,CSw)=(w,S^TCSw)=(w,\\Sigma w).$$\n",
    "\n",
    "From this, we can define $H=T\\Gamma\\Sigma$ so that \n",
    "\n",
    "$$HH^T=(T\\Gamma\\Sigma)(T\\Gamma\\Sigma)^T=T\\Gamma\\Sigma\\Sigma^T\\Gamma^T T^T=T\\Gamma^2 T^T=T\\Lambda T^T=A$$\n",
    "\n",
    "and $$B=H\\Sigma H^T$$\n",
    "\n",
    "which proves the result.\n",
    "\n",
    "**1.** What is the corresponding result in case $A$ is non-negative definite?\n",
    "\n",
    "- If $A$ is non-negative definite, i.e. eigenvalues are $\\geq 0$, then I believe we would run into issues with our second stretching transformation - $\\frac{z_i}{\\sqrt{\\lambda_i}}$ if $\\lambda_i=0$. \n",
    "\n",
    "**2.** Let the notation $A>B$ for two symmetric matrices denote the fact that $A-B$ is positive definite. Use the foregoing result to show that $A>B>0$ implies that $B^{-1}>A^{-1}$.\n",
    "\n",
    "- Skipped...\n",
    "\n",
    "### 13. Hermitian Matrices\n",
    "\n",
    "Hermitian Diagonalization Theorem: If $H$ is a Hermitian matrix, there exists a unitary matrix $U$ such that \n",
    "\n",
    "$$H=U\\text{Diag}(\\lambda_1,\\dots,\\lambda_N)U^*.$$\n",
    "\n",
    "No exercises\n",
    "\n",
    "### 14. The Original Maximization Problem\n",
    "\n",
    "This is an interesting section as it provides some insights into the mindless plug and chug mindset of calculus 2 students. Notably if we let $c=(c_1,\\dots,c_N)$ be a stationary point of a function $f(x_1,\\dots,x_N)$, i.e. $f(c_1,\\dots,c_N)=0$, then we are interested in knowing if the point is a minimum, maximum or neither. Denote the matrix \n",
    "\n",
    "$$Q(c_1,\\dots,c_N)=\\begin{bmatrix}\\frac{\\partial^2 f}{\\partial x_1^2}(c_1,\\dots,c_N)&\\cdots &\\frac{\\partial^2 f}{\\partial x_1\\partial x_N}(c_1,\\dots,c_N)\\\\&\\ddots&\\vdots\\\\ &&\\frac{\\partial^2 f}{\\partial x_N^2}(c_1,\\dots,c_N)\\end{bmatrix}$$\n",
    "\n",
    "as the Hessian if $f$ possesses continuous mixed partial derivatives of the second order. If this matrix is positive definite, then that means that the function is convex (or concave up) and thus the stationary point (point where first derivative is zero) furnishes a local minimum. If instead the matrix is postive indefinite or semidefinite, then $c$ is still a local minimum. If instead the matrix is negative definite, then $(c_1,\\dots,c_N)$ is in fact a local maximum as the function is concave everywhere. We can determine if $Q$ is positive or negative definite or indefinite by finding the eigenvalues. For $N\\gg 0$, this proves to be too infeasible - Chris Pacieorek would be disappointed in you if you tried this and instead we have other means of determining definiteness discussed later on in this book. Oh, and if $Q$ is zero everwhere, then we have to look at the $3$rd, $4$th, etc. derivatives to see what's up with the function - test is indeterminant. \n",
    "\n",
    "**1.** Show that a set of sufficient conditions for $f(x_1,x_2)$ to have a local maximum is \n",
    "\n",
    "$$f_{c_1 c_2}<0,\\;\\;\\;\\;\\left|\\begin{bmatrix} f_{c_1 c_1}&f_{c_1 c_2}\\\\ f_{c_1 c_2}& f_{c_2 c_2}\\end{bmatrix}\\right|>0.$$\n",
    "\n",
    "- Why? Well, if \n",
    "\n",
    "$$|Q(c_1,c_2)|=f_{c_1 c_1}f_{c_2 c_2}>f_{c_1 c_2}^2$$\n",
    "\n",
    "then $f_{c_2 c_2}<0$ as $f_{c_1 c_2}^2\\geq 0$. Obtaining the eigenvalues is straightforward by means of a quadratic $a\\lambda^2 + b\\lambda + cc$. The roots of this quadratic are all less than zero provided \n",
    "\n",
    "$$\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}<0\\iff \\sqrt{b^2-4ac}<b\\iff b^2-4ac<b^2.$$\n",
    "\n",
    "\n",
    "Solving for the eigenvalues in our case yields the quadratic \n",
    "\n",
    "$$\\lambda^2-(f_{c_1 c_1}+f_{c_2 c_2})\\lambda + f_{c_1 c_1}f_{c_2 c_2} - f_{c_1 c_2}^2=0$$\n",
    "\n",
    "where $b=-(f_{c_1 c_1}+f_{c_2 c_2})$ and $4ac=4(f_{c_1 c_1}f_{c_2 c_2} - f_{c_1 c_2}^2)$. Clearly as $ac>0$, we have that $b^2-4ac<b^2$ or all eigenvalues are negative and the matrix is negative definite in which case we furnish a local maximum at $(c_1,c_2)$.\n",
    "\n",
    "### 15/16. Perturbation Theory - I and II\n",
    "\n",
    "This section is about what would happen to the characteristic vectors and roots of a matrix $A$ if we perturb it by $\\epsilon B$ where $\\epsilon$ is small ($A$ and $B$ are symmetric matrices). The question is what do we know about $A+\\epsilon B$ - think about dynamical systems, rocket ships being controlled - this problem can be thought of in this setting as what happens to the ship dynamics when we input some small force $B$ to the ship. This may be a dumb example, but I think it provides intuition for why we care. Here Bellman sheds some case analysis to get the ball rolling. If $A$ and $B$ commute, then we know there exists a $T$ that simultaneously diagonalizes both matrices such that $A=T\\Lambda T^T$ and $B=T\\Gamma T^T$, so $A+\\epsilon B=T(\\Lambda + \\epsilon \\Gamma)T^T$ which implies that the eigenvalues are of the form $\\lambda_i + \\epsilon \\mu_i$. If the two matrices do not commute, then let us assume that they have distinct characteristic roots. Then letting $T$ denote the matrix diagonalizing $A$ yields that \n",
    "\n",
    "$$|A+\\epsilon B|=|T^T|\\cdot |A+\\epsilon B|\\cdot |T|=|T^TAT+\\epsilon T^TBT|=|\\Lambda + \\epsilon T^TBT|.$$\n",
    "\n",
    "Notice that $|T^T T|=|I|$, so we just multiplied by one :) and let $C=T^TBT$. From here, I guess I'm not a expert in determinantal expansions and so I leave it as a challenge for you to figure out what this dude wants from me ;) as the solution supposedly takes the form \n",
    "\n",
    "$$\\lambda = \\lambda_i + d_{1i}\\epsilon + d_{2i}\\epsilon^2 + \\cdots .$$\n",
    "\n",
    "In the second section on perturbation theory, suppose that the characteristic roots and vectors of $A+\\epsilon B$ are close to $A$ and we have a power series in $\\epsilon$ for these new quantities. We denote the pair $(\\mu_i,y^i)$ as an eigenvalue eigenvector pair of $A+\\epsilon B$ that corresponds to a similar pair $(\\lambda_i,x^i)$ of $A$. Then by hypothesis we have\n",
    "\n",
    "$$\\mu_i = \\lambda_i + \\epsilon \\lambda_{1i} +\\cdots$$\n",
    "\n",
    "and \n",
    "\n",
    "$$y^i = x^i + \\epsilon x^{i1} + \\cdots .$$\n",
    "\n",
    "To determine the sequence of coefficients $\\{\\lambda_{ji}\\}_j$ and vectors $\\{x^{ij}\\}_j$, we substitute these into the expression\n",
    "\n",
    "$$(A+\\epsilon B)y^i = \\mu_i y^i \\iff (A+\\epsilon B)(x^i + \\epsilon x^{i1} + \\cdots )=(\\lambda_i + \\epsilon \\lambda_{1i} +\\cdots)(x^i + \\epsilon x^{i1} + \\cdots ).$$\n",
    "\n",
    "If we group together the terms with the same powers of $\\epsilon$, we end up with the system\n",
    "\n",
    "$$\\begin{cases} Ax^i=\\lambda_i\\\\ Ax^{i1} + Bx^i = \\lambda_i x^{i1} + \\lambda_{i1} x^i\\\\ Ax^{i2} + Bx^{i1} = \\lambda_i x^{i2} + \\lambda_{i1} x^{i1} + \\lambda_{i2} x^i\\\\\\vdots\\end{cases}.$$\n",
    "\n",
    "The first equation is determined. The second equation can be rewritten as \n",
    "\n",
    "$$(A-\\lambda_i I)x^{i1} = (\\lambda_{i1} I - B)x^i$$\n",
    "\n",
    "where $A-\\lambda_i I$ is singular and so there are either zero or infinite solutions to the above expression. If we let $x^1,\\dots,x^N$ denote the normalized characteristic vectors of $A$, then these form a basis for $\\mathbb{R}^N$ or $\\mathbb{C}^N$ as the eigenvalues are distinct, so we can represent $x^{i1}$ and $(\\lambda_{i1} I - B)x^i$ as linear combinations of them:\n",
    "\n",
    "$$x^{i1} = \\sum\\limits_{j=1}^N (x^{i1},x^j) x^j,\\;\\;\\;\\; (\\lambda_{i1} I - B)x^i = \\sum\\limits_{j=1}^N ((\\lambda_{i1} I - B)x^i, x^j) x^j$$\n",
    "\n",
    "so then \n",
    "\n",
    "$$(A - \\lambda_i I)\\sum\\limits_{j=1}^N (x^{i1},x^j) x^j = \\sum\\limits_{j=1}^N ((\\lambda_{i1} I - B)x^i, x^j) x^j.$$\n",
    "\n",
    "As $Ax^j = \\lambda_j x^j$, this simplifies to \n",
    "\n",
    "$$\\sum\\limits_{j=1}^N (x^{i1},x^j)(\\lambda_j - \\lambda_i)x^j = \\sum\\limits_{j=1}^N ((\\lambda_{i1} I - B)x^i, x^j) x^j$$\n",
    "\n",
    "which give us the relations \n",
    "\n",
    "$$(x^{i1},x^j)(\\lambda_j - \\lambda_i) = ((\\lambda_{i1} I - B)x^i, x^j).$$\n",
    "\n",
    "We only obtain a solution (i.e. not $5=0$) iff we have the right hand side for $x^j = x^i$ is zero as that would imply that $(x^{i1},x^j)(\\lambda_i - \\lambda_i)=(x^{i1},x^j)\\times 0 = 0$ as does the right hand side. Thus $(x^{i1},x^i)$ can be anything, but the others ($i\\neq j$) must be of the form\n",
    "\n",
    "$$(x_i,x_j)=\\frac{((\\lambda_{i1} I - B)x^i, x^j)}{\\lambda_j - \\lambda_i}.$$\n",
    "\n",
    "Then we obtain that \n",
    "\n",
    "$$x^{i1}=(x^{i1},x^i)x^i+\\sum\\limits_{j\\neq i} \\frac{((\\lambda_{i1} I - B)x^i, x^j)x^j}{\\lambda_j - \\lambda_i}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$(x^i, (\\lambda_{i1}I - B)x^i)=0\\iff (x^i, \\lambda_{i1}x^i) - (x^i,Bx^i)=0\\iff \\lambda_{i1}=(x^i,Bx^i).$$\n",
    "\n",
    "**1.** Find the value of $\\lambda_{i2}$.\n",
    "\n",
    "- We have from derivation above that $\\lambda_{i1}=(x^i,Bx^i)$,\n",
    "\n",
    "$$Ax^{i2} + Bx^{i1} = \\lambda_i x^{i2} + \\lambda_{i1}x^{i1} + \\lambda_{i2}x^i,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$x^{i1} = (x^{i1},x^i)x^i + \\sum\\limits_{j\\neq i} \\frac{((\\lambda_{i1} I - B)x^i, x^j)x^j}{\\lambda_j - \\lambda_i}.$$\n",
    "\n",
    "Then combining the three together yields the expression\n",
    "\n",
    "$$(A-\\lambda_i I)x^{i2}+\\left(B-(x^i,Bx^i)I\\right)\\sum\\limits_{j\\neq i} \\frac{((\\lambda_{i1} I - B)x^i, x^j)x^j}{\\lambda_j - \\lambda_i}=\\left[(\\lambda_{i2} + (x^i,Bx^i))I - B\\right]x^i.$$\n",
    "\n",
    "From here, we would like to establish that this replicates a system of equations as Bellman does in the notes, but I don't quite see it to be honest. The notation used in the book gets rather convoluted.\n",
    "\n",
    "**2.** Consider the case of multiple roots, first for the $2\\times 2$ case, and then, in general.\n",
    "\n",
    "- In the $2\\times 2$ case, we can just solve the determinantal equation as it's a quadratic:\n",
    "\n",
    "$$|A+\\epsilon B - \\lambda I| = \\lambda^2 + \\left(\\epsilon (c_{11} + c_{22}) - 2\\lambda_1\\right)\\lambda + \\left[\\lambda_1^2 - \\epsilon(c_{11} + c_{22})\\lambda_1 + \\epsilon^2 c_{11}c_{22}=0\\right].$$\n",
    "\n",
    "$$\\iff \\lambda = \\frac{-\\left(\\epsilon (c_{11} + c_{22}) - 2\\lambda_1\\right) \\pm \\sqrt{\\left(\\epsilon (c_{11} + c_{22}) - 2\\lambda_1\\right)^2 - 4\\left[\\lambda_1^2 - \\epsilon(c_{11} + c_{22})\\lambda_1 + \\epsilon^2 c_{11}c_{22}=0\\right]}}{2}.$$\n",
    "\n",
    "In general, I haven't solved yet - there's a source below on background of perturbation theory.\n",
    "\n",
    "**3.** Let $\\lambda_i$ denote the characteristic roots of $A$ and $\\lambda_i (z)$ those of $A+zB$. Show that\n",
    "\n",
    "$$\\lambda_j(A+zB) = \\sum\\limits_{m=0}^\\infty \\lambda_j(m)z^m,\\;\\;\\;\\; \\lambda_j(0)=\\lambda_j$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\lambda_j(m) = (-1)^m m^{-1}\\text{Tr}\\left(\\sum\\limits_{k_1+\\cdots + k_n = m-1,\\;k_i\\geq 0} BS_j^{k_1}BS_j^{k_2}\\cdots BS_j^{k_m}\\right)$$\n",
    "\n",
    "and $S_j = -E_j$ such that the $E_j$ are defined by the relation $A=\\sum\\limits_j \\lambda_j E_j$, and $S_j = \\sum\\limits_{k\\neq j} \\frac{E_k}{\\lambda_k - \\lambda_j}$.\n",
    "\n",
    "- So this is a tough problem and it is even harder provided Bellman has a typo... Basically, the subindices on the trace of the matrix should be $k_1+\\cdots + k_m$ and not $k_n$. With any tough problem, let us make sense of it first by considering the first few terms of the series\n",
    "\n",
    "$$(A+zB)\\lambda_j = \\lambda_j^{(0)}z^0 + \\lambda_j^{(1)}z^1 + \\lambda_j^{(2)}z^2$$\n",
    "\n",
    "$$= \\lambda_j + \\frac{(-1)^1}{1}\\text{Tr}\\left(\\sum\\limits_{k_1+\\cdots + k_m = 1,\\;k_i\\geq 0} BS_j^{k_1}BS_j^{k_2}\\cdots BS_j^{k_m}\\right) + \\frac{(-1)^2}{2}\\text{Tr}\\left(\\sum\\limits_{k_1+\\cdots + k_m = 2,\\;k_i\\geq 0} BS_j^{k_1}BS_j^{k_2}\\cdots BS_j^{k_m}\\right)+\\cdots$$\n",
    "\n",
    "From here, we notice that since $E_iE_j=0$ and $E_i^t=E$, so for $t>1$,\n",
    "\n",
    "$$S_j^t=\\left(\\sum\\limits_{k\\neq j} \\frac{E_k}{\\lambda_k - \\lambda_j}\\right)^t=\\sum\\limits_{k\\neq j} \\frac{E_i}{(\\lambda_k - \\lambda_j)^t}.$$\n",
    "\n",
    "Aside from this I'm not sure the best continuation: https://math.stackexchange.com/questions/4355314/perturbation-matrix-eigenvalues-and-power-series.\n",
    "\n",
    "**4.** Latex nightmare - not doing...\n",
    "\n",
    "\n",
    "\n",
    "## Miscellaneous Exercises from Reduction of General Symmetric Matrices\n",
    "\n",
    "**1.** Show that a real symmetric matrix may be written in the form \n",
    "\n",
    "$$A=\\sum\\limits_{i=1}^N \\lambda_i E_i$$\n",
    "\n",
    "where the $E_i$ are non-negative definite matrices satisfying the conditions\n",
    "\n",
    "$$E_iE_j=0,\\;\\;\\;\\; i\\neq j,\\;\\;\\;\\; E_{i^2}=E_i$$\n",
    "\n",
    "and the $\\lambda_i$ are the characteristic roots of $A$. This is called the spectral decomposition of $A$.\n",
    "\n",
    "- This trivially follows from the diagonalization theorem in this section. Notably we have that for any symmetric matrix $A=S\\Lambda S^T$ where $S$ is the matrix of eigenvectors and $\\Lambda$ is the matrix of eigenvalues. Then \n",
    "\n",
    "$$S\\Lambda S^T = \\begin{bmatrix}\\lambda_1x_1&\\lambda_2x_2&\\cdots & \\lambda_N x_N\\end{bmatrix}\\begin{bmatrix}x_1&x_2&\\cdots & x_N\\end{bmatrix}^T=\\sum\\limits_{i=1}^N \\lambda_i x_ix_i^T$$\n",
    "\n",
    "where $(x_ix_i^T)(x_jx_j^T)=0$ and $(x_i,x_i)=0$ as $x_i$ and $x_j$ are orthogonal and normalized.\n",
    "\n",
    "**2.** Let $A$ be a real skew-symmetric matrix. As we know, the characteristic roots are pure imaginary or zero. Let $x+yi$ be a characteristic vector associated with $i\\mu$, where $\\mu$ is a real nonzero quantity and where $x$ and $y$ are real. Show that $x$ and $y$ are orthogonal. \n",
    "\n",
    "- asdf\n",
    "\n",
    "**3.** Referring to the above problem, let $T_1$ be an orthogonal matrix whose first two columns are $x$ and $y$, \n",
    "\n",
    "$$T_1 = (x,y,x^3,x^4,\\dots,x^N).$$\n",
    "\n",
    "Show that \n",
    "\n",
    "$$T_1^T A T = \\begin{bmatrix}\\begin{bmatrix}0 & \\mu \\\\ -\\mu & 0\\end{bmatrix} & 0\\\\ 0 & A_{N-2}\\end{bmatrix}$$\n",
    "\n",
    "where $A_{N-2}$ is again skew-symmetric.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**4.** Prove inductively that if $A$ is a real skew-symmetric matrix of even dimension we can find an orthogonal matrix $T$ such that \n",
    "\n",
    "$$T^T AT=\\begin{bmatrix}\\begin{bmatrix} 0 & \\mu_1 \\\\ -\\mu_1 & 0 \\end{bmatrix} & & & \\\\ & \\begin{bmatrix} 0 & \\mu_2 \\\\ -\\mu_2 & 0\\end{bmatrix}&&\\\\&&\\ddots &\\\\&&&\\begin{bmatrix}0 & \\mu_N\\\\ -\\mu_N & 0\\end{bmatrix}\\end{bmatrix}$$\n",
    "\n",
    "where some of the $\\mu_i$ may be zero.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**5.** If $A$ is of odd dimension, show that the canonical form is \n",
    "\n",
    "$$T^T AT=\\begin{bmatrix}\\begin{bmatrix}0 & \\mu_1\\\\ -\\mu_1 & 0\\end{bmatrix}&&&\\\\&\\begin{bmatrix}0 & \\mu_2\\\\ -\\mu_2 & 0\\end{bmatrix}&&\\\\&&\\ddots &\\\\&&&\\begin{bmatrix}0 & \\mu_N\\\\ -\\mu_N & 0\\end{bmatrix}\\end{bmatrix}$$\n",
    "\n",
    "where again some of the $\\mu_i$ may be zero.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**6.** Show that the determinant of a skew-symmetric matrix of odd dimension is zero.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**7.** The determinant of a skew-symmetric matrix of even dimension is the square of a polynomial in the elements of the matrix.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**8.** Let $A$ be an orthogonal matrix, and let $\\lambda$ be a characteristic root of absolute value 1 but not equal to $\\pm 1$, with $x+iy$ an associated characteristic vector, $x$ and $y$ real. Show that $x$ and $y$ are orthogonal.\n",
    "\n",
    "- asdf\n",
    "\n",
    "\n",
    "**9.** Proceeding inductively as before, show that every orthogonal matrix $A$ can be reduced to the form \n",
    "\n",
    "$$A=T\\begin{bmatrix}\\begin{bmatrix} \\cos\\lambda_1 &-\\sin\\lambda_1\\\\\\sin\\lambda_1&\\cos\\lambda_1\\end{bmatrix}&&&&&\\\\&\\ddots &&&&\\\\&&\\begin{bmatrix} \\cos\\lambda_k &-\\sin\\lambda_k\\\\\\sin\\lambda_k&\\cos\\lambda_k\\end{bmatrix}&&&\\\\&&&\\pm 1&&\\\\&&&&\\ddots &\\\\&&&&&\\pm 1\\end{bmatrix}T^T.$$\n",
    "\n",
    "- asdf\n",
    "\n",
    "**10.** Prove that $T\\Lambda T^T$ is a positive definite matrix whenever $T$ is an orthogonal matrix and $\\Lambda$ is a diagonal matrix with positive elements down the main diagonal.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**11.** Prove the simultaneous diagonalization theorem by means of an inductive argument, along the lines of the proof given in Sec. 6.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**12.** Establish the analogue of the result in Exercise 9 for unitary matrices.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**13.** Write down a sufficient condition that the function $f(x_1,\\dots,x_N)$ possess a local maximum at $x_i=c_i$ for $i=1,\\dots , N$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**14.** Given that $A$ is a positive definite matrix, find all the solutions of $X^TX=A$.\n",
    "\n",
    "- asdf\n",
    "\n",
    "**15.** Define the Gramian of $N$ real functions $f_1,\\dots,f_N$ over $(a,b)$ by means of the expression\n",
    "\n",
    "$$G(f_1,\\dots,f_N)=\\left|\\int\\limits_a^b f_i(t)f_j(t)dt\\right|.$$\n",
    "\n",
    "Prove that if \n",
    "\n",
    "$$Q(x_1,x_2,\\dots,x_N)=\\int\\limits_a^b \\left[g(t)-\\sum\\limits_{i=1}^N x_i f_i(t)\\right]^2\\;dt$$\n",
    "\n",
    "then \n",
    "\n",
    "$$\\min\\limits_x Q = \\frac{G(g,f_1,\\dots,f_N)}{G(f_1,\\dots,f_N)}.$$\n",
    "\n",
    "- asdf\n",
    " \n",
    "There are like 25 more exercises in this section...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37060b44",
   "metadata": {},
   "source": [
    "### Detour - Orthogonal Functions\n",
    "\n",
    "Nearly 100% of the time when I think of vector spaces, I think numbers and vectors as an ordered list of numbers or an arrow directed from the origin of some magnitude and position. However, this quickly grows out of style quickly, when we talk about function spaces. For instance, in your linear algebra class you most likely showed that the space of polynomials forms of degree $n$ is a vector space whose basis is typically $(x^0, x^1,\\dots, x^N)$. Now what is the meaning of orthogonality and bases here right?? Well, as seen below functions in the vector/numeric sense as orthgonal is quite nonsensical - we see below that they are not perpedicular over $[-1,1]$. However, if we evaluate their product at every point, we see that the product of the functions is zero everywhere over $[-1,1]$. This is the definition and intuition - the inner product is zero, then the two functions are orthogonal. We can then speak of bases and the like as building blocks that are orthogonal functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efaa0261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# orthogonal polynomials and function spaces.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def polynomial_func(degree, coef, domain):\n",
    "    \"\"\"\n",
    "    :param: degree - integer - highest degree of polynomial\n",
    "    :param: coef - list of floats, coefficients of polynomial - [a_0x^0+...+a_nx^n]\n",
    "    :param: domain - list of floats, list of points in domain to evaluate at \n",
    "    :return: range of polynomial defined over domain, list of floats\n",
    "    \"\"\"\n",
    "    # coefficient count must match degree\n",
    "    assert len(coef) == degree + 1\n",
    "    \n",
    "    # naive/brute implementation so you see what's going on\n",
    "    codomain = []\n",
    "    for point in domain:\n",
    "        y = 0\n",
    "        for i in range(0, degree + 1):\n",
    "            y += coef[i] * (point ** i)\n",
    "            \n",
    "        codomain.append(y)\n",
    "        \n",
    "    return codomain\n",
    "    \n",
    "def inner_prod(range1, range2):\n",
    "    \"\"\"\n",
    "    :params: range1, range2 - codomains of two functions f, g over domain [-1, 1]\n",
    "    :return: inner product defined as integral from -1 to 1 of f*g\n",
    "    \"\"\"\n",
    "    return [x * y for x, y in zip(range1, range2)]\n",
    "  \n",
    "# x^2 and 1e-4 are orthogonal under the inner product above\n",
    "domain = [x * 0.01 for x in range(-1, 1)]\n",
    "func1_range = polynomial_func(0, [0.0001], domain)\n",
    "func2_range = polynomial_func(2, [0, 0, 1], domain)\n",
    "func_prod = inner_prod(func1_range, func2_range)\n",
    "\n",
    "# plotting function and inner product\n",
    "plt.plot(domain, func1_range, color = \"blue\", label = \"$f_1(x) = 1 x 10^{-4}$\")\n",
    "plt.plot(domain, func2_range, color = \"green\", label = \"$f_2(x) = x^2$\")\n",
    "plt.plot(domain, func_prod, color = \"red\", label = \"$(f_1, f_2)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88188b0a",
   "metadata": {},
   "source": [
    "# Scratch Work/Notes\n",
    "\n",
    "The entries of a vector belong to a **field**. A field  $\\mathcal{F}$ is a set that satisfies the following axioms. For $x,y,z\\in\\mathcal{F}$,\n",
    "\n",
    "a) $x+y=y+x$ and $xy=yx$ (commutative under addition and multiplication)\n",
    "\n",
    "b) $(x+y)+z=x+(y+z)$ and $(xy)z=x(yz)$ (associative under addition and multiplication)\n",
    "\n",
    "c) For all $a\\in\\mathcal{F}\\setminus \\{0\\}$, there are elements $b$ and $c\\neq 0$ such that $a+b=0$ and $ac=1$ (existence of additive and multiplicative inverse)\n",
    "\n",
    "Usually we take fields for granted, but they are extremely important when manipulating systems. Common fields used include the reals $\\mathbb{R}$, the complex numbers $\\mathbb{C}$, and the rationals $\\mathbb{Q}$. Nonexamples include the integers (no multiplicative inverse of course, the binary set $\\{0,1\\}$, the set of prime numbers (again existence of inverses is violated). \n",
    "\n",
    "For a field $\\mathcal{F}$, we can now define a $n$-dimensional row vector as a collection of $n$ elements taken from $\\mathcal{F}$. For instance, in space we may consider a space shuttle's location as represented conveniently by a vector - the $(x,y,z)$ coordinates taken from the rationals. Just like our field, vectors while they can make sense without imposing any rules or limitations, are much more useful to us and developed in theory under the following axioms. These following axioms define what is called a **linear vector space**. A linear vector space $\\mathcal{X}$ on some field $\\mathcal{F}$ satisfies the following for elements $x,y\\in\\mathcal{X}$ and $a,b\\in\\mathcal{F}$:\n",
    "\n",
    "a) $x+y=y+x$ (commutative under addition)\n",
    "\n",
    "b) $(x+y)+z=x+(y+z)$ and $a(bx)=b(ax)$ (associative under addition and scalar multiplication)\n",
    "\n",
    "c) $(a+b)(x+y)=ax+ay+bx+by$ (distributive law for vectors and scalar multiplication)\n",
    "\n",
    "d) There is a unique $z\\in\\mathcal{F}$ such that $x+z=y$.\n",
    "\n",
    "Examples of linear vector spaces include the $k$-tuples of real numbers $(x_1,\\dots,x_k)$ included in so many applications, the set of polynomials of degree $n$ - $a_1x^n+a_2x^{n-1}+\\cdots +a_nx+a_{n+1}$ which pops up in regression and density estimation, along with the trivial vector space - $\\{0\\}$ which shows up nowhere outside of a math textbook (this is a called a trivial/improper vector space).\n",
    "\n",
    "A **linear subspace** of a vector space $\\mathcal{X}$ is a subset of vectors $\\mathcal{A}$ closed under vector addition and scalar multiplication. By closed, I mean that the sum of vectors is still in the subspace and the scalar multiples of all vectors are still vectors in the subspace. **TODO** - introduce concept of bases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702b0b5",
   "metadata": {},
   "source": [
    "# Some Competition Linear Algebra Problems & Answers \n",
    "\n",
    "Here are an assortment of easy competition problems - my solutions are not complete, but sufficient if we are talking about statistics for the sake of my sanity with latex.\n",
    "\n",
    "1. Putnam 2008 Problem A2: Alan and Barbara play a game in which they take turns filling entries of an initially empty $2008\\times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy?\n",
    "\n",
    "- There is a very neat solution/optimal strategy for Barbara to win every time. If Alan fills in a real number $x$ on the $a_{ij}$ entry, Barbara can fill in the same real number x in the $a_{ij+1}$ or $a_{ij-1}$ entry (sticking to either adjacent rows or columns for the entire game). Thus in the end, there will always be at least one pairing of identical columns. Barbara could also have done this pairing strategy by rows instead which also ensures the determinant of the resultant matrix is $0$.\n",
    "\n",
    "2.\\. IMC 2005 Day 1 Problem 1: Let $A$ be an $n\\times n$ matrix such that $A_{ij}=i+j$. Find the rank of $A$.\n",
    "\n",
    "- For $n=1$, we have that the rank is 1. For $n\\geq 2$, we can do the following: take the first row and subtract it from each of the subsequent rows. Then the second through $n$th row are all increasing multiples of each other. Hence we have just two rows that make up the rank for any matrix larger than $1\\times 1$.\n",
    "\n",
    "3.\\. Putnam 2011 Problem A4: For which positive integers n is there an n\\times n matrix with integer entries such that every dot product of a row with itself is even, while every dot product of two different rows is odd?\n",
    "\n",
    "- WLOG, we can just inspect matrices with entries in $\\mathbb{Z}/2\\mathbb{Z}$ as we are only concerned with whether or not the dot product is even/odd. For odd matrices, it is easy to see that if we consider a matrix with 0s along the diagonal and 1s everywhere else, then what we have is that each row as an even n-1 number of 1s whereas when we take the dot product with another row, we will always have 2 cases of $0*1$ and the rest are $1*1 – n-2$ is odd, so this works. This generalizes for all odd $n$, so now we turn attention to even $n$ where this simple argument doesn’t extend nicely. Instead, we can use a bit of linear algebra. If we have row vectors $V_1,\\dots,V_n$, our problem is characterized by trying to show that $\\forall i,\\;j\\in\\{1,\\dots,n\\}$: $i\\neq j,\\;V_i\\cdot V_j=1$ and $V_i\\cdot V_i=0$. As the dot product of a vector with itself is even, we must have that the matrix is not invertible. But, there’s a contradiction – left up to the reader 🙂\n",
    "\n",
    "## Other Comp Problems \n",
    "- https://artofproblemsolving.com/community - this is a goat resource to improve, look at contest, high school olympiad and college math forums.\n",
    "- https://href.li/?http://www.math.utoronto.ca/barbeau/putnamla.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f277c",
   "metadata": {},
   "source": [
    "# Exercises from \"Linear Statistical Inference and Its Applications\" by C.R Rao\n",
    "\n",
    "## 1a\n",
    "\n",
    "**1.** If $\\alpha_1,\\dots,\\alpha_k$ are $k$ independent vectors in a a vector space $\\mathcal{X}$, show that it can always be exptended to a basis of $\\mathcal{X}$, that is, we can add $\\alpha_{k+1},\\dots,\\alpha_r$ such that $\\alpha_1,\\dots,\\alpha_r$ form a basis of $\\mathcal{X}$, where $r=\\text{Dim}(\\mathcal{X})$.\n",
    "\n",
    "- do Mark\n",
    "\n",
    "**4.** If $pq$ elements $a_{ij}$, $i\\in\\{1,\\dots,p\\}$ and $j\\in\\{1,\\dots,q\\}$ are such that the tetra difference $a_{ij}+a_{rs}-a_{is}-a_{rj}=0$ for all $i,j,r,s$, then $a_{ij}$ can be expressed in the form $a_{ij}=a_i+b_j$ where $a_1,\\dots,a_p$ and $b_1,\\dots,b_q$ are properly chosen. \n",
    "\n",
    "- do Mark\n",
    "\n",
    "**5.** Find the number of independent vectors in the set ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38408ed9",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "- Introduction to Matrix Analysis by Richard Bellman\n",
    "    - I like the book for the problems and storytelling/smooth explanations. It assumes some first course linear algebra exposure and maybe differential equations for applications, but that is it.\n",
    "    \n",
    "    \n",
    "- Scientific Computing: An Introductory Survey by Heath\n",
    "    - I just bought this book. It seems full of applications and is used in an ML grad course at Stanford, so I'm all for it. This has epic problems just from eyeballing it.\n",
    "    \n",
    "    \n",
    "- Linear Statistical Inference and Its Applications by C.R Rao\n",
    "    - I really enjoy how he introduces linear algebra with statistics in mind. I have found his writing much better for getting exposure to linear algebra used frequently in heavier duty books like Keener's \"Theoretical Statistics\".\n",
    "   \n",
    "   \n",
    "- Linear Algebra by Hoffman and Kunze\n",
    "    - I used this undergrad for linear algebra. It's full of good examples and problems, but be warned this dude doesn't mess around and it's tough and he quickly abstracts to results in algebra - like Grassman manifolds which have no place in my heart tbh.\n",
    "    \n",
    "    \n",
    "- Mathematics for Machine Learning by Deisenroth et al.\n",
    "    - This is super light, but if like Rao's book but at a higher level, you want only the barebones, this might quench your thirst.\n",
    "\n",
    "\n",
    "- https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/thm_sed.html\n",
    "    - EECS 127 has a good proof of spectral theorem :) \n",
    "    \n",
    "- https://www.d.umn.edu/~jgreene/Math_5327_Linear/Cayley_Hamilton.pdf\n",
    "    - The best pdf I read through on the topic - starts out simple and doesn't over complicate it.\n",
    "    \n",
    "- http://www.ms.uky.edu/~sohum/ma322/lectures/Cayley-Hamilton.pdf\n",
    "    - The first page on this document provides the cleanest and simplest proof of the Cayley Hamilton theorem (for diagonalizable matrices).\n",
    "    \n",
    "- https://www.math.uh.edu/~bgb/Courses/Math6304/MatrixTheory-20120906.pdf\n",
    "    - Simultaneous diagonalization theorem (top of page 3) - quick and simple. By $M_n$, the author simple means the class of symmetric matrices that are $n\\times n$. \n",
    "\n",
    "- http://sycon.rutgers.edu/~speer/528s17/Perturbation-DG.pdf\n",
    "    - Perturbation theory - interesting material\n",
    "\n",
    "This weird econ dude has surprisingly good notes on the topic w/ visuals too - screw MIT, Iowa State is where it's at :)\n",
    "- http://www2.econ.iastate.edu/classes/econ500/hallam/Char_Vec.pdf\n",
    "- http://www2.econ.iastate.edu/classes/econ500/hallam/documents/Quad_Forms_000.pdf\n",
    "- http://www2.econ.iastate.edu/classes/econ500/hallam/documents/Geometry_Matrices_002.pdf\n",
    "\n",
    "\n",
    "\n",
    "Number Theory:\n",
    "- https://bookstore.ams.org/mbk-105/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
