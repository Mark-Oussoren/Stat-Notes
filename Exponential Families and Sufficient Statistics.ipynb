{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c2f8e1",
   "metadata": {},
   "source": [
    "# Sufficient Statistics & The Exponential Family\n",
    "\n",
    "## Exposition\n",
    "\n",
    "Sufficient statistics are an intuitive way of reducing the data down to what is necessary for computation. For an unrealistic example, say we have amassed $1e10$ data points and know the data follows a normal distribution, then we do not need to store the $1e10$ points, but rather we could compute the sample mean and standard deviation only. Thus sufficiency is primarily a tool akin to filtering and dimension reduction for reducing our data to what is necessary for a particular distribution. **Notice** however, that sufficiency requires us to **know beforehand the underlying distribution**. For this reason, sufficiency is primarly of interest to wacky theoretical statiscians and not scientists in practice who enjoy non-parametric inference....\n",
    "\n",
    "\n",
    "## Definitions\n",
    "\n",
    "### Sufficient Statistic: \n",
    "\n",
    "- Definition 1 (Wasserman - confusing): A statistic $T(x^n)$, where $x^n$ is a vector of $n$ observations, is **sufficient** for a parameter $\\theta$ if $P(T(x^n)|\\theta)=cP(T(y^n)|\\theta)$ implies that $P(x^n|\\theta)=dP(y^n|\\theta)$ for some constants $c,d\\in\\mathbb{R}$ that can depend on $x^n$ and $y^n$ but not $\\theta$. \n",
    "\n",
    "\n",
    "- Definition 2 (Keener): Suppose that a random variable $X:\\Omega\\rightarrow \\mathcal{X}$ has distribution from a family $\\mathcal{P}=\\{P_\\theta:\\theta\\in\\Omega\\}$. Then $T=T(X)$ is a **sufficient** statistic for $\\mathcal{P}$ (or for $X$ or for $\\theta$) if for every $t$ and $\\theta$, the conditional distribution of $X$ under $P_\\theta$ given $T=t$ does not depend on $\\theta$. \n",
    "\n",
    "\n",
    "- Definition 3 (Rao): A statistic $T$ (a random variable mapping observations/data to some $\\sigma$-algebra of the reals) is said to be **sufficient** for the family of measures $P_\\theta$ iff one of the folowing equivalent conditions holds:\n",
    "    - $P(A|T=t)$ is independent of $\\theta$ for every $A$ measurable under $P$.\n",
    "    - $\\mathbb{E}[Y|T=t]$ is independent of $\\theta$ for every random variable $Y$ such that $\\mathbb{E}(Y)$ exists.\n",
    "    - The conditional distribution of every random variable $Y$ given $T=t$, which always exists, is independent of $\\theta$.\n",
    "    \n",
    "\n",
    "### Minimally Sufficient Statistic:\n",
    "- Definition (Wasserman): A statistic $T$ is minimal sufficient if 1) it is sufficient and 2) it is a function of every other sufficient statistic.\n",
    "\n",
    "### Complete Statistic:\n",
    "- Definition (Keener): A statistic $T$ is complete for a family $\\mathcal{P}=\\{P_\\theta:\\theta\\in\\Omega\\}$ if \n",
    "\n",
    "$$\\mathbb{E}[f(T)|\\theta]=c\\;\\;\\;\\forall \\theta$$ \n",
    "\n",
    "implies $f(T)=c$.\n",
    "\n",
    "### Fisher-Neyman Factorization Theorem:\n",
    "- Statement (Wasserman): $T$ is sufficient iff there are functions $g(t,\\theta)$ and $h(x)$ such that $f(x^n|\\theta)=g(t(x^n),\\theta)h(x^n)$. \n",
    "\n",
    "### Rao-Blackwell Theorem:\n",
    "- Statement (Wasserman): Let $\\hat{\\theta}$ be an estimator and let $T$ be a sufficient statistic. Define a new estimator by \n",
    "\n",
    "$$\\tilde{\\theta}=\\mathbb{E}[\\hat{\\theta}|T].$$\n",
    "\n",
    "Then for every $\\theta$, we have $R(\\theta,\\tilde{\\theta})\\leq R(\\theta,\\hat{\\theta})$ where the risk $R$ is the mean squared error.\n",
    "\n",
    "### Exponential Family: \n",
    "- Definition\n",
    "\n",
    "## Problems\n",
    "\n",
    "**1.** Find a sufficient statistic for $\\theta$ where\n",
    "\n",
    "a) $X_1,\\dots,X_n\\sim \\text{Unif}(-\\theta,\\theta)$\n",
    "\n",
    "b) $X_1,\\dots,X_n\\sim \\mathcal{N}(\\theta,\\sigma)$ for a known $\\sigma\\in\\mathbb{R}$\n",
    "\n",
    "c) $X_1,\\dots,X_n\\sim \\text{Gamma}(\\frac{\\theta}{2},\\frac{1}{2})$\n",
    "\n",
    "d) $X_1,\\dots,X_n\\sim \\text{Beta}(\\theta, \\theta)$\n",
    "    \n",
    "- answer\n",
    "\n",
    "**2.** Find an example of a minimally sufficient statistic that is not complete. Can you find an example of the converse?\n",
    "\n",
    "- answer\n",
    "\n",
    "**2. (Rao Chapter 2 - Problem 21)**: Consider the probability space $(\\Omega, \\mathcal{B},P)$. Let $Y$ be a space of points $y$, $\\mathcal{A}$ a $\\sigma$-algebra of sets of $Y$, and $T:\\Omega\\rightarrow Y$ a function such that $A\\in\\mathcal{A}\\Rightarrow T^{-1}(A)\\in\\mathcal{B}$. Let $f(\\omega,y)$ be a real-valued $\\mathcal{B}\\times \\mathcal{A}$ measurable function on $\\Omega\\times Y$. Then show that \n",
    "\n",
    "$$\\mathbb{E}\\left[f(\\omega,T(\\omega))|T(\\omega)=y\\right]=\\mathbb{E}\\left[f(\\omega,y)|T(\\omega)=y\\right].$$\n",
    "\n",
    "- answer\n",
    "\n",
    "**3. (Rao Chapter 2 - Problem 22)**: Let $X$ be a vector valued random variable whose distribution depends on a vector parameter $\\theta$. Further, let $T$ be a statistic such that the distribution of $T$ only depends on $\\phi$, a function of $\\theta$. Then $T$ is said to be **inference sufficient** for $\\phi$ if the conditional distribution of $X$ given $T$ depends only on functions of $\\theta$ which are independent of $\\phi$. Let $X_1,\\dots, X_n$ be $n$ independent observations from $\\mathcal{N}(\\mu,\\sigma^2)$. Show that $\\sum (X_i-\\bar{X})^2$ is inference sufficient for the parameter $\\sigma^2$.\n",
    "\n",
    "- answer\n",
    "\n",
    "**4. (Rao Chapter 5 - Problem 6.1)** Consider a power series distribution with proability \n",
    "\n",
    "$$P(X=r)=\\frac{a_r}{f(\\theta)}\\theta^r,\\;\\;\\;\\;r=c,c+1,\\dots,\\infty.$$\n",
    "\n",
    "Let $x_1,\\dots,x_n$ be a sample of size $n$ witha. total, $T=x_1+\\cdots +x_n$. Show that $T$ is a complete sufficient statistic for $\\theta$ and the distribution of $T$ is of the same form \n",
    "\n",
    "$$P(T=t)=\\frac{b_t\\theta^t}{f(\\theta)^n},\\;\\;\\;\\;t=nc,nc+1,\\dots,\\infty.$$\n",
    "\n",
    "- answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfd64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7492f90",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- \"Theoretical Statistics\" by Keener - Chapters 2, 3, 5\n",
    "- \"All of Statistics\" by Wasserman - Chapter 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
