{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47c68b0",
   "metadata": {},
   "source": [
    "# Optimization 2\n",
    "\n",
    "Because one optimization notebook is never enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397e6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from IPython.core.display import Image, display\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set plotting font sizes and properties\n",
    "TINY_SIZE = 12\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 20\n",
    "MARKER_SIZE = 6\n",
    "LINE_SIZE = 4\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=BIGGER_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=TINY_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc(\"lines\", markersize=MARKER_SIZE)  # marker size\n",
    "plt.rc(\"lines\", linewidth=LINE_SIZE)  # line width\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 180 # sets the image quality\n",
    "\n",
    "# Height and width per row and column of subplots\n",
    "FIG_HEIGHT = 18\n",
    "FIG_WIDTH = 16\n",
    "fig_fcn = lambda kwargs: plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT), **kwargs)\n",
    "color_list = sns.color_palette(\"Paired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02324870",
   "metadata": {},
   "source": [
    "# Level Method\n",
    "\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11152bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level method over the maxquad functions\n",
    "class LevelMethod():\n",
    "    \n",
    "    def __init__(self, dim=10, k=5, C=1e-10, max_iter=1000):\n",
    "        \"\"\"\n",
    "        :param: dim - int, size of matrices A_k \n",
    "        :param: k - int, number of matrices A_1,\\dots, A_k\n",
    "        :param: C - float, learning rate constant C: eta = C / sqrt{t}\n",
    "        :param: max_iter - int, max number of iterations allowed\n",
    "        \"\"\"\n",
    "        self.dim = int(dim)\n",
    "        self.k = int(k)\n",
    "        self.C = C\n",
    "        self.max_iter = int(max_iter)\n",
    "        \n",
    "        # starting guess for optimum\n",
    "        self.start = np.ones(dim)\n",
    "        \n",
    "        # initialize the set of matrices A_k\n",
    "        self.A = {}\n",
    "        for i in range(k):\n",
    "            A_i = np.zeros(shape=(dim, dim))\n",
    "            for m in range(dim):\n",
    "                for n in range(i):\n",
    "                    A_i[m, n] = np.exp((n + 1) / (m + 1)) * np.cos((m + 1) * (n + 1)) * np.sin((i + 1))\n",
    "                    A_i[n, m] = A_i[m, n]\n",
    "                A_i[m, m] = (i / 10) * np.abs(np.sin(i)) + np.sum(np.abs(A_i[m, :m])) + np.sum(np.abs(A_i[m, m+1:]))\n",
    "            self.A[i] = A_i\n",
    "\n",
    "        # initialize the vectors b_k\n",
    "        self.b = {}\n",
    "        for i in range(k):\n",
    "            b_i = np.zeros(dim)\n",
    "            for j in range(dim):\n",
    "                b_i[j] = np.exp((j + 1) / (i + 1)) * np.sin((i + 1) * (j + 1))\n",
    "            self.b[i] = b_i \n",
    "            \n",
    "        # keep track of best guess thus far\n",
    "        self.best_x = self.start\n",
    "        self.max_val = max([np.dot(self.start, np.dot(self.A[i], self.start)) - np.dot(self.b[i], self.start) for i in range(self.k)])\n",
    "\n",
    "    def maxquad_obj(self, x):\n",
    "        results = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)]\n",
    "        return max(results)\n",
    "    \n",
    "    def subgrad(self, x):\n",
    "        # grab index of the largest function at point x\n",
    "        i_max = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)].index(self.maxquad_obj(x))\n",
    "        return 2 * np.dot(self.A[i_max], x) - self.b[i_max]\n",
    "        \n",
    "    def constant_update(self, x, iter):\n",
    "        return x - (self.C / np.sqrt(iter)) * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def polyak_update(self, x, iter):\n",
    "        learning_rate = (self.maxquad_obj(x) - self.max_val + (1 / iter)) / np.dot(self.subgrad(x), self.subgrad(x))\n",
    "        return x - learning_rate * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def train(self):\n",
    "        f_vals = []\n",
    "        x = self.start\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            # take a step\n",
    "            x = self.polyak_update(x, epoch)\n",
    "            f_new = self.maxquad_obj(x) \n",
    "            if f_new < self.max_val:\n",
    "                self.best_x = x\n",
    "                self.max_val = f_new\n",
    "                f_vals.append(f_new)\n",
    "            else:\n",
    "                f_vals.append(self.max_val)\n",
    "        return f_vals\n",
    "    \n",
    "    def graph_log_iter(self):\n",
    "        y = self.train()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(np.log(np.arange(1, self.max_iter + 1)), np.log(y - self.max_val + 1), color='navy')\n",
    "        plt.xlabel('Log of Iterations')\n",
    "        plt.ylabel('Log of Sub-Optimality Gap')\n",
    "        plt.show()\n",
    "        fig.savefig('polyak_update.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3176f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant update here\n",
    "subgrad = SubgradientDescent()\n",
    "results = subgrad.graph_log_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c05530",
   "metadata": {},
   "source": [
    "# Accelerated Gradient Descent - A Symphony\n",
    "\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38671807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerated gradient descent over the maxquad functions\n",
    "class AGD():\n",
    "    \n",
    "    def __init__(self, dim=10, k=5, C=1e-10, max_iter=1000):\n",
    "        \"\"\"\n",
    "        :param: dim - int, size of matrices A_k \n",
    "        :param: k - int, number of matrices A_1,\\dots, A_k\n",
    "        :param: C - float, learning rate constant C: eta = C / sqrt{t}\n",
    "        :param: max_iter - int, max number of iterations allowed\n",
    "        \"\"\"\n",
    "        self.dim = int(dim)\n",
    "        self.k = int(k)\n",
    "        self.C = C\n",
    "        self.max_iter = int(max_iter)\n",
    "        \n",
    "        # starting guess for optimum\n",
    "        self.start = np.ones(dim)\n",
    "        \n",
    "        # initialize the set of matrices A_k\n",
    "        self.A = {}\n",
    "        for i in range(k):\n",
    "            A_i = np.zeros(shape=(dim, dim))\n",
    "            for m in range(dim):\n",
    "                for n in range(i):\n",
    "                    A_i[m, n] = np.exp((n + 1) / (m + 1)) * np.cos((m + 1) * (n + 1)) * np.sin((i + 1))\n",
    "                    A_i[n, m] = A_i[m, n]\n",
    "                A_i[m, m] = (i / 10) * np.abs(np.sin(i)) + np.sum(np.abs(A_i[m, :m])) + np.sum(np.abs(A_i[m, m+1:]))\n",
    "            self.A[i] = A_i\n",
    "\n",
    "        # initialize the vectors b_k\n",
    "        self.b = {}\n",
    "        for i in range(k):\n",
    "            b_i = np.zeros(dim)\n",
    "            for j in range(dim):\n",
    "                b_i[j] = np.exp((j + 1) / (i + 1)) * np.sin((i + 1) * (j + 1))\n",
    "            self.b[i] = b_i \n",
    "            \n",
    "        # keep track of best guess thus far\n",
    "        self.best_x = self.start\n",
    "        self.max_val = max([np.dot(self.start, np.dot(self.A[i], self.start)) - np.dot(self.b[i], self.start) for i in range(self.k)])\n",
    "\n",
    "    def maxquad_obj(self, x):\n",
    "        results = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)]\n",
    "        return max(results)\n",
    "    \n",
    "    def subgrad(self, x):\n",
    "        # grab index of the largest function at point x\n",
    "        i_max = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)].index(self.maxquad_obj(x))\n",
    "        return 2 * np.dot(self.A[i_max], x) - self.b[i_max]\n",
    "        \n",
    "    def constant_update(self, x, iter):\n",
    "        return x - (self.C / np.sqrt(iter)) * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def polyak_update(self, x, iter):\n",
    "        learning_rate = (self.maxquad_obj(x) - self.max_val + (1 / iter)) / np.dot(self.subgrad(x), self.subgrad(x))\n",
    "        return x - learning_rate * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def train(self):\n",
    "        f_vals = []\n",
    "        x = self.start\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            # take a step\n",
    "            x = self.polyak_update(x, epoch)\n",
    "            f_new = self.maxquad_obj(x) \n",
    "            if f_new < self.max_val:\n",
    "                self.best_x = x\n",
    "                self.max_val = f_new\n",
    "                f_vals.append(f_new)\n",
    "            else:\n",
    "                f_vals.append(self.max_val)\n",
    "        return f_vals\n",
    "    \n",
    "    def graph_log_iter(self):\n",
    "        y = self.train()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(np.log(np.arange(1, self.max_iter + 1)), np.log(y - self.max_val + 1), color='navy')\n",
    "        plt.xlabel('Log of Iterations')\n",
    "        plt.ylabel('Log of Sub-Optimality Gap')\n",
    "        plt.show()\n",
    "        fig.savefig('polyak_update.jpg', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
