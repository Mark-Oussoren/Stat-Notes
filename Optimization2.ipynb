{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368d9668",
   "metadata": {},
   "source": [
    "# Optimization 2\n",
    "\n",
    "Because one optimization notebook is never enough. \n",
    "\n",
    "This notebook will assume familiarity with most definitions, theorems, and topics from the first notebook.\n",
    "\n",
    "Tentative Topics to Cover below:\n",
    "- Level Method (as an alternative to sugradient descent)\n",
    "- Stochastic Gradient Descent (under many regimes and step sizes - analyzing last iterate and average)\n",
    "- Structural Optimization: enough of this black box stuff :) - interior point methods and accelerated cubic newton\n",
    "- Accelerated Gradient Descent (and the dawn of State of the art Adam, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9477b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from IPython.core.display import Image, display\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set plotting font sizes and properties\n",
    "TINY_SIZE = 12\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 20\n",
    "MARKER_SIZE = 6\n",
    "LINE_SIZE = 4\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=BIGGER_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=TINY_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc(\"lines\", markersize=MARKER_SIZE)  # marker size\n",
    "plt.rc(\"lines\", linewidth=LINE_SIZE)  # line width\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 180 # sets the image quality\n",
    "\n",
    "# Height and width per row and column of subplots\n",
    "FIG_HEIGHT = 18\n",
    "FIG_WIDTH = 16\n",
    "fig_fcn = lambda kwargs: plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT), **kwargs)\n",
    "color_list = sns.color_palette(\"Paired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb1d36",
   "metadata": {},
   "source": [
    "# Level Method\n",
    "\n",
    "Let's discuss the Level Method which is an algorithmic scheme more flexible than the subgradient method for solving problems of the form\n",
    "\n",
    "$$\\min\\limits_{x\\in G} f(x)$$\n",
    "\n",
    "\n",
    "where $f(\\cdot)$ is a Lipschitz continuous convex function and $G$ is a closed convex set. This method revolves around the idea of a \\textit{nonsmooth model} for a convex objective function that aids us with the difficult task of identifying a lower bound for the global minimum.\n",
    "\n",
    "**Definition 3.3.1**: Let $X=\\{x_k\\}_{k=0}^\\infty$ be a sequence of points in $G$. Define \n",
    "    \n",
    "$$\\hat{f}_k(X; x)=\\max\\limits_{i\\in[k]} \\left[ f(x_i) + \\langle g(x_i), x-x_i\\rangle\\right]$$\n",
    "    \n",
    "    \n",
    "where $g(x_i)$ are some subgradients of $f$ at $x_i$. The function $\\hat{f}_k(X;\\cdot)$ is called a *nonsmooth model* of the convex function $f$.\n",
    "\n",
    "\n",
    "This function consists of $k$ points in $G$ ($x_1,\\dots,x_k$) where $k$ linear tangents to the curve are drawn and the tangent closest to the curve at the desired point $x$ is this model's estimate of $f(x)$. From this, we gather that \n",
    "\n",
    "1. $\\hat{f}_k(X;\\cdot)$ is piece-wise linear \n",
    "2. $\\hat{f}_k(X;x)\\leq f(x)$ for all $x\\in\\mathbb{R}^n$ as $f(\\cdot)$ is convex\n",
    "3. $\\hat{f}_{k+1}(X;x)\\geq \\hat{f}_k(X;x)$\n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "From point 2., we have now identified a point that forms a lower bound for the minimum $f(x^*)$ and we can easily form an upper bound as the function is convex so the Level Method is constructed around the following important bounds at each step and their difference:\n",
    "\n",
    "$$\\hat{f}_k^-(x)=\\min\\limits_{x\\in G} \\hat{f}_k(X;x),\\;\\;\\;\\; \\hat{f}_k^+(x)=\\min\\limits_{i\\in[k]} f(x_i),\\;\\;\\;\\; \\Delta_k(x)=\\hat{f}^+_k(x)-\\hat{f}^-_k(x).\n",
    "$$\n",
    "\n",
    "\n",
    "Notice that $\\hat{f}^+_k(x)$ is monotonically decreasing with respect to $k$ as the minimum can only decrease with more search points and $f(x^*)\\leq \\hat{f}^+(x)$ for all $x\\in\\mathbb{R}^n$. From this and 3. from above, we see that $\\Delta_k$ is monotonically decreasing and bounded above by zero. These points bolster good intuition as for why this method will at least converge. However, developing a nice update scheme is still a bit hazy. It is obvious that we desire our next iterate to lie somewhere between this lower and upper bound, and the following level set characterizes this region.\n",
    "\n",
    "\n",
    "$$\\mathcal{Q}(\\lambda)=\\left\\{x\\in G:\\hat{f}_k(X;x)\\leq (1-\\lambda)\\hat{f}_k^-(x) + \\lambda\\hat{f}_k^+(x)\\right\\}$$\n",
    "\n",
    "\n",
    "This set is clearly closed and convex and $\\mathcal{Q}(\\lambda)$ increases in cardinality as $\\lambda$ increases. Our next step should then simply project the prior iterate onto this smaller region and this finally yields the Level Method below.\n",
    "\n",
    "___\n",
    "___\n",
    "**ALGORITHM: Level Method**\n",
    "\n",
    "**Require** $x_0\\in G$, $\\epsilon >0$, $\\lambda\\in(0,1)$\n",
    "\n",
    "**for** $t\\leftarrow 1,2,\\dots$\n",
    "- Compute $\\hat{f}_i^-(x_i)$ and $\\hat{f}_i^+(x_i)$\n",
    "-  **If** $\\hat{f}_i^+(x_i)-\\hat{f}_i^-(x_i)\\leq \\epsilon$: Break **for**\n",
    "- **Else**:\n",
    "- Form the level set $\\mathcal{Q}(\\lambda)$\n",
    "- $x_{i+1}\\leftarrow \\Pi_{\\mathcal{Q}(\\lambda)}(x_i)$\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level method over the maxquad functions\n",
    "class LevelMethod():\n",
    "    \n",
    "    def __init__(self, dim=10, k=5, C=1e-10, max_iter=1000):\n",
    "        \"\"\"\n",
    "        :param: dim - int, size of matrices A_k \n",
    "        :param: k - int, number of matrices A_1,\\dots, A_k\n",
    "        :param: C - float, learning rate constant C: eta = C / sqrt{t}\n",
    "        :param: max_iter - int, max number of iterations allowed\n",
    "        \"\"\"\n",
    "        self.dim = int(dim)\n",
    "        self.k = int(k)\n",
    "        self.C = C\n",
    "        self.max_iter = int(max_iter)\n",
    "        \n",
    "        # starting guess for optimum\n",
    "        self.start = np.ones(dim)\n",
    "        \n",
    "        # initialize the set of matrices A_k\n",
    "        self.A = {}\n",
    "        for i in range(k):\n",
    "            A_i = np.zeros(shape=(dim, dim))\n",
    "            for m in range(dim):\n",
    "                for n in range(i):\n",
    "                    A_i[m, n] = np.exp((n + 1) / (m + 1)) * np.cos((m + 1) * (n + 1)) * np.sin((i + 1))\n",
    "                    A_i[n, m] = A_i[m, n]\n",
    "                A_i[m, m] = (i / 10) * np.abs(np.sin(i)) + np.sum(np.abs(A_i[m, :m])) + np.sum(np.abs(A_i[m, m+1:]))\n",
    "            self.A[i] = A_i\n",
    "\n",
    "        # initialize the vectors b_k\n",
    "        self.b = {}\n",
    "        for i in range(k):\n",
    "            b_i = np.zeros(dim)\n",
    "            for j in range(dim):\n",
    "                b_i[j] = np.exp((j + 1) / (i + 1)) * np.sin((i + 1) * (j + 1))\n",
    "            self.b[i] = b_i \n",
    "            \n",
    "        # keep track of best guess thus far\n",
    "        self.best_x = self.start\n",
    "        self.max_val = max([np.dot(self.start, np.dot(self.A[i], self.start)) - np.dot(self.b[i], self.start) for i in range(self.k)])\n",
    "\n",
    "    def maxquad_obj(self, x):\n",
    "        results = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)]\n",
    "        return max(results)\n",
    "    \n",
    "    def subgrad(self, x):\n",
    "        # grab index of the largest function at point x\n",
    "        i_max = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)].index(self.maxquad_obj(x))\n",
    "        return 2 * np.dot(self.A[i_max], x) - self.b[i_max]\n",
    "        \n",
    "    def constant_update(self, x, iter):\n",
    "        return x - (self.C / np.sqrt(iter)) * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def polyak_update(self, x, iter):\n",
    "        learning_rate = (self.maxquad_obj(x) - self.max_val + (1 / iter)) / np.dot(self.subgrad(x), self.subgrad(x))\n",
    "        return x - learning_rate * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def train(self):\n",
    "        f_vals = []\n",
    "        x = self.start\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            # take a step\n",
    "            x = self.polyak_update(x, epoch)\n",
    "            f_new = self.maxquad_obj(x) \n",
    "            if f_new < self.max_val:\n",
    "                self.best_x = x\n",
    "                self.max_val = f_new\n",
    "                f_vals.append(f_new)\n",
    "            else:\n",
    "                f_vals.append(self.max_val)\n",
    "        return f_vals\n",
    "    \n",
    "    def graph_log_iter(self):\n",
    "        y = self.train()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(np.log(np.arange(1, self.max_iter + 1)), np.log(y - self.max_val + 1), color='navy')\n",
    "        plt.xlabel('Log of Iterations')\n",
    "        plt.ylabel('Log of Sub-Optimality Gap')\n",
    "        plt.show()\n",
    "        fig.savefig('polyak_update.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2af356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant update here\n",
    "subgrad = SubgradientDescent()\n",
    "results = subgrad.graph_log_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a225f",
   "metadata": {},
   "source": [
    "# Accelerated Gradient Descent - A Symphony\n",
    "\n",
    "Aw GD and MGD are such a power couple ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a49e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerated gradient descent over the maxquad functions\n",
    "class AGD():\n",
    "    \n",
    "    def __init__(self, dim=10, k=5, C=1e-10, max_iter=1000):\n",
    "        \"\"\"\n",
    "        :param: dim - int, size of matrices A_k \n",
    "        :param: k - int, number of matrices A_1,\\dots, A_k\n",
    "        :param: C - float, learning rate constant C: eta = C / sqrt{t}\n",
    "        :param: max_iter - int, max number of iterations allowed\n",
    "        \"\"\"\n",
    "        self.dim = int(dim)\n",
    "        self.k = int(k)\n",
    "        self.C = C\n",
    "        self.max_iter = int(max_iter)\n",
    "        \n",
    "        # starting guess for optimum\n",
    "        self.start = np.ones(dim)\n",
    "        \n",
    "        # initialize the set of matrices A_k\n",
    "        self.A = {}\n",
    "        for i in range(k):\n",
    "            A_i = np.zeros(shape=(dim, dim))\n",
    "            for m in range(dim):\n",
    "                for n in range(i):\n",
    "                    A_i[m, n] = np.exp((n + 1) / (m + 1)) * np.cos((m + 1) * (n + 1)) * np.sin((i + 1))\n",
    "                    A_i[n, m] = A_i[m, n]\n",
    "                A_i[m, m] = (i / 10) * np.abs(np.sin(i)) + np.sum(np.abs(A_i[m, :m])) + np.sum(np.abs(A_i[m, m+1:]))\n",
    "            self.A[i] = A_i\n",
    "\n",
    "        # initialize the vectors b_k\n",
    "        self.b = {}\n",
    "        for i in range(k):\n",
    "            b_i = np.zeros(dim)\n",
    "            for j in range(dim):\n",
    "                b_i[j] = np.exp((j + 1) / (i + 1)) * np.sin((i + 1) * (j + 1))\n",
    "            self.b[i] = b_i \n",
    "            \n",
    "        # keep track of best guess thus far\n",
    "        self.best_x = self.start\n",
    "        self.max_val = max([np.dot(self.start, np.dot(self.A[i], self.start)) - np.dot(self.b[i], self.start) for i in range(self.k)])\n",
    "\n",
    "    def maxquad_obj(self, x):\n",
    "        results = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)]\n",
    "        return max(results)\n",
    "    \n",
    "    def subgrad(self, x):\n",
    "        # grab index of the largest function at point x\n",
    "        i_max = [np.dot(x, np.dot(self.A[i], x)) - np.dot(self.b[i], x) for i in range(self.k)].index(self.maxquad_obj(x))\n",
    "        return 2 * np.dot(self.A[i_max], x) - self.b[i_max]\n",
    "        \n",
    "    def constant_update(self, x, iter):\n",
    "        return x - (self.C / np.sqrt(iter)) * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def polyak_update(self, x, iter):\n",
    "        learning_rate = (self.maxquad_obj(x) - self.max_val + (1 / iter)) / np.dot(self.subgrad(x), self.subgrad(x))\n",
    "        return x - learning_rate * self.subgrad(x) / np.sum(self.subgrad(x) ** 2)\n",
    "    \n",
    "    def train(self):\n",
    "        f_vals = []\n",
    "        x = self.start\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            # take a step\n",
    "            x = self.polyak_update(x, epoch)\n",
    "            f_new = self.maxquad_obj(x) \n",
    "            if f_new < self.max_val:\n",
    "                self.best_x = x\n",
    "                self.max_val = f_new\n",
    "                f_vals.append(f_new)\n",
    "            else:\n",
    "                f_vals.append(self.max_val)\n",
    "        return f_vals\n",
    "    \n",
    "    def graph_log_iter(self):\n",
    "        y = self.train()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(np.log(np.arange(1, self.max_iter + 1)), np.log(y - self.max_val + 1), color='navy')\n",
    "        plt.xlabel('Log of Iterations')\n",
    "        plt.ylabel('Log of Sub-Optimality Gap')\n",
    "        plt.show()\n",
    "        fig.savefig('polyak_update.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e260a",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "Books\n",
    "\n",
    "- \"Lectures on Convex Optimization\" by Nesterov\n",
    "\n",
    "- \"Algorithms for Convex Optimization\" by Vishnoi\n",
    "\n",
    "\n",
    "Lecture Notes\n",
    "\n",
    "- EE C227C Berkeley past notes\n",
    "- Anything by Nesterov, Nemirovski, Hazad, Bubeck is golden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
