{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5834a3c1",
   "metadata": {},
   "source": [
    "# PAC Learning\n",
    "\n",
    "In this notebook, I will cover the Probably Approximately Correct (PAC) learning framework. \n",
    "\n",
    "# Motivation\n",
    "In a sentence, the PAC framework quantifies the requirements (number of training points, time complexity, and space complexity) to approximately learn patterns in the data. THIS IS HUGE! For instance, a simple example is learning linear versus nonlinear maps between the input and outputs - linear patterns intuitively require less points to learn the pattern. Concretely, we could look at predicting height with weight data. If we desired a linear pattern, then we only need to search over $\\mathbb{R}_+$ to find a parameter $\\alpha$ such that $\\text{height}=\\alpha\\cdot \\text{weight}$. If instead we desire a nonlinear pattern, then think about the expansion of the space! We could fit polynomials, trigonometric, and the list goes on. I believe this should provide sufficient motivation for why we care deeply about quantifying the number of samples we need depending on the relationships we deem important (linear, nonlinear, indicator functions, exponentials, etc.). Now, I turn to mathematics for precision and clarity.\n",
    "\n",
    "## Some Notations\n",
    "\n",
    "- Let $\\mathcal{X}$ denote the input space (includes all data excluding the output variable)\n",
    "\n",
    "- Let $\\mathcal{Y}$ denote the set of labels/output/target variables (as in [1], I limit discussion to binary outputs right now)\n",
    "\n",
    "- Examples $(x,y)\\in(\\mathcal{X},\\mathcal{Y})$ are assumed to be drawn independently and identically distributed (i.i.d.) according to some unknown distribution $\\mathcal{D}$ \n",
    "\n",
    "# The PAC Learning Framework - Chapter 2\n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Concept**: A concept $c:\\mathcal{X}\\rightarrow\\mathcal{Y}$ is a mapping from $\\mathcal{X}$ to $\\mathcal{Y}$.\n",
    "\n",
    "**Concept Class**: A concept class $\\mathcal{C}$ is a set of concepts we may wish to learn.  \n",
    "\n",
    "**Generalization Error**: Given a hypothesis $h\\in\\mathcal{H}$, a target concept $c\\in\\mathcal{C}$, and an underlying distribution $D$, the generalization error or risk of $h$ is defined by \n",
    "\n",
    "$$R(h) = P_{x\\sim D}\\left[h(x)\\neq c(x)\\right] = \\mathbb{E}_{x\\sim D}\\left[\\mathbb{I}_{h(x)\\neq c(x)}\\right].$$\n",
    "\n",
    "In words, it is the probability that the hypothesis does not align with the target concept. This is of course unknown as we do not know $D$; however, we can measure the empirical error of the hypothesis based on a labeled sample of the data.\n",
    "\n",
    "**Empirical Error**: Given a hypothesis $h\\in\\mathcal{H}$, a target concept $c\\in\\mathcal{C}$, and a sample $S=(x_1,\\dots,x_m)$, the empirical error or risk of $h$ is defined by \n",
    "\n",
    "$$\\hat{R}(h) = \\frac{1}{m}\\sum\\limits_{i=1}^m \\mathbb{I}_{h(x_i)\\neq c(x_i)}.$$\n",
    "\n",
    "It is an emsemble of the error derived from the sample $S$.\n",
    "\n",
    "**PAC-learning**: A concept class $C$ is said to be PAC-learnable if there exists an algorithm $\\mathcal{A}$ and a polynomial function $poly(\\cdot,\\cdot,\\cdot,\\cdot)$ such that for any $\\epsilon>0$ and $\\delta>0$, for all distributions $D$ on $\\mathcal{X}$ and for any target concept $c\\in\\mathcal{C}$, the following holds for any sample size $m\\geq poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$:\n",
    "\n",
    "$$\\mathbb{P}_{S\\sim D^m} \\left[R(h_S)\\leq \\epsilon\\right]\\geq 1-\\delta.$$ \n",
    "\n",
    "If $\\mathcal{A}$ further runs in $poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$, then $C$ is said to be efficiently PAC-learnable. When such an algorithm $\\mathcal{A}$ exists, it is called a PAC-learning algorithm for $C$.\n",
    "\n",
    "*Note on PAC-learning*: This definition is CRAZY!! I can draw the samples from any distribution of my choosing - the exponential, Cauchy, uniform, a crazy convolution of all three, etc. and it still holds that the generalization error of a hypothesis on the samples is bounded above by $\\epsilon$ with some high probability $1-\\delta$. The definition stattes that after enough samples (whose exact number is based on some polynomial of the error, bound, the dimension of the data, and the size of the concept representation). HOWEVER, the test data must be distributed according to the same as the training set - in other words, we impose a stationary condition that is all too common and all too unrealistic in practice. FURTHERMORE, we make a statement about the entire concept class - so we must cast quite a wide net to catch all the possible fish (target concepts). I wonder if this can be relaxed to just focus on a subset of a concept class...\n",
    "\n",
    "**Agnostic PAC-learning**: Let $H$ be a hypothesis set. $\\mathcal{A}$ is an agnostic PAC-learning algorithm if there exists a polynomial function $poly(\\cdot,\\cdot,\\cdot,\\cdot)$ such that for any $\\epsilon > 0$ and $\\delta > 0$, for all distributions $D$ over $\\mathcal{X}\\times\\mathcal{Y}$, the following holds for any sample size $m\\geq poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$:\n",
    "\n",
    "$$\\mathbb{P}_{S\\sim D^m}\\left[R(h_S) - \\min\\limits_{h\\in H} R(h) \\leq \\epsilon\\right]\\geq 1-\\delta.$$\n",
    "\n",
    "If $\\mathcal{A}$ further runs in $poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$, then it is said to be an efficient agnostic PAC-learning algorithm.\n",
    "\n",
    "*Note on Agnostic PAC*: Note the differences. The BIG difference is that original PAC-learning only features a distribution over the input space. What does this mean? Well, it means that our hypothesis is simply a DETERMINISTIC function - that is every $x$ is mapped to a unique output and there is no randomness. Here, our output is a random function of the input - so a given $x_1$ can be mapped to $1$ with probability $0.4$ and $0$ with probability $0.6$. Beautiful - as in most real-world scenarios a scientist surveys, this is by far the most common. HOWEVER, the author is boring and chose to exclusively focus on the deterministic setting where ouputs are uniquely determined by some measurable function WP1.\n",
    "\n",
    "**Bayes Error**: Given a distribution $D$ over $\\mathcal{X}\\times\\mathcal{Y}$, the Bayes error $R^*$ is defined as the infimum of the errors achieved by measurable functions $h:\\mathcal{X}\\rightarrow\\mathcal{Y}$:\n",
    "\n",
    "$$R^*=\\inf\\limits_{\\text{measurable } h} R(h).$$\n",
    "\n",
    "A hypothesis $h$ with $R(h)=R^*$ is called a Bayes hypothesis or Bayes classifier.\n",
    "\n",
    "*Notes on Bayes Error*: The Bayes error is the hypothesis that minimizes the generalization error - simple, I have no further comment.\n",
    "\n",
    "**Noise**: Given a distribution $D$ over $\\mathcal{X}\\times\\mathcal{Y}$, the noise at point $x\\in\\mathcal{X}$ is defined by \n",
    "\n",
    "$$noise(x)=\\min\\limits_{y\\in\\{0,1\\}}\\left\\{P(y|x)\\right\\}.$$\n",
    "\n",
    "The average noise or the noise associated to $D$ is $\\mathbb{E}[noise(x)]$.\n",
    "\n",
    "*Notes on Noise*: This is quite intuitive in the case of binary variables - noise refers to the outcome corresponding to $x$ that occurs with the smallest frequency.\n",
    "\n",
    "**Error Decomposition**: We can decompose the error of a hypothesis $h\\in H$ and the Bayes error as:\n",
    "\n",
    "$$R(h)-R^*=(R(h)-R(h^*))+(R(h^*)-R^*)$$\n",
    "\n",
    "where $h^*=\\inf\\limits_{h\\in H} R(h)$ i.e. it is the best in-class hypothesis. The first parenthesis is the estimation error and the latter denotes the approximation error. \n",
    "\n",
    "\n",
    "## Theorems, Lemmas, Corrolaries, etc.\n",
    "\n",
    "**Learning bounds - finite H, consistent case**: Let $H$ be a finite set of functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. Let $\\mathcal{A}$ be an algorithm that for any target concept $c\\in H$ and i.i.d sample $S$ returns a consistent hypothesis $h_S$: $\\hat{R}(h_S)=0$. Then, for any $\\epsilon,\\delta>0$, the inequality $P_{S\\sim D^m}[R(h_S)\\leq \\epsilon]\\geq 1-\\delta$ holds if \n",
    "\n",
    "$$m\\geq \\frac{1}{\\epsilon}\\left(\\log|H|+\\log\\frac{1}{\\delta}\\right).$$\n",
    "\n",
    "**Proof**: Let $\\epsilon > 0$ and $h$ be a consistent hypothesis: $\\hat{R}(h)=0$. Then we bound the following probability:\n",
    "\n",
    "$$\\mathbb{P}\\left\\{\\exists h\\in H: \\hat{R}(h)=0\\cap R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=\\mathbb{P}\\left\\{\\bigcup\\limits_{h\\in H} \\hat{R}(h)=0\\cap R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$\\leq^1 \\sum\\limits_{h\\in H} \\mathbb{P}\\left\\{\\hat{R}(h)=0\\cap R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=^2 \\sum\\limits_{h\\in H} \\mathbb{P}\\left\\{\\hat{R}(h)=0|R(h)>\\epsilon\\right\\}\\mathbb{P}\\left\\{R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$\\leq^3 \\mathbb{P}\\left\\{\\hat{R}(h)=0|R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=^4 \\sum\\limits_{h\\in H} \\mathbb{P}\\left\\{\\bigcap\\limits_{i=1}^m \\mathbb{I}\\{h(x_i)\\neq c(x_i)\\}=0|R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=^5 \\sum\\limits_{h\\in H}\\prod\\limits_{i=1}^m \\mathbb{P}\\left\\{h(x_i)=c(x_i)|R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$\\leq^6 \\sum\\limits_{h\\in H}\\prod\\limits_{i=1}^m (1-\\epsilon)$$\n",
    "\n",
    "$$=|H|\\cdot (1-\\epsilon)^m.$$\n",
    "\n",
    "where $\\leq^1$ follows by subadditivity, $=^2$ follows by definition of conditional probability, $\\leq^3$ follows as the distribution function is bounded above by one, $=^4$ follows by definition of the empirical error, $=^5$ follows by independence of the data points sampled, and $\\leq^6$ follows as \n",
    "\n",
    "$$R(h)>\\epsilon\\Rightarrow P(h(x)\\neq c(x))\\geq \\epsilon\\Rightarrow P(h(x)=c(x))\\leq 1-\\epsilon.$$\n",
    "\n",
    "*Intuition Check*: Let us check what we just said. This is intuitively correct as our $\\epsilon$ is inversely proportional to the number of points collected. We made the statement that this holds for ANY distribution, so of course this relationship makes sense. Likewise, it makes sense that more hypotheses implies we need more training points - more candidate functions (a larger function space) means we need more points to ensure our algorithm correctly approximates the concepts. Finally, it trivially follows the first check that the number of points we neded to approximate is inversely proportional to the probability lower bound we desire to obtain.\n",
    "\n",
    "**Hoeffding's Lemma**: Let $X$ be a random variable with $\\mathbb{E}[X]=0$ and $X\\in [a,b]$. Then for any $t>0$, \n",
    "\n",
    "$$\\mathbb{E}[e^{tX}]\\leq e^{\\frac{t^2(b-a)^2}{8}}.$$\n",
    "\n",
    "**Proof**: We first note that as $X$ is a bounded random variable that\n",
    "\n",
    "$$\\mathbb{V}[X]=\\mathbb{V}\\left[X-\\frac{b+a}{2}\\right]$$\n",
    "\n",
    "$$=\\mathbb{E}\\left[\\left(X-\\frac{b+a}{2}\\right)^2\\right]$$\n",
    "\n",
    "$$\\leq \\mathbb{E}\\left[\\left(b-\\frac{b+a}{2}\\right)^2\\right]=\\frac{(b-a)^2}{4}.$$\n",
    "\n",
    "Then let $\\psi_X(\\lambda)=\\log\\mathbb{E}[e^{tX}]$ denote the logarithmic MGF. We can then deduce that \n",
    "\n",
    "$$\\psi''_X(t)=e^{-\\psi_X(t)}\\mathbb{E}\\left[X^2e^{tX}\\right]-e^{-2\\psi_X(t)}\\left(\\mathbb{E}\\left[Xe^{tX}\\right]\\right)^2$$\n",
    "$$=\\mathbb{V}[X]\\leq \\frac{(b-a)^2}{4}.$$\n",
    "\n",
    "Then ultimately as $\\psi_X(0)=\\psi'_X(0)=0$, we have by Taylor's theorem for some $\\theta\\in[0,t]$ that \n",
    "\n",
    "$$\\psi_X(t)=\\psi_X(0)+\\psi'_X(0)+\\frac{t^2}{2}\\psi_X''(\\theta)=\\frac{t^2}{2}\\psi_X''(\\theta)$$\n",
    "\n",
    "$$\\leq \\frac{t^2(b-a)^2}{8}$$\n",
    "\n",
    "$$\\iff e^{\\psi_X(t)}=\\mathbb{E}[e^{tX}]\\leq e^{\\frac{t^2(b-a)^2}{8}}.$$\n",
    "\n",
    "*Intuition Check/Recap*: This neat proof evades the dumb argument found in most textbooks - here we take the logarithm of the left hand side of the inequality in the lemma and Taylor expand and find the only term left is the variance which we can upper bound as it is a bounded random variable. Very cool proof - credited to Boucheron, Lugosi, and Massart in \"Concentration Inequalities\".\n",
    "\n",
    "**Hoeffding's Inequality**: Let $X_1,\\dots,X_m$ be independent random variables with $X_i$ taking values in $[a_i,b_i]$ for all $i\\in[m]$. Then for any $\\epsilon > 0$, the following inequalities hold for $S_m=\\sum_{i=1}^m X_i$:\n",
    "\n",
    "$$\\begin{cases} \\mathbb{P}\\left[S_m-\\mathbb{E}[S_m]\\geq \\epsilon\\right] \\leq e^{-2\\epsilon^2/\\sum_{i=1}^m (b_i-a_i)^2} \\\\ \\mathbb{P}\\left[S_m - \\mathbb{E}[S_m]\\leq -\\epsilon\\right]\\leq e^{-2\\epsilon^2/\\sum_{i=1}^m (b_i-a_i)^2}\\end{cases}$$\n",
    "\n",
    "**Proof**: We apply Markov's inequality with the monotically increasing, non-negative function $x\\mapsto e^{tx}$ to obtain\n",
    "\n",
    "$$\\mathbb{P}\\left[S_m-\\mathbb{E}[S_m]\\geq \\epsilon\\right]\\leq \\frac{\\mathbb{E}\\left[e^{t\\left(S_m-\\mathbb{E}[S_m]\\right)}\\right]}{e^{t\\epsilon}}$$\n",
    "\n",
    "$$=e^{-t\\epsilon}\\prod\\limits_{i=1}^m \\mathbb{E}\\left[e^{t(X_i-\\mathbb{E}[X_i])}\\right]$$\n",
    "\n",
    "$$\\leq e^{-t\\epsilon}\\prod\\limits_{i=1}^m e^{t^2\\left(b_i-a_i\\right)^2/8}.$$\n",
    "\n",
    "This function is of the form $e^{\\alpha t^2-\\beta t}$ which can be maximized by completing the square - setting $t=4\\epsilon/\\sum_{i=1}^m (b_i-a_i)^2$ does the job from which we obtain\n",
    "\n",
    "$$\\mathbb{P}\\left[S_m-\\mathbb{E}[S_m]\\geq \\epsilon\\right]\\leq e^{-2\\epsilon^2/\\sum_{i=1}^m (b_i-a_i)^2}.$$\n",
    "\n",
    "**Corollary 2.1**: Let $\\epsilon>0$ and let $S$ denote an i.i.d sample of size $m$. Then for any hypothesis $h:X\\rightarrow \\{0,1\\}$, the following inequality holds \n",
    "\n",
    "$$\\mathbb{P}\\left[|\\hat{R}(h)-R(h)|\\geq \\epsilon\\right]\\leq 2e^{-2m\\epsilon^2}.$$\n",
    "\n",
    "**Proof**: We have \n",
    "\n",
    "$$\\mathbb{P}\\left[|\\hat{R}(h)-R(h)|\\geq \\epsilon\\right]=\\mathbb{P}\\left[\\hat{R}(h)-R(h)\\geq \\epsilon\\cup \\hat{R}(h)-R(h)\\leq -\\epsilon\\right]$$\n",
    "\n",
    "$$\\leq^1 \\mathbb{P}\\left[\\hat{R}(h)-R(h)\\geq \\epsilon\\right]+\\mathbb{P}\\left[\\hat{R}(h)-R(h)\\leq -\\epsilon\\right]$$\n",
    "\n",
    "$$\\leq^2 e^{-2m\\epsilon^2}+e^{-2m\\epsilon^2}=2e^{-2m\\epsilon^2}$$\n",
    "\n",
    "where $\\leq^1$ follows by subadditivity of measure and $\\leq^2$ follows as $\\hat{R}$ is comprised of a finite sum of indicator random variables and thus bounded below by $0$ and above by $1$ (a sub-Gaussian random variable with variance factor $1$). \n",
    "\n",
    "**Theorem 2.2 Learning bound - finite $H$, inconsistent case**: Let $H$ be a finite hypothesis set. Then for any $\\delta>0$, with probability at least $1-\\delta$, the following inequality holds:\n",
    "\n",
    "$$\\forall h\\in H,\\;\\;\\; R(h)\\leq\\hat{R}(h)+\\sqrt{\\frac{\\log|H|+\\log2\\delta^{-1}}{2m}}.$$\n",
    "\n",
    "**Proof**: Trivial from above corollary 2.1 - that is, we apply subadditivity of $\\mathbb{P}$ and corollary 2.1 to each of the individual hypotheses to obtain\n",
    "\n",
    "$$\\mathbb{P}\\left[\\exists h\\in H:|\\hat{R}(h)-R(h)|>\\epsilon\\right]=\\mathbb{P}\\left[\\bigcup\\limits_{i=1}^{|H|} |\\hat{R}(h_i)-R(h_i)|>\\epsilon\\right]$$\n",
    "\n",
    "$$\\leq \\sum\\limits_{i=1}^{|H|} \\mathbb{P}\\left[|\\hat{R}(h_i)-R(h_i)|>\\epsilon\\right]\\leq |H|\\cdot 2e^{-2m\\epsilon^2}.$$\n",
    "\n",
    "*Intuition Check/Recap*: Note that in the case of inconsistent hypotheses, there is no arbitrary epsilon bounding the generalization error. Instead, we can only bound the generalization error by the emprical error of the same hypothesis plus some small term. Notice that this small term is given by \n",
    "\n",
    "$$2e^{-2m\\epsilon^2}=\\delta\\iff \\epsilon=\\pm \\sqrt{\\frac{\\log_e\\left(\\sqrt{\\frac{2|H|}{\\delta}}\\right)}{m}}\\approx O\\left(\\sqrt{\\frac{\\log_2(|H|)}{m}}\\right)$$\n",
    "\n",
    "because $\\log_2(|H|)=\\frac{\\log_e(|H|)}{\\log_e(2)}$ and I make the crude approximation $$\\log_e(\\sqrt{\\frac{2}{\\delta}})+\\log_e(\\sqrt{|H|})\\approx 1.4\\log_e(|H|).$$\n",
    "\n",
    "To be honest, this bound is kind of trash when you think about it... $\\hat{R}(h)\\in[0,1]$, but we do not necessarily have $\\hat{R}(h)+O(stuff)\\leq 1$ and $R(h)$ is not necessarily bounded above by 1, so how can we interpret this??? Well, $\\log_2|H|$ is the number of bits to encode the set of permissible hypotheses, $m\\gg 0$ leads to more tight bounds on the error, and our error grows proportionally to the square root of the logarithm of the number of hypotheses. The number here is meaningless as a standalone figure depending on $|H|$, $\\delta$, and $m$. The important bits are just the intuitive relationship among the variables at hand (as per usual)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1d34f",
   "metadata": {},
   "source": [
    "# Chapter 2 Exercises\n",
    "\n",
    "**2.1**: Two-oracle variant of the PAC model. Assume that positive and negative examples are now drawn from two separate distributions $D_+$ and $D_-$. For an accuracy $(1-\\delta)$, the learning algorithm must find a hypothesis $h$ such that:\n",
    "\n",
    "$$\\mathbb{P}_{x\\sim D_+}\\left[h(x)=0\\right]\\leq\\epsilon,\\text{and}\\;\\; \\mathbb{P}_{x\\sim D_-}\\left[h(x)=1\\right]\\leq\\epsilon.$$\n",
    "\n",
    "Thus the hypothesis must have a small error on both distirbutions. Let $C$ be any concept class and $H$ be any hypothesis space. Let $h_0$ and $h_1$ represent the identically 0 and identically 1 functions, respectively. Prove that $C$ is efficiently PAC-learnable using $H$ in the standard (one-oracle) PAC-model if and only if it is efficiently PAC-learnable using $H\\cup \\{h_0,h_1\\}$ in this two-oracle PAC model.\n",
    "\n",
    "**Answer**: This problem has a typo - it should be accuracy $1-\\delta$, not $1-\\epsilon$. \n",
    "\n",
    "For $(\\Rightarrow)$, we know it holds for all distributions over $\\mathcal{X}$, so we take our measure to be defined by the following distribution:\n",
    "\n",
    "$$\\mathbb{P}_{x\\sim D}(X\\leq x)=\\frac{1}{2}\\left[\\mathbb{P}_{x\\sim D_-}(X\\leq x)+\\mathbb{P}_{x\\sim D_+}(X\\leq x)\\right].$$\n",
    "\n",
    "Thus as we assume the one-oracle model is PAC-learnable, for all $\\epsilon,\\delta>0$, we have for the above distribution defined over $\\mathcal{X}$ that\n",
    "\n",
    "$$\\mathbb{P}(R(h)\\leq \\frac{\\epsilon}{2})\\geq 1-\\delta$$\n",
    "\n",
    "where \n",
    "\n",
    "$$R(h)=\\mathbb{P}_{x\\sim D}(h(x)\\neq c(x))=\\frac{1}{2}\\left[\\mathbb{P}_{x\\sim D_-}(h(x)\\neq c(x))+\\mathbb{P}_{x\\sim D_+}(h(x)\\neq c(x))\\right]$$\n",
    "\n",
    "and thus \n",
    "\n",
    "$$\\mathbb{P}(R(h)\\leq \\frac{\\epsilon}{2})=\\mathbb{P}\\left(\\mathbb{P}_{x\\sim D_-}(h(x)\\neq c(x))+\\mathbb{P}_{x\\sim D_+}(h(x)\\neq c(x))\\leq \\epsilon\\right)\\geq 1-\\delta$$\n",
    "\n",
    "implying the two-oracle model is indeed PAC-learnable. As the algorithm $\\mathcal{A}$ used in the one-oracle model was polynomial in $\\epsilon^{-1},\\delta^{-1},n,$ and $size(c)$, we have the same is true for the one-oracle model as we can employ the same algorithm.\n",
    "\n",
    "For $(\\Leftarrow)$, we notice that if it takes $m_1$ examples for us to achieve $1-\\delta$ accuracy in the negative example distribution and $m_2$ examples for us to achieve $1-\\delta$ accuracy in the positive example distribution, then setting $m=\\max\\{m_1,m_2\\}$ trivially proves our result and is of course still polynomial in $\\epsilon^{-1},\\delta^{-1},n,$ and $size(c)$ because for any distribution $D$ over both positive and negative examples and $m$ as defined, we have by subadditivity of $\\mathbb{P}$ that\n",
    "\n",
    "$$R(h)=\\mathbb{P}_{x\\sim D}\\left(h(x)\\neq c(x)\\right)=\\mathbb{P}\\left(h(x)\\neq c(x)|x=0\\cup h(x)\\neq c(x)|x=1\\right)\\leq \\mathbb{P}_{x\\sim D_-}(h(x)\\neq c(x))+\\mathbb{P}_{x\\sim D_+}(h(x)\\neq c(x))\\leq 2\\epsilon.$$\n",
    "\n",
    "Moreover, we have\n",
    "\n",
    "$$\\mathbb{P}_{x\\sim D^m}\\left(R(h)\\leq \\epsilon\\right)=\\mathbb{P}_{x\\sim D^m}\\left(R(h)\\leq \\epsilon|m\\geq m_1\\text{ negative examples }\\cap m \\geq m_2 \\text{ positive examples}\\right)\\mathbb{P}_{x\\sim D^m}\\left(m\\geq m_1\\text{ negative examples }\\cap m\\geq m_2 \\text{ positive examples}\\right)$$\n",
    "$$\\geq (1-\\delta)\\mathbb{P}_{x\\sim D^m}\\left(m\\geq m_1\\text{ negative examples }\\cap m\\geq m_2 \\text{ positive examples}\\right)$$\n",
    "\n",
    "and so success in this direction boils down to ensuring that we can generate enough positive and negative examples to reach $m_2$ and $m_1$ in $m$ samples. If the distribution $D$ is biased, then we can pick $h_0$ or $h_1$ (i.e. exclusively guessing positive or negative for all $x\\in\\mathcal{X}$). If the distribution is not biased, then we use Chernoff bounds to pick $m$ to ensure this probability of $m_1$ and $m_2$ negative and positive examples is close enough to one for $(1-\\delta)$ accuracy. Phew - this was quite the first question for a textbook :).\n",
    "\n",
    "**2.2**: PAC learning of hyper-rectangles. An axis-aligned hyper-rectangle in $\\mathbb{R}^n$ is a set of the form $[a_1,b_1]\\times\\cdots\\times[a_n,b_n]$. Show that axis-aligned hyper-rectangles are PAC-learnable by extending the proof given in Example 2.1 for the case $n=2$.\n",
    "\n",
    "**Answer**: This is quite dumb. We take the example generated in the book and the only edit to the proof is that the algorithm now returns the tightest hyper-rectangle, and we have $2n$ regions $r_1,\\dots,r_{2n}$ where our algorithm could possibly go wrong. Thus we end up showing that \n",
    "\n",
    "$$\\mathbb{P}_{S\\sim D^m}\\left\\{R(R_S)>\\epsilon\\right\\}\\leq 2ne^{-m\\epsilon/2n}\\leq \\delta$$\n",
    "\n",
    "$$\\iff m\\geq \\frac{2n}{\\epsilon}\\log\\frac{2n}{\\delta}$$\n",
    "\n",
    "which shows the scale of the problem difficulty as we amp up the dimension of the input space.\n",
    "\n",
    "\n",
    "**2.3**: Concentric circles: Let $X=\\mathbb{R}^2$ and consider the set of concepts of the form $c=\\{(x,y):x^2+y^2\\leq r^2\\}$ for some real number $r$. Show that this class can be $(\\epsilon,\\delta)-$PAC-learned from training data of size $m\\geq \\epsilon^{-1}\\log(\\delta^{-1})$.\n",
    "\n",
    "**Answer**: Simple, given $m$ points $x_1,\\dots,x_m\\sim D$, it suffices for the hypothesis to select \n",
    "\n",
    "$$r=\\max\\{\\lVert x_1\\rVert,\\dots,\\lVert x_m\\rVert\\}$$\n",
    "\n",
    "so that our hypothesis $C_S$ is defined as follows:\n",
    "\n",
    "$$C_S(x)=\\begin{cases} 1 & \\lVert x\\rVert\\leq r \\\\ 0 & \\text{otherwise}.\\end{cases}$$\n",
    "\n",
    "Let us draw a circle $A$ centered at the origin with $P_{x\\sim D}(x\\in A)=1-\\epsilon$ so that the $P_{x\\sim D}(x\\in C_T\\setminus A)=\\epsilon$. Then we have \n",
    "\n",
    "$$\\mathbb{P}_{x\\sim D}\\left(R(C_S)>\\epsilon\\right)=\\mathbb{P}_{x\\sim D}\\left(\\bigcap\\limits_{i=1}^m x_i\\cap C_T\\setminus A=\\emptyset\\right)$$\n",
    "\n",
    "$$=\\prod\\limits_{i=1}^m \\mathbb{P}_{x\\sim D}\\left(x_i\\cap C_T\\setminus A=\\emptyset\\right)$$\n",
    "\n",
    "$$\\leq (1-\\epsilon)^m\\leq e^{-m\\epsilon}\\leq \\delta$$\n",
    "\n",
    "$$\\iff m\\geq \\epsilon^{-1}\\log \\delta^{-1}.$$\n",
    "\n",
    "**2.4**: Non-concentric circles. Let $X=\\mathbb{R}^2$ and consider the set of concepts of the form $c=\\{x\\in\\mathbb{R}^2:\\lVert x-x_0\\rVert\\leq r\\}$ for some point $x_0\\in\\mathbb{R}^2$ and real number $r$. Gertrude, an aspiring machine learning researcher, attempts to show that this class of concepts may be $(\\epsilon,\\delta)$-PAC-learned with sample complexity $m\\geq 3\\epsilon^{-1}\\log(3\\delta^{-1})$, but she is having trouble with her proof. Her idea is that the learning algorithm would select the smallest circle consistent with the training data. She has drawn three regions $r_1,r_2,r_3$ around the edge of concept $c$, with each region having probability $\\epsilon/3$. She wants to argue that if the generalization error is greater than or equal to $\\epsilon$, then one of these regions must have been missed by the training data, and hence this event will occur with probability at most $\\delta$. Can you tell Gertrude if her approach works?\n",
    "\n",
    "**Answer**: Who names their kid Gertrude first off? Second, I think her approach works. Notice that partitioning the circle into two regions $r_1$ and $r_2$ is not enough as the circle can trivially be off-center, but still touch both regions. Three or more regions are necessary to enforce that one is not touched by the circle if there is $>\\epsilon$ error. \n",
    "\n",
    "**2.5**: Triangles. Let $X=\\mathbb{R}^2$ with orthonormal basis $(e_1,e_2)$, and consider the set of concepts defined by the area inside a right triangle $ABC$ with two sides parallel to the axes, with $AB/\\lVert AB\\rVert = e_1$ and $AC/\\lVert AC\\rVert = e_2$, and $\\lVert AB\\rVert/\\lVert AC\\rVert=\\alpha$ for some $\\alpha\\in\\mathbb{R}_+$. Show, using similar methods to those used in the chapter for the axis-aligned rectangles, that this class can be $(\\epsilon,\\delta)$-PAC-learned from the training data of size $m\\geq 3\\epsilon^{-1}\\log(3\\delta^{-1})$. \n",
    "\n",
    "**Answer**: We must first propose an algorithm. We can do the following to develop the tightest, consistent triangle encompassing the data. \n",
    "\n",
    "1. Find and set: \n",
    "\n",
    "$$x_0=\\min\\limits_{(x_i,y_i)\\in \\{(x_1,y_1),\\cdots,(x_m,y_m)\\}} x_i$$\n",
    "\n",
    "and \n",
    "\n",
    "$$y_0=\\min\\limits_{(x_i,y_i)\\in \\{(x_1,y_1),\\cdots,(x_m,y_m)\\}} y_i$$\n",
    "\n",
    "2. Find and set:\n",
    "\n",
    "$$(x_l,y_l)=\\arg\\max\\limits_{(x_i,y_i)\\in \\{(x_1,y_1),\\cdots,(x_m,y_m)\\}}\\left\\{\\lVert (x_i,y_i)-(x_0,y_0)\\rVert\\right\\}$$\n",
    "\n",
    "3. Find the line segment intersecting the lines $y=x_0$, $x=y_0$, and includes the point $(x_l,y_l)$ on it.\n",
    "\n",
    "We now show this algorithm is efficiently PAC-learnable. Again, similar to the other geometric learning concepts discussed prior, this will only have errors where we mistake a positive example as a negative one because $T_S\\subset T_C$, the true concept/triangle. As before, we have three regions that could possibly yield error. Then we apply the exact same analysis to yield \n",
    "\n",
    "$$m\\geq 3\\epsilon^{-1}\\log(3\\delta^{-1}).$$\n",
    "\n",
    "**2.6**: Learning in the presence of noise - rectangles. In example 2.1, we showed that the concept class of axis-aligned rectangles is PAC-learnable. Consider now the case where the training points received by the learner are subject to the following noise: points negatively labeled are unaffected by the noise but the label of a positive training point is randomly flipped to negative with probability $\\eta\\in(0,\\frac{1}{2})$. The exact value of the noise rate $\\eta$ is not known to the learner but an upper bound $\\eta'$ is supplied to him with $\\eta\\leq\\eta'\\leq\\frac{1}{2}$. Show that the algorithm described in class returning the tightest rectangle containing positive points can still PAC-learn axis-aligned rectangles in the presence of this noise. To do so, you can proceed using the following steps:\n",
    "\n",
    "(a) Using the same notation in example 2.1, assume that $P(R)\\geq \\epsilon$. Suppose that $R(R')\\geq \\epsilon$. Give an upper bound on the probabilty that $R'$ misses a region $r_j,j\\in[1,4]$ in terms of $\\epsilon$ and $\\eta'$?\n",
    "\n",
    "(b) Use that to give an upper bound on $P(R(R')\\geq \\epsilon)$ in terms of $\\epsilon$, $\\eta'$, and conclude by giving a sample complexity bound.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "(a) Again, the exercises share too much overlap. We have\n",
    "\n",
    "$$\\mathbb{P}_{S\\sim D^m}(S\\notin r_j)=\\mathbb{P}_{S\\sim D^m}\\left\\{\\bigcap\\limits_{i=1}^m x_i\\notin r_j\\cup x_i\\in r_j,\\text{ mislabeled}\\right\\}$$\n",
    "\n",
    "$$\\leq \\prod\\limits_{i=1}^m \\mathbb{P}(x_i\\notin r_j)+\\mathbb{P}(x_i\\in r_j)\\eta$$\n",
    "\n",
    "$$=\\prod\\limits_{i=1}^m 1-\\mathbb{P}(x_i\\in r_j)+\\mathbb{P}(x_i\\in r_j)\\eta$$\n",
    "\n",
    "$$\\leq \\prod\\limits_{i=1}^m 1-\\frac{\\epsilon}{2n}+\\eta\\frac{\\epsilon}{2n}$$\n",
    "\n",
    "$$=\\left( 1-\\epsilon\\left(1-\\eta\\right)\\right)^m$$\n",
    "\n",
    "(b) And so from (a), we can deduce that \n",
    "\n",
    "$$m\\geq \\frac{2n}{(1-\\eta)\\epsilon}\\log 2n\\delta^{-1}$$\n",
    "\n",
    "for $1-\\delta$ confidence and $\\epsilon$ accuracy.\n",
    "\n",
    "**2.7**: Too much text :0\n",
    "\n",
    "**2.8**: Super boring - who cares about learning intervals...\n",
    "\n",
    "**2.9**: Consistent hypotheses. In this chapter, we showed that for a finite hypothesis set $H$, a consistent learning algorithm $\\mathcal{A}$ is a PAC-learning algorithm. Here, we consider a converse question. Let $Z$ be a finite set of $m$ labeled points. Suppose that you are given a PAC-learning algorithm $\\mathcal{A}$. Show that you can use $\\mathcal{A}$ and a finite training sample $S$ to find in polynomial time a hypothesis $h\\in H$ that is consistent with $Z$ with high probability.\n",
    "\n",
    "**Answer**: I am not sure about this one... If we set $x\\sim Unif(Z)$, then we have \n",
    "\n",
    "$$R_D(h)\\mathbb{E}_{x\\sim D}\\left[\\mathbb{I}\\{h(x)\\neq c(x)\\}\\right]=\\sum\\limits_{i=1}^m \\mathbb{I}\\{h(x)\\neq c(x)\\}\\cdot\\frac{1}{m}$$\n",
    "\n",
    "or in other words, the error for guessing wrong is $\\frac{1}{m}$. This implies that if $R_D(h)<\\frac{1}{m}$, then $R_D(h)$ could not have made any mistake over all points in $Z$. As $\\mathcal{A}$ is a PAC-learning algorithm, we have, with $\\epsilon=\\frac{1}{m^2}$, that there is a polynomial function that returns a hypothesis $h_S$ such that $R_D(h_S)<\\epsilon$ with confidence $1-\\delta$. As $R_D(h_S)<\\epsilon=\\frac{1}{m^2}<\\frac{1}{m}$, then \n",
    "\n",
    "$$\\hat{R}(h_S)=0$$\n",
    "\n",
    "as desired with high probability.\n",
    "\n",
    "**2.10**: Senate laws. For important questions, President Mouth relies on expert advice. He selects an appropriate advisor from a collection of $H=2,800$ experts. \n",
    "\n",
    "(a) Assume that laws are proposed in a random fashion independently and identically according to some distribution $D$ determined by an unknown group of senators. Assume that President Mouth can find and select an expert senator out of $H$ who has consistently voted with the majority for the last $m=200$ laws. Give a bound on the probability that such a senator incorrectly predicts the global vote for a future law. What is the value of the bound with $95$% confidence?\n",
    "\n",
    "(b) Assume now that President Mouth can find and select an expert senator out of $H$ who has consistently voted with the majority for all but $m'=20$ of the last $m=200$ laws. What is the value of the new bound?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "(a) \n",
    "\n",
    "\n",
    "(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077ef65",
   "metadata": {},
   "source": [
    "# Chapter 3 - Rademacher Complexity and VC-Dimension\n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Martingale Difference**: A sequence of random variables $V_1,V_2,\\dots,$ is a martingale difference sequence with respect to $X_1,X_2,\\dots$ if for all $i>0$, $V_i$ is a function of $X_1,\\dots,X_i$ and \n",
    "\n",
    "$$\\mathbb{E}\\left[V_{i+1}|X_1,\\dots,X_i\\right]=0.$$\n",
    "\n",
    "*Intuition*: Basically, a martingale difference sequence is a fair game - that is you expect to win nothing the next turn conditioned on the past turns and sigma field.\n",
    "\n",
    "**Empirical Rademacher complexity**: Let $G$ be a family of functions mapping from $Z$ to $[a,b]$ and $S=(z_1,\\dots,z_m)$ a fixed sample of size $m$ with elements in $Z$. Then, the empirical Rademacher complexity of $G$ with resepct to the sample $S$ is defined as:\n",
    "\n",
    "$$\\hat{\\mathcal{R}}_S(G)=\\mathbb{E}_\\sigma\\left[\\sup\\limits_{g\\in G} \\frac{1}{m}\\sum\\limits_{i=1}^m \\sigma_i g(z_i)\\right],$$\n",
    "\n",
    "where $\\sigma=(\\sigma_1,\\dots,\\sigma_m)^T$, with $\\sigma_i$s independent uniform random variables taking values in $\\{-1,1\\}^T$. The random variables $\\sigma_i$ are called Rademacher variables.\n",
    "\n",
    "*Alternative definition*: Alternatively if we instead take let $\\mathbb{P}_n$ denote the empirical distribution over $g_s=\\left(g(z_1),\\dots,g(z_m)\\right)^T$, that is,\n",
    "\n",
    "$$\\mathbb{P}_n(x)=\\frac{\\# \\text{ of occurances of x in } g_s}{m},$$\n",
    "\n",
    "then we have \n",
    "\n",
    "$$\\hat{\\mathcal{R}}_S(G)=\\mathbb{E}_\\sigma\\left[\\langle \\sigma,\\frac{g_s}{m}\\rangle\\right].$$\n",
    "\n",
    "*Intuition*: At the highest level, this serves as a proxy for how complex and dense the class of functions $G$ is or moreover the size of the set. For instance if $G$ is a singleton consisting of the map $f(x)=1$ for all $x$, then $\\hat{\\mathcal{R}}_S(G)\\approx \\frac{1}{\\sqrt{m}}$. If instead $G$ was the space of all polynomials, a behemoth class of functions, then since the space of polynomials is dense over the reals, we'd easily obtain that $\\hat{\\mathcal{R}}_S(G)=1$. This is interesting because it is the **maximum correlation between a random vector $\\sigma$ and a hypothesis from the class $G$**! We can easily deduce that $\\hat{\\mathcal{R}}_S(G)\\in[-1,1]$ and it is also the normalized inner product as was the case for the sample correlation of two vectors. Rademacher complexity, intuitively to me, is like a penalization term/constraint in our objective of finding the best algorithm to regress the data - too much complexity and you'd expect to spend too much time training your algorithm. \n",
    "\n",
    "**Rademacher complexity**: Let $D$ denote the distribution according to which samples are drawn. For any integer $m\\geq 1$, the Rademacher complexity of $G$ is the expectation of the empirical Rademacher complexity over all samples of size $m$ drawn according to $D$:\n",
    "\n",
    "$$\\mathcal{R}_m(G)=\\mathbb{E}_{S\\sim D^m}\\left[\\hat{\\mathcal{R}}_S(G)\\right].$$\n",
    "\n",
    "*Intuition*: More or less a simple extension of the empirical Rademacher complexity.\n",
    "\n",
    "**Covering Numbers**: A covering number $\\mathcal{N}_p(G,\\epsilon)$ is the minimal number of $L_p$ balls of radius $\\epsilon >0$ needed to cover a family of loss functions $G$.\n",
    "\n",
    "*Intuition*: An alternative, but less frequently encountered, complexity measure for a family of functions $G$.\n",
    "\n",
    "**Packing Numbers**: A packing number $\\mathcal{M}_p(G,\\epsilon)$ is the maximum number of non-overlapping $L_p$ balls of radius $\\epsilon$ centered in $G$.\n",
    "\n",
    "*Intuition*: There is quite extensive literature proving similarity and bounds between this, covering numbers, and Rademacher complexity by Dudley't theorem, the work of Haussler, and Ledoux/Talagrad.\n",
    "\n",
    "## Theorems, Lemmas, and Corollaries\n",
    "\n",
    "**Lemma D.2**: Let $V$ and $Z$ be random variables satisfying $\\mathbb{E}[V|Z]=0$ and, for some measurable function $f$ and constant $c\\geq 0$, the inequalities: \n",
    "\n",
    "$$f(Z)\\leq V\\leq f(Z)+c.$$\n",
    "\n",
    "Then, for $t>0$,\n",
    "\n",
    "$$\\mathbb{E}[e^{sV}|Z]\\leq e^{t^2c^2/8}.$$\n",
    "\n",
    "**Proof**: Note that as $V|Z$ is a random variable still, we can apply Hoeffding's lemma because $V|Z\\in[f(Z),f(Z)+c]$ and $V|Z$ is centered at zero.\n",
    "\n",
    "*Intuition*: As stated, we have the same assumptions as Hoeffding's and thus this results in the same bound.\n",
    "\n",
    "**Azuma's inequality**: Let $V_1,V_2,\\dots$ be a martingale difference sequence with respect to the random variables $X_1,X_2,\\dots$, and assume that for $i>0$, there is a constant $c_i>0$ and random variable $Z_i$, which is a function of $X_1,\\dots,X_{i-1}$, that satisfy\n",
    "\n",
    "$$Z_i\\leq V_i\\leq Z_i+c_i.$$\n",
    "\n",
    "Then, for all $\\epsilon>0$ and $m$, \n",
    "\n",
    "$$\\mathbb{P}\\left\\{\\left|\\sum\\limits_{i=1}^m V_i\\right|\\geq \\epsilon\\right\\}\\leq 2e^{\\frac{-2\\epsilon^2}{\\sum_1^m c_i^2}}.$$\n",
    "\n",
    "**Proof**: Nearly the same as Hoeffding's inequality. We apply iterated expectation to reach similar conclusions and invoke the prior lemma to get $\\leq^*$:\n",
    "\n",
    "$$\\mathbb{P}(S_m\\geq\\epsilon)\\leq \\frac{\\mathbb{E}[e^{t\\sum V_i}]}{e^{t\\epsilon}}$$\n",
    "\n",
    "$$=\\frac{\\mathbb{E}\\left[\\prod\\limits_{i=1}^m \\mathbb{E}\\left[e^{tV_i}|X_1,\\dots,X_{i-1}\\right]\\right]}{e^{t\\epsilon}}$$\n",
    "\n",
    "$$\\leq^* e^{-t\\epsilon}e^{t^2\\sum c_i^2/8}$$\n",
    "\n",
    "$$\\leq e^{-\\frac{2\\epsilon}{\\sum c_i^2}}.$$\n",
    "\n",
    "*Intuition*: Think of this as conditional Hoeffding's - that is, we condition on some set to bound the random variable and then apply the same result.\n",
    "\n",
    "**McDiarmid's inequality**: Let $X_1,\\dots,X_m\\in\\mathcal{X}^m$ be a set of $m\\geq 1$ independent random variables and assume that there exist $c_1,\\dots,c_m>0$ such that $f:\\mathcal{X}^m\\rightarrow\\mathbb{R}$ satisfies the following conditions:\n",
    "\n",
    "$$\\left|f(x_1,\\dots,x_i,\\dots,x_m)-f(x_1,\\dots,x_i',\\dots,x_m)\\right|\\leq c_i,$$\n",
    "\n",
    "for all $i\\in[1,m]$ and any points $x_1,\\dots,x_m,x_i'\\in\\mathcal{X}$. Let $f(S)$ denote $f(X_1,\\dots,X_m)$, then, for all $\\epsilon>0$, the following inequality holds:\n",
    "\n",
    "$$\\mathbb{P}\\left\\{\\left|f(S)-\\mathbb{E}[f(s)]\\right|\\geq \\epsilon\\right\\}\\leq e^{-\\frac{2\\epsilon^2}{\\sum_1^m c_i^2}}.$$\n",
    "\n",
    "**Proof**: \n",
    "\n",
    "*Intuition*: This is a slight generalization of Hoeffding's - it's saying that if the i.i.d random variables are Lipschitz-esq, then we can find an exponential bound with the Lipschitz-esq constants. I say Lipschitz-esq because the traces are indeed Lipschitz; however, the function itself is not and so this is a weaker condition. \n",
    "\n",
    "## Chapter 3 Exercises\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ffdf65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe36bfb",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "[1] \"Foundations of Machine Learning\" by Mohri et al., 2012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
