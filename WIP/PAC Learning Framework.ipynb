{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5834a3c1",
   "metadata": {},
   "source": [
    "# PAC Learning Framework\n",
    "\n",
    "In this section, I will concisely cover the Probably Approximately Correct (PAC) learning framework. \n",
    "\n",
    "## Motivation\n",
    "In a sentence, the PAC framework quantifies the requirements (number of training points, time complexity, and space complexity) to approximately learn patterns in the data. THIS IS HUGE! For instance, a simple example is learning linear versus nonlinear maps between the input and outputs - linear patterns intuitively require less points to learn the pattern. Concretely, we could look at predicting height with weight data. If we desired a linear pattern, then we only need to search over $\\mathbb{R}_+$ to find a parameter $\\alpha$ such that $\\text{height}=\\alpha\\cdot \\text{weight}$. If instead we desire a nonlinear pattern, then think about the expansion of the space! We could fit polynomials, trigonometric, and the list goes on. I believe this should provide sufficient motivation for why we care deeply about quantifying the number of samples we need depending on the relationships we deem important (linear, nonlinear, indicator functions, exponentials, etc.). Now, I turn to mathematics for precision and clarity.\n",
    "\n",
    "## Some Notations\n",
    "\n",
    "- Let $\\mathcal{X}$ denote the input space (includes all data excluding the output variable)\n",
    "\n",
    "- Let $\\mathcal{Y}$ denote the set of labels/output/target variables (as in [1], I limit discussion to binary outputs right now)\n",
    "\n",
    "- Examples $(x,y)\\in(\\mathcal{X},\\mathcal{Y})$ are assumed to be drawn independently and identically distributed (i.i.d.) according to some unknown distribution $\\mathcal{D}$ \n",
    "\n",
    "- \n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Concept**: A concept $c:\\mathcal{X}\\rightarrow\\mathcal{Y}$ is a mapping from $\\mathcal{X}$ to $\\mathcal{Y}$.\n",
    "\n",
    "**Concept Class**: A concept class $\\mathcal{C}$ is a set of concepts we may wish to learn. THESE \n",
    "\n",
    "## Examples\n",
    "\n",
    "- Let us consider the features to be height, weight, age, ethnicity, and cancer (boolean) with the target being death (boolean). Then let us discuss concepts: for instance, all instances of Germans over 6' with weight >300lbs with cancer and over the age 55 get mapped to 1 i.e. they're dead (this is made up). This concept is thus an indicator function. We could consider a concept class to be a union of similar indicator variables. This type of concept class forms the basis for rule-based algorithms which will be discussed in a separate notebook.\n",
    "\n",
    "- In the book, they discuss out of the ordinary examples - such as the concept class of triangles or circles or other geometric figures. While yes these are technically concept classes, they are sparsely used in practice.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Generalization Error**: Given a hypothesis $h\\in\\mathcal{H}$, a target concept $c\\in\\mathcal{C}$, and an underlying distribution $D$, the generalization error or risk of $h$ is defined by \n",
    "\n",
    "$$R(h) = P_{x\\sim D}\\left[h(x)\\neq c(x)\\right] = \\mathbb{E}_{x\\sim D}\\left[\\mathbb{I}_{h(x)\\neq c(x)}\\right].$$\n",
    "\n",
    "In words, it is the probability that the hypothesis does not align with the target concept. This is of course unknown as we do not know $D$; however, we can measure the empirical error of the hypothesis based on a labeled sample of the data.\n",
    "\n",
    "**Empirical Error**: Given a hypothesis $h\\in\\mathcal{H}$, a target concept $c\\in\\mathcal{C}$, and a sample $S=(x_1,\\dots,x_m)$, the empirical error or risk of $h$ is defined by \n",
    "\n",
    "$$\\hat{R}(h) = \\frac{1}{m}\\sum\\limits_{i=1}^m \\mathbb{I}_{h(x_i)\\neq c(x_i)}.$$\n",
    "\n",
    "It is an emsemble of the error derived from the sample $S$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d57547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe36bfb",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "[1] \"Foundations of Machine Learning\" by Mohri et al., 2012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
