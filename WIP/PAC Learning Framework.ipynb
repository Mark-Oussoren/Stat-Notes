{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5834a3c1",
   "metadata": {},
   "source": [
    "# PAC Learning\n",
    "\n",
    "In this notebook, I will cover the Probably Approximately Correct (PAC) learning framework. \n",
    "\n",
    "# Motivation\n",
    "In a sentence, the PAC framework quantifies the requirements (number of training points, time complexity, and space complexity) to approximately learn patterns in the data. THIS IS HUGE! For instance, a simple example is learning linear versus nonlinear maps between the input and outputs - linear patterns intuitively require less points to learn the pattern. Concretely, we could look at predicting height with weight data. If we desired a linear pattern, then we only need to search over $\\mathbb{R}_+$ to find a parameter $\\alpha$ such that $\\text{height}=\\alpha\\cdot \\text{weight}$. If instead we desire a nonlinear pattern, then think about the expansion of the space! We could fit polynomials, trigonometric, and the list goes on. I believe this should provide sufficient motivation for why we care deeply about quantifying the number of samples we need depending on the relationships we deem important (linear, nonlinear, indicator functions, exponentials, etc.). Now, I turn to mathematics for precision and clarity.\n",
    "\n",
    "## Some Notations\n",
    "\n",
    "- Let $\\mathcal{X}$ denote the input space (includes all data excluding the output variable)\n",
    "\n",
    "- Let $\\mathcal{Y}$ denote the set of labels/output/target variables (as in [1], I limit discussion to binary outputs right now)\n",
    "\n",
    "- Examples $(x,y)\\in(\\mathcal{X},\\mathcal{Y})$ are assumed to be drawn independently and identically distributed (i.i.d.) according to some unknown distribution $\\mathcal{D}$ \n",
    "\n",
    "# The PAC Learning Framework - Chapter 2\n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Concept**: A concept $c:\\mathcal{X}\\rightarrow\\mathcal{Y}$ is a mapping from $\\mathcal{X}$ to $\\mathcal{Y}$.\n",
    "\n",
    "**Concept Class**: A concept class $\\mathcal{C}$ is a set of concepts we may wish to learn.  \n",
    "\n",
    "**Generalization Error**: Given a hypothesis $h\\in\\mathcal{H}$, a target concept $c\\in\\mathcal{C}$, and an underlying distribution $D$, the generalization error or risk of $h$ is defined by \n",
    "\n",
    "$$R(h) = P_{x\\sim D}\\left[h(x)\\neq c(x)\\right] = \\mathbb{E}_{x\\sim D}\\left[\\mathbb{I}_{h(x)\\neq c(x)}\\right].$$\n",
    "\n",
    "In words, it is the probability that the hypothesis does not align with the target concept. This is of course unknown as we do not know $D$; however, we can measure the empirical error of the hypothesis based on a labeled sample of the data.\n",
    "\n",
    "**Empirical Error**: Given a hypothesis $h\\in\\mathcal{H}$, a target concept $c\\in\\mathcal{C}$, and a sample $S=(x_1,\\dots,x_m)$, the empirical error or risk of $h$ is defined by \n",
    "\n",
    "$$\\hat{R}(h) = \\frac{1}{m}\\sum\\limits_{i=1}^m \\mathbb{I}_{h(x_i)\\neq c(x_i)}.$$\n",
    "\n",
    "It is an emsemble of the error derived from the sample $S$.\n",
    "\n",
    "**PAC-learning**: A concept class $C$ is said to be PAC-learnable if there exists an algorithm $\\mathcal{A}$ and a polynomial function $poly(\\cdot,\\cdot,\\cdot,\\cdot)$ such that for any $\\epsilon>0$ and $\\delta>0$, for all distributions $D$ on $\\mathcal{X}$ and for any target concept $c\\in\\mathcal{C}$, the following holds for any sample size $m\\geq poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$:\n",
    "\n",
    "$$\\mathbb{P}_{S\\sim D^m} \\left[R(h_S)\\leq \\epsilon\\right]\\geq 1-\\delta.$$ \n",
    "\n",
    "If $\\mathcal{A}$ further runs in $poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$, then $C$ is said to be efficiently PAC-learnable. When such an algorithm $\\mathcal{A}$ exists, it is called a PAC-learning algorithm for $C$.\n",
    "\n",
    "*Note on PAC-learning*: This definition is CRAZY!! I can draw the samples from any distribution of my choosing - the exponential, Cauchy, uniform, a crazy convolution of all three, etc. and it still holds that the generalization error of a hypothesis on the samples is bounded above by $\\epsilon$ with some high probability $1-\\delta$. The definition stattes that after enough samples (whose exact number is based on some polynomial of the error, bound, the dimension of the data, and the size of the concept representation). HOWEVER, the test data must be distributed according to the same as the training set - in other words, we impose a stationary condition that is all too common and all too unrealistic in practice. FURTHERMORE, we make a statement about the entire concept class - so we must cast quite a wide net to catch all the possible fish (target concepts). I wonder if this can be relaxed to just focus on a subset of a concept class...\n",
    "\n",
    "**Agnostic PAC-learning**: Let $H$ be a hypothesis set. $\\mathcal{A}$ is an agnostic PAC-learning algorithm if there exists a polynomial function $poly(\\cdot,\\cdot,\\cdot,\\cdot)$ such that for any $\\epsilon > 0$ and $\\delta > 0$, for all distributions $D$ over $\\mathcal{X}\\times\\mathcal{Y}$, the following holds for any sample size $m\\geq poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$:\n",
    "\n",
    "$$\\mathbb{P}_{S\\sim D^m}\\left[R(h_S) - \\min\\limits_{h\\in H} R(h) \\leq \\epsilon\\right]\\geq 1-\\delta.$$\n",
    "\n",
    "If $\\mathcal{A}$ further runs in $poly(\\epsilon^{-1},\\delta^{-1},n,size(c))$, then it is said to be an efficient agnostic PAC-learning algorithm.\n",
    "\n",
    "*Note on Agnostic PAC*: Note the differences. The BIG difference is that original PAC-learning only features a distribution over the input space. What does this mean? Well, it means that our hypothesis is simply a DETERMINISTIC function - that is every $x$ is mapped to a unique output and there is no randomness. Here, our output is a random function of the input - so a given $x_1$ can be mapped to $1$ with probability $0.4$ and $0$ with probability $0.6$. Beautiful - as in most real-world scenarios a scientist surveys, this is by far the most common. HOWEVER, the author is boring and chose to exclusively focus on the deterministic setting where ouputs are uniquely determined by some measurable function WP1.\n",
    "\n",
    "**Bayes Error**: Given a distribution $D$ over $\\mathcal{X}\\times\\mathcal{Y}$, the Bayes error $R^*$ is defined as the infimum of the errors achieved by measurable functions $h:\\mathcal{X}\\rightarrow\\mathcal{Y}$:\n",
    "\n",
    "$$R^*=\\inf\\limits_{\\text{measurable} h} R(h).$$\n",
    "\n",
    "A hypothesis $h$ with $R(h)=R^*$ is called a Bayes hypothesis or Bayes classifier.\n",
    "\n",
    "*Notes on Bayes Error*: The Bayes error is the hypothesis that minimizes the generalization error - simple, I have no further comment.\n",
    "\n",
    "**Noise**: Given a distribution $D$ over $\\mathcal{X}\\times\\mathcal{Y}$, the noise at point $x\\in\\mathcal{X}$ is defined by \n",
    "\n",
    "$$noise(x)=\\min\\limits_{y\\in\\{0,1\\}}\\left\\{P(y|x)\\right\\}.$$\n",
    "\n",
    "The average noise or the noise associated to $D$ is $\\mathbb{E}[noise(x)]$.\n",
    "\n",
    "*Notes on Noise*: This is quite intuitive in the case of binary variables - noise refers to the outcome corresponding to $x$ that occurs with the smallest frequency.\n",
    "\n",
    "**Error Decomposition**: We can decompose the error of a hypothesis $h\\in H$ and the Bayes error as:\n",
    "\n",
    "$$R(h)-R^*=(R(h)-R(h^*))+(R(h^*)-R^*)$$\n",
    "\n",
    "where $h^*=\\inf\\limits_{h\\in H} R(h)$ i.e. it is the best in-class hypothesis. The first parenthesis is the estimation error and the latter denotes the approximation error. \n",
    "\n",
    "\n",
    "## Theorems, Lemmas, Corrolaries, etc.\n",
    "\n",
    "**Learning bounds - finite H, consistent case**: Let $H$ be a finite set of functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. Let $\\mathcal{A}$ be an algorithm that for any target concept $c\\in H$ and i.i.d sample $S$ returns a consistent hypothesis $h_S$: $\\hat{R}(h_S)=0$. Then, for any $\\epsilon,\\delta>0$, the inequality $P_{S\\sim D^m}[R(h_S)\\leq \\epsilon]\\geq 1-\\delta$ holds if \n",
    "\n",
    "$$m\\geq \\frac{1}{\\epsilon}\\left(\\log|H|+\\log\\frac{1}{\\delta}\\right).$$\n",
    "\n",
    "**Proof**: Let $\\epsilon > 0$ and $h$ be a consistent hypothesis: $\\hat{R}(h)=0$. Then we bound the following probability:\n",
    "\n",
    "$$\\mathbb{P}\\left\\{\\exists h\\in H: \\hat{R}(h)=0\\cap R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=\\mathbb{P}\\left\\{\\bigcup\\limits_{h\\in H} \\hat{R}(h)=0\\cap R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$\\leq^1 \\sum\\limits_{h\\in H} \\mathbb{P}\\left\\{\\hat{R}(h)=0\\cap R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=^2 \\sum\\limits_{h\\in H} \\mathbb{P}\\left\\{\\hat{R}(h)=0|R(h)>\\epsilon\\right\\}\\mathbb{P}\\left\\{R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$\\leq^3 \\mathbb{P}\\left\\{\\hat{R}(h)=0|R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=^4 \\sum\\limits_{h\\in H} \\mathbb{P}\\left\\{\\bigcap\\limits_{i=1}^m \\mathbb{I}\\{h(x_i)\\neq c(x_i)\\}=0|R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$=^5 \\sum\\limits_{h\\in H}\\prod\\limits_{i=1}^m \\mathbb{P}\\left\\{h(x_i)=c(x_i)|R(h)>\\epsilon\\right\\}$$\n",
    "\n",
    "$$\\leq^6 \\sum\\limits_{h\\in H}\\prod\\limits_{i=1}^m (1-\\epsilon)$$\n",
    "\n",
    "$$=|H|\\cdot (1-\\epsilon)^m.$$\n",
    "\n",
    "where $\\leq^1$ follows by subadditivity, $=^2$ follows by definition of conditional probability, $\\leq^3$ follows as the distribution function is bounded above by one, $=^4$ follows by definition of the empirical error, $=^5$ follows by independence of the data points sampled, and $\\leq^6$ follows as \n",
    "\n",
    "$$R(h)>\\epsilon\\Rightarrow P(h(x)\\neq c(x))\\geq \\epsilon\\Rightarrow P(h(x)=c(x))\\leq 1-\\epsilon.$$\n",
    "\n",
    "*Intuition Check*: Let us check what we just said. This is intuitively correct as our $\\epsilon$ is inversely proportional to the number of points collected. We made the statement that this holds for ANY distribution, so of course this relationship makes sense. Likewise, it makes sense that more hypotheses implies we need more training points - more candidate functions (a larger function space) means we need more points to ensure our algorithm correctly approximates the concepts. Finally, it trivially follows the first check that the number of points we neded to approximate is inversely proportional to the probability lower bound we desire to obtain.\n",
    "\n",
    "**Hoeffding's Lemma**: Let $X$ be a random variable with $\\mathbb{E}[X]=0$ and $X\\in [a,b]$. Then for any $t>0$, \n",
    "\n",
    "$$\\mathbb{E}[e^{tX}]\\leq e^{\\frac{t^2(b-a)^2}{8}}.$$\n",
    "\n",
    "**Proof**: We first note that as $X$ is a bounded random variable that\n",
    "\n",
    "$$\\mathbb{V}[X]=\\mathbb{V}\\left[X-\\frac{b+a}{2}\\right]$$\n",
    "\n",
    "$$=\\mathbb{E}\\left[\\left(X-\\frac{b+a}{2}\\right)^2\\right]$$\n",
    "\n",
    "$$\\leq \\mathbb{E}\\left[\\left(b-\\frac{b+a}{2}\\right)^2\\right]=\\frac{(b-a)^2}{4}.$$\n",
    "\n",
    "Then let $\\psi_X(\\lambda)=\\log\\mathbb{E}[e^{tX}]$ denote the logarithmic MGF. We can then deduce that \n",
    "\n",
    "$$\\psi''_X(t)=e^{-\\psi_X(t)}\\mathbb{E}\\left[X^2e^{tX}\\right]-e^{-2\\psi_X(t)}\\left(\\mathbb{E}\\left[Xe^{tX}\\right]\\right)^2$$\n",
    "$$=\\mathbb{V}[X]\\leq \\frac{(b-a)^2}{4}.$$\n",
    "\n",
    "Then ultimately as $\\psi_X(0)=\\psi'_X(0)=0$, we have by Taylor's theorem for some $\\theta\\in[0,t]$ that \n",
    "\n",
    "$$\\psi_X(t)=\\psi_X(0)+\\psi'_X(0)+\\frac{t^2}{2}\\psi_X''(\\theta)=\\frac{t^2}{2}\\psi_X''(\\theta)$$\n",
    "\n",
    "$$\\leq \\frac{t^2(b-a)^2}{8}$$\n",
    "\n",
    "$$\\iff e^{\\psi_X(t)}=\\mathbb{E}[e^{tX}]\\leq e^{\\frac{t^2(b-a)^2}{8}}.$$\n",
    "\n",
    "*Intuition Check/Recap*: This neat proof evades the dumb argument found in most textbooks - here we take the logarithm of the left hand side of the inequality in the lemma and Taylor expand and find the only term left is the variance which we can upper bound as it is a bounded random variable. Very cool proof - credited to Boucheron, Lugosi, and Massart in \"Concentration Inequalities\".\n",
    "\n",
    "**Hoeffding's Inequality**: Let $X_1,\\dots,X_m$ be independent random variables with $X_i$ taking values in $[a_i,b_i]$ for all $i\\in[m]$. Then for any $\\epsilon > 0$, the following inequalities hold for $S_m=\\sum_{i=1}^m X_i$:\n",
    "\n",
    "$$\\begin{cases} \\mathbb{P}\\left[S_m-\\mathbb{E}[S_m]\\geq \\epsilon\\right] \\leq e^{-2\\epsilon^2/\\sum_{i=1}^m (b_i-a_i)^2} \\\\ \\mathbb{P}\\left[S_m - \\mathbb{E}[S_m]\\leq -\\epsilon\\right]\\leq e^{-2\\epsilon^2/\\sum_{i=1}^m (b_i-a_i)^2}\\end{cases}$$\n",
    "\n",
    "**Proof**: We apply Markov's inequality with the monotically increasing, non-negative function $x\\mapsto e^{tx}$ to obtain\n",
    "\n",
    "$$\\mathbb{P}\\left[S_m-\\mathbb{E}[S_m]\\geq \\epsilon\\right]\\leq \\frac{\\mathbb{E}\\left[e^{t\\left(S_m-\\mathbb{E}[S_m]\\right)}\\right]}{e^{t\\epsilon}}$$\n",
    "\n",
    "$$=e^{-t\\epsilon}\\prod\\limits_{i=1}^m \\mathbb{E}\\left[e^{t(X_i-\\mathbb{E}[X_i])}\\right]$$\n",
    "\n",
    "$$\\leq e^{-t\\epsilon}\\prod\\limits_{i=1}^m e^{t^2\\left(b_i-a_i\\right)^2/8}.$$\n",
    "\n",
    "This function is of the form $e^{\\alpha t^2-\\beta t}$ which can be maximized by completing the square - setting $t=4\\epsilon/\\sum_{i=1}^m (b_i-a_i)^2$ does the job from which we obtain\n",
    "\n",
    "$$\\mathbb{P}\\left[S_m-\\mathbb{E}[S_m]\\geq \\epsilon\\right]\\leq e^{-2\\epsilon^2/\\sum_{i=1}^m (b_i-a_i)^2}.$$\n",
    "\n",
    "**Corollary 2.1**: Let $\\epsilon>0$ and let $S$ denote an i.i.d sample of size $m$. Then for any hypothesis $h:X\\rightarrow \\{0,1\\}$, the following inequality holds \n",
    "\n",
    "$$\\mathbb{P}\\left[|\\hat{R}(h)-R(h)|\\geq \\epsilon\\right]\\leq 2e^{-2m\\epsilon^2}.$$\n",
    "\n",
    "**Proof**: We have \n",
    "\n",
    "$$\\mathbb{P}\\left[|\\hat{R}(h)-R(h)|\\geq \\epsilon\\right]=\\mathbb{P}\\left[\\hat{R}(h)-R(h)\\geq \\epsilon\\cup \\hat{R}(h)-R(h)\\leq -\\epsilon\\right]$$\n",
    "\n",
    "$$\\leq^1 \\mathbb{P}\\left[\\hat{R}(h)-R(h)\\geq \\epsilon\\right]+\\mathbb{P}\\left[\\hat{R}(h)-R(h)\\leq -\\epsilon\\right]$$\n",
    "\n",
    "$$\\leq^2 e^{-2m\\epsilon^2}+e^{-2m\\epsilon^2}=2e^{-2m\\epsilon^2}$$\n",
    "\n",
    "where $\\leq^1$ follows by subadditivity of measure and $\\leq^2$ follows as $\\hat{R}$ is comprised of a finite sum of indicator random variables and thus bounded below by $0$ and above by $1$ (a sub-Gaussian random variable with variance factor $1$). \n",
    "\n",
    "**Theorem 2.2 Learning bound - finite $H$, inconsistent case**: Let $H$ be a finite hypothesis set. Then for any $\\delta>0$, with probability at least $1-\\delta$, the following inequality holds:\n",
    "\n",
    "$$\\forall h\\in H,\\;\\;\\; R(h)\\leq\\hat{R}(h)+\\sqrt{\\frac{\\log|H|+\\log2\\delta^{-1}}{2m}}.$$\n",
    "\n",
    "**Proof**: Trivial from above corollary 2.1 - that is, we apply subadditivity of $\\mathbb{P}$ and corollary 2.1 to each of the individual hypotheses to obtain\n",
    "\n",
    "$$\\mathbb{P}\\left[\\exists h\\in H:|\\hat{R}(h)-R(h)|>\\epsilon\\right]=\\mathbb{P}\\left[\\bigcup\\limits_{i=1}^{|H|} |\\hat{R}(h_i)-R(h_i)|>\\epsilon\\right]$$\n",
    "\n",
    "$$\\leq \\sum\\limits_{i=1}^{|H|} \\mathbb{P}\\left[|\\hat{R}(h_i)-R(h_i)|>\\epsilon\\right]\\leq |H|\\cdot 2e^{-2m\\epsilon^2}.$$\n",
    "\n",
    "*Intuition Check/Recap*: Note that in the case of inconsistent hypotheses, there is no arbitrary epsilon bounding the generalization error. Instead, we can only bound the generalization error by the emprical error of the same hypothesis plus some small term. Notice that this small term is given by \n",
    "\n",
    "$$2e^{-2m\\epsilon^2}=\\delta\\iff \\epsilon=\\pm \\sqrt{\\frac{\\log_e\\left(\\sqrt{\\frac{2|H|}{\\delta}}\\right)}{m}}\\approx O\\left(\\sqrt{\\frac{\\log_2(|H|)}{m}}\\right)$$\n",
    "\n",
    "because $\\log_2(|H|)=\\frac{\\log_e(|H|)}{\\log_e(2)}$ and I make the crude approximation $$\\log_e(\\sqrt{\\frac{2}{\\delta}})+\\log_e(\\sqrt{|H|})\\approx 1.4\\log_e(|H|).$$\n",
    "\n",
    "To be honest, this bound is kind of trash when you think about it... $\\hat{R}(h)\\in[0,1]$, but we do not necessarily have $\\hat{R}(h)+O(stuff)\\leq 1$ and $R(h)$ is not necessarily bounded above by 1, so how can we interpret this??? Well, $\\log_2|H|$ is the number of bits to encode the set of permissible hypotheses, $m\\gg 0$ leads to more tight bounds on the error, and our error grows proportionally to the square root of the logarithm of the number of hypotheses. The number here is meaningless as a standalone figure depending on $|H|$, $\\delta$, and $m$. The important bits are just the intuitive relationship among the variables at hand (as per usual)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27fcd6",
   "metadata": {},
   "source": [
    "# Chapter 2 Exercises\n",
    "\n",
    "**2.1**: Two-oracle variant of the PAC model. Assume that positive and negative examples are now drawn from two separate distributions $D_+$ and $D_-$. For an accuracy $(1-\\delta)$, the learning algorithm must find a hypothesis $h$ such that:\n",
    "\n",
    "$$\\mathbb{P}_{x\\sim D_+}\\left[h(x)=0\\right]\\leq\\epsilon,\\text{and}\\;\\; \\mathbb{P}_{x\\sim D_-}\\left[h(x)=1\\right]\\leq\\epsilon.$$\n",
    "\n",
    "Thus the hypothesis must have a small error on both distirbutions. Let $C$ be any concept class and $H$ be any hypothesis space. Let $h_0$ and $h_1$ represent the identically 0 and identically 1 functions, respectively. Prove that $C$ is efficiently PAC-learnable using $H$ in the standard (one-oracle) PAC-model if and only if it is efficiently PAC-learnable using $H\\cup \\{h_0,h_1\\}$ in this two-oracle PAC model.\n",
    "\n",
    "**Answer**: This problem has a typo - it should be accuracy $1-\\delta$, not $1-\\epsilon$. \n",
    "\n",
    "For $(\\Rightarrow)$, we know it holds for all distributions over $\\mathcal{X}$, so we take our measure to be defined by the following distribution:\n",
    "\n",
    "$$\\mathbb{P}_{x\\sim D}(X\\leq x)=\\frac{1}{2}\\left[\\mathbb{P}_{x\\sim D_-}(X\\leq x)+\\mathbb{P}_{x\\sim D_+}(X\\leq x)\\right].$$\n",
    "\n",
    "Thus as we assume the one-oracle model is PAC-learnable, for all $\\epsilon,\\delta>0$, we have for the above distribution defined over $\\mathcal{X}$ that\n",
    "\n",
    "$$\\mathbb{P}(R(h)\\leq \\frac{\\epsilon}{2})\\geq 1-\\delta$$\n",
    "\n",
    "where \n",
    "\n",
    "$$R(h)=\\mathbb{P}_{x\\sim D}(h(x)\\neq c(x))=\\frac{1}{2}\\left[\\mathbb{P}_{x\\sim D_-}(h(x)\\neq c(x))+\\mathbb{P}_{x\\sim D_+}(h(x)\\neq c(x))\\right]$$\n",
    "\n",
    "and thus \n",
    "\n",
    "$$\\mathbb{P}(R(h)\\leq \\frac{\\epsilon}{2})=\\mathbb{P}\\left(\\mathbb{P}_{x\\sim D_-}(h(x)\\neq c(x))+\\mathbb{P}_{x\\sim D_+}(h(x)\\neq c(x))\\leq \\epsilon\\right)\\geq 1-\\delta$$\n",
    "\n",
    "implying the two-oracle model is indeed PAC-learnable. As the algorithm $\\mathcal{A}$ used in the one-oracle model was polynomial in $\\epsilon^{-1},\\delta^{-1},n,$ and $size(c)$, we have the same is true for the one-oracle model as we can employ the same algorithm.\n",
    "\n",
    "For $(\\Leftarrow)$, we have that \n",
    "\n",
    "**2.2**: PAC learning of hyper-rectables. An axis-aligned hyper-rectangle in $\\mathbb{R}^n$ is a set of the form $[a_1,b_1]\\times\\cdots\\times[a_n,b_n]$. Show that axis-aligned hyper-rectangles are PAC-learnable by extending the proof given in Example 2.1 for the case $n=2$.\n",
    "\n",
    "**Answer**: later today - most likely induction with slight twist in probability\n",
    "\n",
    "**2.3**: Concentric circles: Let $X=\\mathbb{R}^2$ and consider the set of concepts of the form $c=\\{(x,y):x^2+y^2\\leq r^2\\}$ for some real number $r$. Show that this class can be $(\\epsilon,\\delta)-$PAC-learned from training data of size $m\\geq \\epsilon^{-1}\\log(\\delta^{-1})$.\n",
    "\n",
    "**Answer**: interesting.. later\n",
    "\n",
    "**2.4**: Non-concentric circles. Let $X=\\mathbb{R}^2$ and consider the set of concepts of the form $c=\\{x\\in\\mathbb{R}^2:\\lVert x-x_0\\rVert\\leq r\\}$ for some point $x_0\\in\\mathbb{R}^2$ and real number $r$. Gertrude, an aspiring machine learning researcher, attempts to show that this class of concepts may be $(\\epsilon,\\delta)$-PAC-learned with sample complexity $m\\geq 3\\epsilon^{-1}\\log(3\\delta^{-1})$, but she is having trouble with her proof. Her idea is that the learning algorithm would select the smallest circle consistent with the training data. She has drawn three regions $r_1,r_2,r_3$ around the edge of concept $c$, with each region having probability $\\epsilon/3$. She wants to argue that if the generalization error is greater than or equal to $\\epsilon$, then one of these regions must have been missed by the training data, and hence this event will occur with probability at most $\\delta$. Can you tell Gertrude if her approach works?\n",
    "\n",
    "**Answer**: Who names their kid Gertrude? Yes Gertrude.\n",
    "\n",
    "**2.5**: Triangles. Let $X=\\mathbb{R}^2$ with orthonormal basis $(e_1,e_2)$, and consider the set of concepts defined by the area inside a right triangle $ABC$ with two sides parallel to the axes, with $AB/\\lVert AB\\rVert = e_1$ and $AC/\\lVert AC\\rVert = e_2$, and $\\lVert AB\\rVert/\\lVert AC\\rVert=\\alpha$ for some $\\alpha\\in\\mathbb{R}_+$. Show, using similar methods to those used in the chapter for the axis-aligned rectangles, that this class can be $(\\epsilon,\\delta)$-PAC-learned from the training data of size $m\\geq 3\\epsilon^{-1}\\log(3\\delta^{-1})$. \n",
    "\n",
    "**2.6/2.7**: Too much text :0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddbd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe36bfb",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "[1] \"Foundations of Machine Learning\" by Mohri et al., 2012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
